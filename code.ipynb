{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports, data load, metric function definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-22T06:57:38.110735Z",
     "start_time": "2022-03-22T06:57:37.640740Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import BallTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-22T06:57:39.038683Z",
     "start_time": "2022-03-22T06:57:39.007102Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = np.load('X_train_surge.npz')\n",
    "Y_train = pd.read_csv('Y_train_surge.csv')\n",
    "X_test = np.load('X_test_surge.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-22T06:57:42.769417Z",
     "start_time": "2022-03-22T06:57:39.877082Z"
    }
   },
   "outputs": [],
   "source": [
    "def split_train_set(X, Y, val_size=0.2, seed=None):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    \n",
    "    nb_examples = len(Y)\n",
    "    val_examples = int(val_size * nb_examples)\n",
    "    \n",
    "    val_indices = rng.choice(nb_examples, size=val_examples, replace=False)\n",
    "    \n",
    "    train_indices = np.setdiff1d(\n",
    "        np.arange(nb_examples),\n",
    "        val_indices,\n",
    "        assume_unique=True\n",
    "    )\n",
    "    train_indices = rng.permutation(train_indices)\n",
    "    \n",
    "    X_train = {}\n",
    "    X_val = {}\n",
    "    for feat in X.files:\n",
    "        X_train[feat] = X[feat][train_indices]\n",
    "        X_val[feat] = X[feat][val_indices]\n",
    "    \n",
    "    return X_train, Y.iloc[train_indices], \\\n",
    "            X_val, Y.iloc[val_indices]\n",
    "\n",
    "X_train, Y_train, X_val, Y_val = split_train_set(X_train, Y_train, val_size=0.091, seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-22T06:57:43.770623Z",
     "start_time": "2022-03-22T06:57:43.755876Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_slp = X_train['t_slp'] / 3600\n",
    "t_slp_delta = t_slp - t_slp[:, 0].reshape(-1, 1)\n",
    "np.allclose(np.round(t_slp_delta), np.round(t_slp_delta)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-22T06:57:48.728017Z",
     "start_time": "2022-03-22T06:57:46.425196Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-22T06:57:51.169259Z",
     "start_time": "2022-03-22T06:57:51.164177Z"
    }
   },
   "outputs": [],
   "source": [
    "SLP_HEIGHT = SLP_WIDTH = 41\n",
    "SLP_PER_EX = 40\n",
    "T_SURGE_NORMALISATION = 240.\n",
    "SLP_REL_TIMESTAMPS = np.arange(24*5, step=3) / T_SURGE_NORMALISATION\n",
    "\n",
    "\n",
    "def normalised_tensor(array, mean, std):\n",
    "        return torch.from_numpy((array - mean) / std)\n",
    "\n",
    "def preprocessing(X, slp_mean=None, slp_std=None, t_slp_mean=None, t_slp_std=None,\n",
    "                 surge_mean=None, surge_std=None):\n",
    "    slp = X['slp'].reshape(-1, SLP_PER_EX, SLP_HEIGHT, SLP_WIDTH)\n",
    "    slp = np.roll(slp, shift=-11, axis=3)\n",
    "    if slp_mean is None:\n",
    "        slp_mean = np.mean(slp)\n",
    "    if slp_std is None:\n",
    "        slp_std = np.std(slp)\n",
    "    slp = normalised_tensor(slp, slp_mean, slp_std)\n",
    "    \n",
    "    fst_slp = X['t_slp'][:, 0] / 3600\n",
    "    fst_slp_tmp = fst_slp.reshape(-1, 1)\n",
    "    \n",
    "    def rel_surge_time(index):\n",
    "        t_surge = X[index] / 3600\n",
    "        t_surge -= fst_slp_tmp\n",
    "        return torch.from_numpy(t_surge / T_SURGE_NORMALISATION)\n",
    "\n",
    "    t_surge1_in = rel_surge_time('t_surge1_input')\n",
    "    t_surge2_in = rel_surge_time('t_surge2_input')\n",
    "    t_surge1_out = rel_surge_time('t_surge1_output')\n",
    "    t_surge2_out = rel_surge_time('t_surge2_output')\n",
    "    \n",
    "    if t_slp_mean is None:\n",
    "        t_slp_mean = np.mean(fst_slp)\n",
    "    if t_slp_std is None:\n",
    "        t_slp_std = np.std(fst_slp)\n",
    "    fst_slp = normalised_tensor(fst_slp, t_slp_mean, t_slp_std)\n",
    "    \n",
    "    surge1 = X['surge1_input']\n",
    "    surge2 = X['surge2_input']\n",
    "    if surge_mean is None or surge_std is None:\n",
    "        surges = np.concatenate([surge1, surge2], axis=None)\n",
    "        surge_mean = np.mean(surges)\n",
    "        surge_std = np.std(surges)\n",
    "    surge1_in = normalised_tensor(surge1, surge_mean, surge_std)\n",
    "    surge2_in = normalised_tensor(surge2, surge_mean, surge_std)\n",
    "    \n",
    "    return X['id_sequence'], slp, slp_mean, slp_std, \\\n",
    "            fst_slp, t_slp_mean, t_slp_std, \\\n",
    "            t_surge1_in, t_surge2_in, t_surge1_out, t_surge2_out, \\\n",
    "            surge1_in, surge2_in, surge_mean, surge_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-22T06:57:53.560037Z",
     "start_time": "2022-03-22T06:57:52.394226Z"
    }
   },
   "outputs": [],
   "source": [
    "train_id_seq, train_slp, slp_mean, slp_std, \\\n",
    "train_fst_slp, t_slp_mean, t_slp_std, \\\n",
    "train_t_surge1_in, train_t_surge2_in, train_t_surge1_out, train_t_surge2_out, \\\n",
    "train_surge1_in, train_surge2_in, surge_mean, surge_std = preprocessing(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-22T06:57:53.627279Z",
     "start_time": "2022-03-22T06:57:53.561203Z"
    }
   },
   "outputs": [],
   "source": [
    "val_id_seq, val_slp, _, _, \\\n",
    "val_fst_slp, _, _, \\\n",
    "val_t_surge1_in, val_t_surge2_in, val_t_surge1_out, val_t_surge2_out, \\\n",
    "val_surge1_in, val_surge2_in, _, _ = preprocessing(X_val, slp_mean, slp_std, t_slp_mean, t_slp_std, surge_mean, surge_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-22T06:57:53.834508Z",
     "start_time": "2022-03-22T06:57:53.628231Z"
    }
   },
   "outputs": [],
   "source": [
    "test_id_seq, test_slp, _, _, \\\n",
    "test_fst_slp, _, _, \\\n",
    "test_t_surge1_in, test_t_surge2_in, test_t_surge1_out, test_t_surge2_out, \\\n",
    "test_surge1_in, test_surge2_in, _, _ = preprocessing(X_test, slp_mean, slp_std, t_slp_mean, t_slp_std, surge_mean, surge_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-22T06:57:54.568335Z",
     "start_time": "2022-03-22T06:57:54.555829Z"
    }
   },
   "outputs": [],
   "source": [
    "train_Y_id_seq = Y_train['id_sequence'].values\n",
    "assert np.alltrue(train_Y_id_seq == train_id_seq), 'Data/label index mismatch'\n",
    "\n",
    "train_surge1_out = Y_train.iloc[:, 1:11].values\n",
    "train_surge1_out = normalised_tensor(train_surge1_out, surge_mean, surge_std)\n",
    "train_surge2_out = Y_train.iloc[:, 11:].values\n",
    "train_surge2_out = normalised_tensor(train_surge2_out, surge_mean, surge_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-22T06:57:55.323690Z",
     "start_time": "2022-03-22T06:57:55.320668Z"
    }
   },
   "outputs": [],
   "source": [
    "val_Y_id_seq = Y_val['id_sequence'].values\n",
    "assert np.alltrue(val_Y_id_seq == val_id_seq), 'Data/label index mismatch'\n",
    "\n",
    "val_surge1_out = Y_val.iloc[:, 1:11].values\n",
    "val_surge1_out = normalised_tensor(val_surge1_out, surge_mean, surge_std)\n",
    "val_surge2_out = Y_val.iloc[:, 11:].values\n",
    "val_surge2_out = normalised_tensor(val_surge2_out, surge_mean, surge_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-22T04:22:09.128857Z",
     "start_time": "2022-03-22T04:22:09.114659Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "resnet_versions = {\n",
    "    18 : models.resnet18,\n",
    "    34 : models.resnet34,\n",
    "    50 : models.resnet50,\n",
    "}\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_dataset = TensorDataset(\n",
    "    train_slp,\n",
    "#     train_fst_slp,\n",
    "    train_t_surge1_in,\n",
    "    train_surge1_in,\n",
    "    train_t_surge2_in,\n",
    "    train_surge2_in,\n",
    "    train_t_surge1_out,\n",
    "    train_surge1_out,\n",
    "    train_t_surge2_out,\n",
    "    train_surge2_out,\n",
    ")\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "val_dataset = TensorDataset(\n",
    "    val_slp,\n",
    "#     val_fst_slp,\n",
    "    val_t_surge1_in,\n",
    "    val_surge1_in,\n",
    "    val_t_surge2_in,\n",
    "    val_surge2_in,\n",
    "    val_t_surge1_out,\n",
    "    val_surge1_out,\n",
    "    val_t_surge2_out,\n",
    "    val_surge2_out,\n",
    ")\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "test_dataset = TensorDataset(\n",
    "    test_slp,\n",
    "#     test_fst_slp,\n",
    "    test_t_surge1_in,\n",
    "    test_surge1_in,\n",
    "    test_t_surge2_in,\n",
    "    test_surge2_in,\n",
    "    test_t_surge1_out,\n",
    "    test_t_surge2_out\n",
    ")\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "class Network(nn.Module):\n",
    "    \n",
    "    def __init__(self, resnet_layers, lstm_hidden_size, lstm_layers=1, dropout=.2):\n",
    "        super().__init__()\n",
    "        self.resnet = resnet_versions[resnet_layers]()\n",
    "        self.resnet_layers = resnet_layers\n",
    "        self.resnet.conv1 = nn.Conv2d(SLP_PER_EX, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "#         self.resnet.fc = nn.Linear(self.resnet.fc.in_features, lstm_hidden_size)\n",
    "\n",
    "        self.lstm_hidden_size = lstm_hidden_size\n",
    "        self.lstm_layers = lstm_layers\n",
    "        self.fc0 = nn.Linear(self.resnet.fc.out_features, lstm_hidden_size)\n",
    "        self.fc1 = nn.Linear(20, 1)\n",
    "        self.fc2 = nn.Linear(20, 1)\n",
    "        self.lstm = nn.LSTM(input_size=1, hidden_size=lstm_hidden_size, num_layers=lstm_layers, bias=True, batch_first=True, \\\n",
    "                            dropout=dropout, bidirectional=False, proj_size=1)\n",
    "        \n",
    "    def __str__(self):\n",
    "        return f'resnet{self.resnet_layers}_{self.lstm_hidden_size}_{self.lstm_layers}'\n",
    "        \n",
    "    def forward(self, slp, t_surge1_in, surge1_in, t_surge2_in, surge2_in, t_surge1_out, t_surge2_out):\n",
    "        cell = self.resnet(slp)\n",
    "        cell = self.fc0(cell)\n",
    "        cell = torch.cat([cell, cell], dim=0).unsqueeze(0)\n",
    "        c_0 = torch.cat([cell]*self.lstm_layers, dim=0)\n",
    "        \n",
    "        surge1 = torch.cat([t_surge1_in, surge1_in], dim=1)\n",
    "        hidden1 = self.fc1(surge1)\n",
    "        surge2 = torch.cat([t_surge2_in, surge2_in], dim=1)\n",
    "        hidden2 = self.fc2(surge2)\n",
    "        hidden = torch.cat([hidden1, hidden2], dim=0).unsqueeze(0)\n",
    "        h_0 = torch.cat([hidden]*self.lstm_layers, dim=0)\n",
    "        \n",
    "        lstm_input = torch.cat([t_surge1_out, t_surge2_out], dim=0).unsqueeze(2)\n",
    "        output, _ = self.lstm(lstm_input, (h_0, c_0))\n",
    "        output = output.squeeze()\n",
    "        return output[:len(slp)], output[len(slp):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-22T09:36:38.581522Z",
     "start_time": "2022-03-22T09:36:38.571293Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "resnet_versions = {\n",
    "    18 : models.resnet18,\n",
    "    34 : models.resnet34,\n",
    "    50 : models.resnet50,\n",
    "}\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_dataset = TensorDataset(\n",
    "    train_slp,\n",
    "#     train_fst_slp,\n",
    "    train_t_surge1_in,\n",
    "    train_surge1_in,\n",
    "    train_t_surge2_in,\n",
    "    train_surge2_in,\n",
    "    train_t_surge1_out,\n",
    "    train_surge1_out,\n",
    "    train_t_surge2_out,\n",
    "    train_surge2_out,\n",
    ")\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "val_dataset = TensorDataset(\n",
    "    val_slp,\n",
    "#     val_fst_slp,\n",
    "    val_t_surge1_in,\n",
    "    val_surge1_in,\n",
    "    val_t_surge2_in,\n",
    "    val_surge2_in,\n",
    "    val_t_surge1_out,\n",
    "    val_surge1_out,\n",
    "    val_t_surge2_out,\n",
    "    val_surge2_out,\n",
    ")\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "test_dataset = TensorDataset(\n",
    "    test_slp,\n",
    "#     test_fst_slp,\n",
    "    test_t_surge1_in,\n",
    "    test_surge1_in,\n",
    "    test_t_surge2_in,\n",
    "    test_surge2_in,\n",
    "    test_t_surge1_out,\n",
    "    test_t_surge2_out\n",
    ")\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "class Network(nn.Module):\n",
    "    \n",
    "    def __init__(self, resnet_layers, lstm_hidden_size, lstm_layers=1, dropout=.2):\n",
    "        super().__init__()\n",
    "        self.resnet = resnet_versions[resnet_layers]()\n",
    "        self.resnet_layers = resnet_layers\n",
    "        self.resnet.conv1 = nn.Conv2d(SLP_PER_EX, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "#         self.resnet.fc = nn.Linear(self.resnet.fc.in_features, lstm_hidden_size)\n",
    "\n",
    "        self.lstm_hidden_size = lstm_hidden_size\n",
    "        self.lstm_layers = lstm_layers\n",
    "        self.fc0 = nn.Linear(self.resnet.fc.out_features, lstm_hidden_size)\n",
    "        self.fc1 = nn.Linear(20, lstm_hidden_size)\n",
    "        self.fc2 = nn.Linear(20, lstm_hidden_size)\n",
    "        self.lstm = nn.LSTM(input_size=1, hidden_size=lstm_hidden_size, num_layers=lstm_layers, bias=True, batch_first=True, \\\n",
    "                            dropout=dropout, bidirectional=False, proj_size=1)\n",
    "        \n",
    "    def __str__(self):\n",
    "        return f'resnet{self.resnet_layers}_{self.lstm_hidden_size}_{self.lstm_layers}'\n",
    "        \n",
    "    def forward(self, slp, t_surge1_in, surge1_in, t_surge2_in, surge2_in, t_surge1_out, t_surge2_out):\n",
    "        cell = self.resnet(slp)\n",
    "        cell = self.fc0(cell)\n",
    "        cell = torch.cat([cell, cell], dim=0).unsqueeze(0)\n",
    "        \n",
    "        surge1 = torch.cat([t_surge1_in, surge1_in], dim=1)\n",
    "        hidden1 = self.fc1(surge1)\n",
    "        surge2 = torch.cat([t_surge2_in, surge2_in], dim=1)\n",
    "        hidden2 = self.fc2(surge2)\n",
    "        hidden = torch.cat([hidden1, hidden2], dim=0).unsqueeze(0)\n",
    "        cell = cell + hidden\n",
    "        c_0 = torch.cat([cell]*self.lstm_layers, dim=0)\n",
    "        h_0 = torch.randn(self.lstm_layers, c_0.shape[1], 1, device=device)\n",
    "        \n",
    "        lstm_input = torch.cat([t_surge1_out, t_surge2_out], dim=0).unsqueeze(2)\n",
    "        output, _ = self.lstm(lstm_input, (h_0, c_0))\n",
    "        output = output.squeeze()\n",
    "        return output[:len(slp)], output[len(slp):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-22T07:04:12.309452Z",
     "start_time": "2022-03-22T07:04:09.948689Z"
    }
   },
   "outputs": [],
   "source": [
    "loss_weights = torch.linspace(1, 0.1, 10, requires_grad=False).to(device)\n",
    "    \n",
    "def surge_prediction_metric(surge1_true, surge2_true, surge1_pred, surge2_pred):\n",
    "    surge1_score = torch.mean(torch.square(surge1_true - surge1_pred) * loss_weights)\n",
    "    surge2_score = torch.mean(torch.square(surge2_true - surge2_pred) * loss_weights)\n",
    "    return surge1_score + surge2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-22T07:14:51.090731Z",
     "start_time": "2022-03-22T07:14:51.084735Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, loss_fn, optmiser, epochs):\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "    \n",
    "    for epoch_num in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = []\n",
    "\n",
    "        for batch, (slp, t_surge1_in, surge1_in, t_surge2_in, surge2_in, t_surge1_out, surge1_out, t_surge2_out, surge2_out) in enumerate(train_dataloader):\n",
    "            slp = slp.to(device)\n",
    "            t_surge1_in = t_surge1_in.to(device)\n",
    "            surge1_in = surge1_in.to(device)\n",
    "            t_surge2_in = t_surge2_in.to(device)\n",
    "            surge2_in = surge2_in.to(device)\n",
    "            t_surge1_out = t_surge1_out.to(device)\n",
    "            surge1_out = surge1_out.to(device)\n",
    "            t_surge2_out = t_surge2_out.to(device)\n",
    "            surge2_out = surge2_out.to(device)\n",
    "            surge1_out_pred, surge2_out_pred = model(slp, t_surge1_in, surge1_in, t_surge2_in, surge2_in, t_surge1_out, t_surge2_out)\n",
    "            \n",
    "            loss = loss_fn(surge1_out, surge2_out, surge1_out_pred, surge2_out_pred)\n",
    "            running_loss.append(loss.item())\n",
    "            \n",
    "            optimiser.zero_grad()\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            \n",
    "        epoch_loss = np.mean(running_loss)\n",
    "        train_loss.append(epoch_loss)\n",
    "        print(f'Epoch {epoch_num+1:03d} | Loss: {epoch_loss:.4f}')\n",
    "        \n",
    "        if epoch_num % 5 == 0:\n",
    "            val_loss = evaluate(model, loss_fn)\n",
    "            print(f'\\tVal loss: {val_loss:.4f}', end=' ')\n",
    "            if val_loss < .45:\n",
    "                torch.save(model.state_dict(), str(model) + f'_{epoch_num}.pth')\n",
    "                print('saved!')\n",
    "            else:\n",
    "                print()\n",
    "            \n",
    "    return train_loss\n",
    "\n",
    "def evaluate(model, loss_fn):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        losses = []\n",
    "        for slp, t_surge1_in, surge1_in, t_surge2_in, surge2_in, t_surge1_out, surge1_out, t_surge2_out, surge2_out in val_dataloader:\n",
    "            slp = slp.to(device)\n",
    "            t_surge1_in = t_surge1_in.to(device)\n",
    "            surge1_in = surge1_in.to(device)\n",
    "            t_surge2_in = t_surge2_in.to(device)\n",
    "            surge2_in = surge2_in.to(device)\n",
    "            t_surge1_out = t_surge1_out.to(device)\n",
    "            surge1_out = surge1_out.to(device)\n",
    "            t_surge2_out = t_surge2_out.to(device)\n",
    "            surge2_out = surge2_out.to(device)\n",
    "            surge1_out_pred, surge2_out_pred = model(slp, t_surge1_in, surge1_in, t_surge2_in, surge2_in, t_surge1_out, t_surge2_out)\n",
    "            \n",
    "            loss = loss_fn(surge1_out, surge2_out, surge1_out_pred, surge2_out_pred)\n",
    "            losses.append(loss.item())\n",
    "        return np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-22T07:12:00.362967Z",
     "start_time": "2022-03-22T07:09:49.971945Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | Loss: 0.8765\n",
      "\tVal loss: 0.7410 \n",
      "Epoch 002 | Loss: 0.6861\n",
      "Epoch 003 | Loss: 0.6350\n",
      "Epoch 004 | Loss: 0.5898\n",
      "Epoch 005 | Loss: 0.5632\n",
      "Epoch 006 | Loss: 0.5303\n",
      "\tVal loss: 0.5325 \n",
      "Epoch 007 | Loss: 0.5097\n",
      "Epoch 008 | Loss: 0.4925\n",
      "Epoch 009 | Loss: 0.4750\n",
      "Epoch 010 | Loss: 0.4610\n",
      "Epoch 011 | Loss: 0.4479\n",
      "\tVal loss: 0.4808 saved!\n",
      "Epoch 012 | Loss: 0.4298\n",
      "Epoch 013 | Loss: 0.4177\n",
      "Epoch 014 | Loss: 0.4068\n",
      "Epoch 015 | Loss: 0.3963\n",
      "Epoch 016 | Loss: 0.3899\n",
      "\tVal loss: 0.4946 saved!\n",
      "Epoch 017 | Loss: 0.3902\n",
      "Epoch 018 | Loss: 0.3779\n",
      "Epoch 019 | Loss: 0.3585\n",
      "Epoch 020 | Loss: 0.3424\n",
      "Epoch 021 | Loss: 0.3318\n",
      "\tVal loss: 0.4861 saved!\n",
      "Epoch 022 | Loss: 0.3149\n",
      "Epoch 023 | Loss: 0.3114\n",
      "Epoch 024 | Loss: 0.3066\n",
      "Epoch 025 | Loss: 0.3071\n",
      "Epoch 026 | Loss: 0.3091\n",
      "\tVal loss: 0.4870 saved!\n",
      "Epoch 027 | Loss: 0.2960\n",
      "Epoch 028 | Loss: 0.2808\n",
      "Epoch 029 | Loss: 0.2746\n",
      "Epoch 030 | Loss: 0.2660\n",
      "Epoch 031 | Loss: 0.2530\n",
      "\tVal loss: 0.4660 saved!\n",
      "Epoch 032 | Loss: 0.2436\n",
      "Epoch 033 | Loss: 0.2376\n",
      "Epoch 034 | Loss: 0.2386\n",
      "Epoch 035 | Loss: 0.2379\n",
      "Epoch 036 | Loss: 0.2307\n",
      "\tVal loss: 0.4551 saved!\n",
      "Epoch 037 | Loss: 0.2305\n",
      "Epoch 038 | Loss: 0.2237\n",
      "Epoch 039 | Loss: 0.2167\n",
      "Epoch 040 | Loss: 0.2133\n",
      "Epoch 041 | Loss: 0.2080\n",
      "\tVal loss: 0.4704 saved!\n",
      "Epoch 042 | Loss: 0.2035\n",
      "Epoch 043 | Loss: 0.2024\n",
      "Epoch 044 | Loss: 0.1995\n",
      "Epoch 045 | Loss: 0.1994\n",
      "Epoch 046 | Loss: 0.2049\n",
      "\tVal loss: 0.4312 saved!\n",
      "Epoch 047 | Loss: 0.2054\n",
      "Epoch 048 | Loss: 0.1959\n",
      "Epoch 049 | Loss: 0.1853\n",
      "Epoch 050 | Loss: 0.1759\n",
      "Epoch 051 | Loss: 0.1706\n",
      "\tVal loss: 0.4570 saved!\n",
      "Epoch 052 | Loss: 0.1674\n",
      "Epoch 053 | Loss: 0.1673\n",
      "Epoch 054 | Loss: 0.1698\n",
      "Epoch 055 | Loss: 0.1710\n",
      "Epoch 056 | Loss: 0.1724\n",
      "\tVal loss: 0.4385 saved!\n",
      "Epoch 057 | Loss: 0.1757\n",
      "Epoch 058 | Loss: 0.1780\n",
      "Epoch 059 | Loss: 0.1755\n",
      "Epoch 060 | Loss: 0.1831\n",
      "Epoch 061 | Loss: 0.1967\n",
      "\tVal loss: 0.4382 saved!\n",
      "Epoch 062 | Loss: 0.1985\n",
      "Epoch 063 | Loss: 0.1857\n",
      "Epoch 064 | Loss: 0.1666\n",
      "Epoch 065 | Loss: 0.1507\n",
      "Epoch 066 | Loss: 0.1430\n",
      "\tVal loss: 0.4466 saved!\n",
      "Epoch 067 | Loss: 0.1395\n",
      "Epoch 068 | Loss: 0.1386\n",
      "Epoch 069 | Loss: 0.1398\n",
      "Epoch 070 | Loss: 0.1430\n",
      "Epoch 071 | Loss: 0.1464\n",
      "\tVal loss: 0.4402 saved!\n",
      "Epoch 072 | Loss: 0.1526\n",
      "Epoch 073 | Loss: 0.1517\n",
      "Epoch 074 | Loss: 0.1478\n",
      "Epoch 075 | Loss: 0.1460\n",
      "Epoch 076 | Loss: 0.1484\n",
      "\tVal loss: 0.4461 saved!\n",
      "Epoch 077 | Loss: 0.1529\n",
      "Epoch 078 | Loss: 0.1554\n",
      "Epoch 079 | Loss: 0.1555\n",
      "Epoch 080 | Loss: 0.1527\n",
      "Epoch 081 | Loss: 0.1562\n",
      "\tVal loss: 0.4351 saved!\n",
      "Epoch 082 | Loss: 0.1632\n",
      "Epoch 083 | Loss: 0.1522\n",
      "Epoch 084 | Loss: 0.1328\n",
      "Epoch 085 | Loss: 0.1223\n",
      "Epoch 086 | Loss: 0.1173\n",
      "\tVal loss: 0.4166 saved!\n",
      "Epoch 087 | Loss: 0.1169\n",
      "Epoch 088 | Loss: 0.1192\n",
      "Epoch 089 | Loss: 0.1216\n",
      "Epoch 090 | Loss: 0.1217\n",
      "Epoch 091 | Loss: 0.1225\n",
      "\tVal loss: 0.4500 saved!\n",
      "Epoch 092 | Loss: 0.1254\n",
      "Epoch 093 | Loss: 0.1281\n",
      "Epoch 094 | Loss: 0.1316\n",
      "Epoch 095 | Loss: 0.1322\n",
      "Epoch 096 | Loss: 0.1308\n",
      "\tVal loss: 0.4569 saved!\n",
      "Epoch 097 | Loss: 0.1300\n",
      "Epoch 098 | Loss: 0.1273\n",
      "Epoch 099 | Loss: 0.1280\n",
      "Epoch 100 | Loss: 0.1312\n"
     ]
    }
   ],
   "source": [
    "resnet18 = Network(18, 100, 1).to(device)\n",
    "optimiser = optim.Adam(resnet18.parameters())\n",
    "_ = train(resnet18, surge_prediction_metric, optimiser, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-22T07:14:27.810016Z",
     "start_time": "2022-03-22T07:12:11.105970Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | Loss: 0.8706\n",
      "\tVal loss: 0.6980 \n",
      "Epoch 002 | Loss: 0.6680\n",
      "Epoch 003 | Loss: 0.6169\n",
      "Epoch 004 | Loss: 0.5948\n",
      "Epoch 005 | Loss: 0.5562\n",
      "Epoch 006 | Loss: 0.5358\n",
      "\tVal loss: 0.5964 \n",
      "Epoch 007 | Loss: 0.5113\n",
      "Epoch 008 | Loss: 0.4912\n",
      "Epoch 009 | Loss: 0.4739\n",
      "Epoch 010 | Loss: 0.4580\n",
      "Epoch 011 | Loss: 0.4407\n",
      "\tVal loss: 0.5134 saved!\n",
      "Epoch 012 | Loss: 0.4299\n",
      "Epoch 013 | Loss: 0.4191\n",
      "Epoch 014 | Loss: 0.4061\n",
      "Epoch 015 | Loss: 0.3917\n",
      "Epoch 016 | Loss: 0.3781\n",
      "\tVal loss: 0.4521 saved!\n",
      "Epoch 017 | Loss: 0.3627\n",
      "Epoch 018 | Loss: 0.3539\n",
      "Epoch 019 | Loss: 0.3483\n",
      "Epoch 020 | Loss: 0.3389\n",
      "Epoch 021 | Loss: 0.3321\n",
      "\tVal loss: 0.5070 saved!\n",
      "Epoch 022 | Loss: 0.3214\n",
      "Epoch 023 | Loss: 0.3105\n",
      "Epoch 024 | Loss: 0.3011\n",
      "Epoch 025 | Loss: 0.2841\n",
      "Epoch 026 | Loss: 0.2713\n",
      "\tVal loss: 0.4701 saved!\n",
      "Epoch 027 | Loss: 0.2614\n",
      "Epoch 028 | Loss: 0.2644\n",
      "Epoch 029 | Loss: 0.2577\n",
      "Epoch 030 | Loss: 0.2501\n",
      "Epoch 031 | Loss: 0.2470\n",
      "\tVal loss: 0.4955 saved!\n",
      "Epoch 032 | Loss: 0.2446\n",
      "Epoch 033 | Loss: 0.2385\n",
      "Epoch 034 | Loss: 0.2322\n",
      "Epoch 035 | Loss: 0.2298\n",
      "Epoch 036 | Loss: 0.2251\n",
      "\tVal loss: 0.4429 saved!\n",
      "Epoch 037 | Loss: 0.2150\n",
      "Epoch 038 | Loss: 0.2041\n",
      "Epoch 039 | Loss: 0.1979\n",
      "Epoch 040 | Loss: 0.1969\n",
      "Epoch 041 | Loss: 0.1955\n",
      "\tVal loss: 0.4752 saved!\n",
      "Epoch 042 | Loss: 0.1960\n",
      "Epoch 043 | Loss: 0.1989\n",
      "Epoch 044 | Loss: 0.1983\n",
      "Epoch 045 | Loss: 0.1977\n",
      "Epoch 046 | Loss: 0.1958\n",
      "\tVal loss: 0.4352 saved!\n",
      "Epoch 047 | Loss: 0.1982\n",
      "Epoch 048 | Loss: 0.1996\n",
      "Epoch 049 | Loss: 0.1968\n",
      "Epoch 050 | Loss: 0.1972\n",
      "Epoch 051 | Loss: 0.1960\n",
      "\tVal loss: 0.4400 saved!\n",
      "Epoch 052 | Loss: 0.1903\n",
      "Epoch 053 | Loss: 0.1829\n",
      "Epoch 054 | Loss: 0.1776\n",
      "Epoch 055 | Loss: 0.1720\n",
      "Epoch 056 | Loss: 0.1693\n",
      "\tVal loss: 0.4738 saved!\n",
      "Epoch 057 | Loss: 0.1689\n",
      "Epoch 058 | Loss: 0.1700\n",
      "Epoch 059 | Loss: 0.1696\n",
      "Epoch 060 | Loss: 0.1675\n",
      "Epoch 061 | Loss: 0.1665\n",
      "\tVal loss: 0.4449 saved!\n",
      "Epoch 062 | Loss: 0.1692\n",
      "Epoch 063 | Loss: 0.1714\n",
      "Epoch 064 | Loss: 0.1727\n",
      "Epoch 065 | Loss: 0.1776\n",
      "Epoch 066 | Loss: 0.1761\n",
      "\tVal loss: 0.4902 saved!\n",
      "Epoch 067 | Loss: 0.1687\n",
      "Epoch 068 | Loss: 0.1605\n",
      "Epoch 069 | Loss: 0.1526\n",
      "Epoch 070 | Loss: 0.1446\n",
      "Epoch 071 | Loss: 0.1404\n",
      "\tVal loss: 0.4467 saved!\n",
      "Epoch 072 | Loss: 0.1405\n",
      "Epoch 073 | Loss: 0.1459\n",
      "Epoch 074 | Loss: 0.1502\n",
      "Epoch 075 | Loss: 0.1532\n",
      "Epoch 076 | Loss: 0.1577\n",
      "\tVal loss: 0.4578 saved!\n",
      "Epoch 077 | Loss: 0.1675\n",
      "Epoch 078 | Loss: 0.1724\n",
      "Epoch 079 | Loss: 0.1819\n",
      "Epoch 080 | Loss: 0.1815\n",
      "Epoch 081 | Loss: 0.1645\n",
      "\tVal loss: 0.4868 saved!\n",
      "Epoch 082 | Loss: 0.1497\n",
      "Epoch 083 | Loss: 0.1494\n",
      "Epoch 084 | Loss: 0.1508\n",
      "Epoch 085 | Loss: 0.1361\n",
      "Epoch 086 | Loss: 0.1306\n",
      "\tVal loss: 0.4311 saved!\n",
      "Epoch 087 | Loss: 0.1324\n",
      "Epoch 088 | Loss: 0.1318\n",
      "Epoch 089 | Loss: 0.1274\n",
      "Epoch 090 | Loss: 0.1218\n",
      "Epoch 091 | Loss: 0.1232\n",
      "\tVal loss: 0.4401 saved!\n",
      "Epoch 092 | Loss: 0.1296\n",
      "Epoch 093 | Loss: 0.1363\n",
      "Epoch 094 | Loss: 0.1328\n",
      "Epoch 095 | Loss: 0.1317\n",
      "Epoch 096 | Loss: 0.1320\n",
      "\tVal loss: 0.4826 saved!\n",
      "Epoch 097 | Loss: 0.1356\n",
      "Epoch 098 | Loss: 0.1403\n",
      "Epoch 099 | Loss: 0.1333\n",
      "Epoch 100 | Loss: 0.1246\n"
     ]
    }
   ],
   "source": [
    "resnet18 = Network(18, 100, 2).to(device)\n",
    "optimiser = optim.Adam(resnet18.parameters())\n",
    "_ = train(resnet18, surge_prediction_metric, optimiser, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-22T07:17:20.541896Z",
     "start_time": "2022-03-22T07:15:00.822282Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | Loss: 0.8700\n",
      "\tVal loss: 0.6742 \n",
      "Epoch 002 | Loss: 0.6671\n",
      "Epoch 003 | Loss: 0.6236\n",
      "Epoch 004 | Loss: 0.5827\n",
      "Epoch 005 | Loss: 0.5584\n",
      "Epoch 006 | Loss: 0.5331\n",
      "\tVal loss: 0.5189 \n",
      "Epoch 007 | Loss: 0.5056\n",
      "Epoch 008 | Loss: 0.4866\n",
      "Epoch 009 | Loss: 0.4695\n",
      "Epoch 010 | Loss: 0.4518\n",
      "Epoch 011 | Loss: 0.4320\n",
      "\tVal loss: 0.4762 \n",
      "Epoch 012 | Loss: 0.4222\n",
      "Epoch 013 | Loss: 0.4060\n",
      "Epoch 014 | Loss: 0.4010\n",
      "Epoch 015 | Loss: 0.3973\n",
      "Epoch 016 | Loss: 0.3759\n",
      "\tVal loss: 0.4526 \n",
      "Epoch 017 | Loss: 0.3578\n",
      "Epoch 018 | Loss: 0.3490\n",
      "Epoch 019 | Loss: 0.3385\n",
      "Epoch 020 | Loss: 0.3261\n",
      "Epoch 021 | Loss: 0.3139\n",
      "\tVal loss: 0.4716 \n",
      "Epoch 022 | Loss: 0.3087\n",
      "Epoch 023 | Loss: 0.3078\n",
      "Epoch 024 | Loss: 0.2967\n",
      "Epoch 025 | Loss: 0.2714\n",
      "Epoch 026 | Loss: 0.2603\n",
      "\tVal loss: 0.4817 \n",
      "Epoch 027 | Loss: 0.2542\n",
      "Epoch 028 | Loss: 0.2507\n",
      "Epoch 029 | Loss: 0.2421\n",
      "Epoch 030 | Loss: 0.2346\n",
      "Epoch 031 | Loss: 0.2273\n",
      "\tVal loss: 0.5050 \n",
      "Epoch 032 | Loss: 0.2223\n",
      "Epoch 033 | Loss: 0.2227\n",
      "Epoch 034 | Loss: 0.2194\n",
      "Epoch 035 | Loss: 0.2174\n",
      "Epoch 036 | Loss: 0.2243\n",
      "\tVal loss: 0.4440 saved!\n",
      "Epoch 037 | Loss: 0.2210\n",
      "Epoch 038 | Loss: 0.2213\n",
      "Epoch 039 | Loss: 0.2052\n",
      "Epoch 040 | Loss: 0.1953\n",
      "Epoch 041 | Loss: 0.1903\n",
      "\tVal loss: 0.4825 \n",
      "Epoch 042 | Loss: 0.1890\n",
      "Epoch 043 | Loss: 0.1898\n",
      "Epoch 044 | Loss: 0.1899\n",
      "Epoch 045 | Loss: 0.1955\n",
      "Epoch 046 | Loss: 0.1985\n",
      "\tVal loss: 0.4381 saved!\n",
      "Epoch 047 | Loss: 0.1951\n",
      "Epoch 048 | Loss: 0.1920\n",
      "Epoch 049 | Loss: 0.1901\n",
      "Epoch 050 | Loss: 0.1884\n",
      "Epoch 051 | Loss: 0.1803\n",
      "\tVal loss: 0.4553 \n",
      "Epoch 052 | Loss: 0.1699\n",
      "Epoch 053 | Loss: 0.1607\n",
      "Epoch 054 | Loss: 0.1565\n",
      "Epoch 055 | Loss: 0.1562\n",
      "Epoch 056 | Loss: 0.1583\n",
      "\tVal loss: 0.4474 saved!\n",
      "Epoch 057 | Loss: 0.1584\n",
      "Epoch 058 | Loss: 0.1570\n",
      "Epoch 059 | Loss: 0.1536\n",
      "Epoch 060 | Loss: 0.1543\n",
      "Epoch 061 | Loss: 0.1528\n",
      "\tVal loss: 0.4544 \n",
      "Epoch 062 | Loss: 0.1530\n",
      "Epoch 063 | Loss: 0.1534\n",
      "Epoch 064 | Loss: 0.1536\n",
      "Epoch 065 | Loss: 0.1554\n",
      "Epoch 066 | Loss: 0.1585\n",
      "\tVal loss: 0.4651 \n",
      "Epoch 067 | Loss: 0.1628\n",
      "Epoch 068 | Loss: 0.1691\n",
      "Epoch 069 | Loss: 0.1737\n",
      "Epoch 070 | Loss: 0.1706\n",
      "Epoch 071 | Loss: 0.1539\n",
      "\tVal loss: 0.4318 saved!\n",
      "Epoch 072 | Loss: 0.1395\n",
      "Epoch 073 | Loss: 0.1338\n",
      "Epoch 074 | Loss: 0.1309\n",
      "Epoch 075 | Loss: 0.1297\n",
      "Epoch 076 | Loss: 0.1311\n",
      "\tVal loss: 0.4348 saved!\n",
      "Epoch 077 | Loss: 0.1330\n",
      "Epoch 078 | Loss: 0.1343\n",
      "Epoch 079 | Loss: 0.1385\n",
      "Epoch 080 | Loss: 0.1396\n",
      "Epoch 081 | Loss: 0.1368\n",
      "\tVal loss: 0.4225 saved!\n",
      "Epoch 082 | Loss: 0.1332\n",
      "Epoch 083 | Loss: 0.1313\n",
      "Epoch 084 | Loss: 0.1310\n",
      "Epoch 085 | Loss: 0.1325\n",
      "Epoch 086 | Loss: 0.1357\n",
      "\tVal loss: 0.4324 saved!\n",
      "Epoch 087 | Loss: 0.1379\n",
      "Epoch 088 | Loss: 0.1429\n",
      "Epoch 089 | Loss: 0.1504\n",
      "Epoch 090 | Loss: 0.1449\n",
      "Epoch 091 | Loss: 0.1334\n",
      "\tVal loss: 0.4205 saved!\n",
      "Epoch 092 | Loss: 0.1195\n",
      "Epoch 093 | Loss: 0.1155\n",
      "Epoch 094 | Loss: 0.1137\n",
      "Epoch 095 | Loss: 0.1138\n",
      "Epoch 096 | Loss: 0.1158\n",
      "\tVal loss: 0.4465 saved!\n",
      "Epoch 097 | Loss: 0.1157\n",
      "Epoch 098 | Loss: 0.1154\n",
      "Epoch 099 | Loss: 0.1139\n",
      "Epoch 100 | Loss: 0.1164\n"
     ]
    }
   ],
   "source": [
    "resnet18 = Network(18, 100, 3).to(device)\n",
    "optimiser = optim.Adam(resnet18.parameters())\n",
    "_ = train(resnet18, surge_prediction_metric, optimiser, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-22T02:34:32.316958Z",
     "start_time": "2022-03-22T02:31:00.746544Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | Loss: 1.0114\n",
      "\tVal loss: 0.8565\n",
      "Epoch 002 | Loss: 0.7872\n",
      "Epoch 003 | Loss: 0.6887\n",
      "Epoch 004 | Loss: 0.6610\n",
      "Epoch 005 | Loss: 0.6327\n",
      "Epoch 006 | Loss: 0.6371\n",
      "\tVal loss: 0.6249\n",
      "Epoch 007 | Loss: 0.6212\n",
      "Epoch 008 | Loss: 0.6068\n",
      "Epoch 009 | Loss: 0.6165\n",
      "Epoch 010 | Loss: 0.6224\n",
      "Epoch 011 | Loss: 0.6105\n",
      "\tVal loss: 0.5638\n",
      "Epoch 012 | Loss: 0.5944\n",
      "Epoch 013 | Loss: 0.5824\n",
      "Epoch 014 | Loss: 0.5961\n",
      "Epoch 015 | Loss: 0.6069\n",
      "Epoch 016 | Loss: 0.6043\n",
      "\tVal loss: 0.5853\n",
      "Epoch 017 | Loss: 0.6250\n",
      "Epoch 018 | Loss: 0.6136\n",
      "Epoch 019 | Loss: 0.5964\n",
      "Epoch 020 | Loss: 0.5831\n",
      "Epoch 021 | Loss: 0.5742\n",
      "\tVal loss: 0.5372\n",
      "Epoch 022 | Loss: 0.5690\n",
      "Epoch 023 | Loss: 0.5690\n",
      "Epoch 024 | Loss: 0.5611\n",
      "Epoch 025 | Loss: 0.5555\n",
      "Epoch 026 | Loss: 0.5572\n",
      "\tVal loss: 0.5285\n",
      "Epoch 027 | Loss: 0.5504\n",
      "Epoch 028 | Loss: 0.5456\n",
      "Epoch 029 | Loss: 0.5428\n",
      "Epoch 030 | Loss: 0.5438\n",
      "Epoch 031 | Loss: 0.5359\n",
      "\tVal loss: 0.5194\n",
      "Epoch 032 | Loss: 0.5350\n",
      "Epoch 033 | Loss: 0.5391\n",
      "Epoch 034 | Loss: 0.5378\n",
      "Epoch 035 | Loss: 0.5353\n",
      "Epoch 036 | Loss: 0.5236\n",
      "\tVal loss: 0.5091\n",
      "Epoch 037 | Loss: 0.5343\n",
      "Epoch 038 | Loss: 0.5228\n",
      "Epoch 039 | Loss: 0.5134\n",
      "Epoch 040 | Loss: 0.5038\n",
      "Epoch 041 | Loss: 0.5091\n",
      "\tVal loss: 0.5087\n",
      "Epoch 042 | Loss: 0.5048\n",
      "Epoch 043 | Loss: 0.5029\n",
      "Epoch 044 | Loss: 0.5069\n",
      "Epoch 045 | Loss: 0.5176\n",
      "Epoch 046 | Loss: 0.5063\n",
      "\tVal loss: 0.5022\n",
      "Epoch 047 | Loss: 0.4927\n",
      "Epoch 048 | Loss: 0.4902\n",
      "Epoch 049 | Loss: 0.4756\n",
      "Epoch 050 | Loss: 0.4757\n",
      "Epoch 051 | Loss: 0.4811\n",
      "\tVal loss: 0.5174\n",
      "Epoch 052 | Loss: 0.4613\n",
      "Epoch 053 | Loss: 0.4551\n",
      "Epoch 054 | Loss: 0.4453\n",
      "Epoch 055 | Loss: 0.4454\n",
      "Epoch 056 | Loss: 0.4699\n",
      "\tVal loss: 0.5159\n",
      "Epoch 057 | Loss: 0.4451\n",
      "Epoch 058 | Loss: 0.4295\n",
      "Epoch 059 | Loss: 0.4250\n",
      "Epoch 060 | Loss: 0.4188\n",
      "Epoch 061 | Loss: 0.4235\n",
      "\tVal loss: 0.5258\n",
      "Epoch 062 | Loss: 0.4097\n",
      "Epoch 063 | Loss: 0.4041\n",
      "Epoch 064 | Loss: 0.4080\n",
      "Epoch 065 | Loss: 0.4053\n",
      "Epoch 066 | Loss: 0.4031\n",
      "\tVal loss: 0.5450\n",
      "Epoch 067 | Loss: 0.3965\n",
      "Epoch 068 | Loss: 0.4024\n",
      "Epoch 069 | Loss: 0.4346\n",
      "Epoch 070 | Loss: 0.4192\n",
      "Epoch 071 | Loss: 0.3962\n",
      "\tVal loss: 0.5357\n",
      "Epoch 072 | Loss: 0.3880\n",
      "Epoch 073 | Loss: 0.3841\n",
      "Epoch 074 | Loss: 0.3869\n",
      "Epoch 075 | Loss: 0.3659\n",
      "Epoch 076 | Loss: 0.3641\n",
      "\tVal loss: 0.5695\n",
      "Epoch 077 | Loss: 0.3711\n",
      "Epoch 078 | Loss: 0.3568\n",
      "Epoch 079 | Loss: 0.3532\n",
      "Epoch 080 | Loss: 0.3711\n",
      "Epoch 081 | Loss: 0.3694\n",
      "\tVal loss: 0.6356\n",
      "Epoch 082 | Loss: 0.3563\n",
      "Epoch 083 | Loss: 0.3570\n",
      "Epoch 084 | Loss: 0.3613\n",
      "Epoch 085 | Loss: 0.3595\n",
      "Epoch 086 | Loss: 0.3692\n",
      "\tVal loss: 0.5543\n",
      "Epoch 087 | Loss: 0.4021\n",
      "Epoch 088 | Loss: 0.3746\n",
      "Epoch 089 | Loss: 0.3617\n",
      "Epoch 090 | Loss: 0.3596\n",
      "Epoch 091 | Loss: 0.3416\n",
      "\tVal loss: 0.5611\n",
      "Epoch 092 | Loss: 0.3381\n",
      "Epoch 093 | Loss: 0.3265\n",
      "Epoch 094 | Loss: 0.3658\n",
      "Epoch 095 | Loss: 0.3371\n",
      "Epoch 096 | Loss: 0.3251\n",
      "\tVal loss: 0.5682\n",
      "Epoch 097 | Loss: 0.3363\n",
      "Epoch 098 | Loss: 0.3341\n",
      "Epoch 099 | Loss: 0.3149\n",
      "Epoch 100 | Loss: 0.2960\n"
     ]
    }
   ],
   "source": [
    "resnet34 = Network(34, 100).to(device)\n",
    "optimiser = optim.Adam(resnet34.parameters())\n",
    "_ = train(resnet34, surge_prediction_metric, optimiser, epochs=100)\n",
    "torch.save(resnet34.state_dict(), 'resnet34_100.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-22T07:20:51.993090Z",
     "start_time": "2022-03-22T07:17:20.543146Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | Loss: 0.8911\n",
      "\tVal loss: 0.7336 \n",
      "Epoch 002 | Loss: 0.7069\n",
      "Epoch 003 | Loss: 0.6554\n",
      "Epoch 004 | Loss: 0.6249\n",
      "Epoch 005 | Loss: 0.6086\n",
      "Epoch 006 | Loss: 0.5994\n",
      "\tVal loss: 0.7371 \n",
      "Epoch 007 | Loss: 0.5866\n",
      "Epoch 008 | Loss: 0.5561\n",
      "Epoch 009 | Loss: 0.5443\n",
      "Epoch 010 | Loss: 0.5294\n",
      "Epoch 011 | Loss: 0.5116\n",
      "\tVal loss: 0.5128 \n",
      "Epoch 012 | Loss: 0.5020\n",
      "Epoch 013 | Loss: 0.4874\n",
      "Epoch 014 | Loss: 0.4765\n",
      "Epoch 015 | Loss: 0.4684\n",
      "Epoch 016 | Loss: 0.4596\n",
      "\tVal loss: 0.4633 \n",
      "Epoch 017 | Loss: 0.4561\n",
      "Epoch 018 | Loss: 0.4556\n",
      "Epoch 019 | Loss: 0.4467\n",
      "Epoch 020 | Loss: 0.5093\n",
      "Epoch 021 | Loss: 0.4709\n",
      "\tVal loss: 0.4451 saved!\n",
      "Epoch 022 | Loss: 0.4588\n",
      "Epoch 023 | Loss: 0.4461\n",
      "Epoch 024 | Loss: 0.4440\n",
      "Epoch 025 | Loss: 0.4333\n",
      "Epoch 026 | Loss: 0.4214\n",
      "\tVal loss: 0.4474 saved!\n",
      "Epoch 027 | Loss: 0.4210\n",
      "Epoch 028 | Loss: 0.4221\n",
      "Epoch 029 | Loss: 0.4150\n",
      "Epoch 030 | Loss: 0.4219\n",
      "Epoch 031 | Loss: 0.4066\n",
      "\tVal loss: 0.4327 saved!\n",
      "Epoch 032 | Loss: 0.3924\n",
      "Epoch 033 | Loss: 0.3778\n",
      "Epoch 034 | Loss: 0.4019\n",
      "Epoch 035 | Loss: 0.3975\n",
      "Epoch 036 | Loss: 0.3737\n",
      "\tVal loss: 0.4382 saved!\n",
      "Epoch 037 | Loss: 0.3649\n",
      "Epoch 038 | Loss: 0.3532\n",
      "Epoch 039 | Loss: 0.3701\n",
      "Epoch 040 | Loss: 0.3476\n",
      "Epoch 041 | Loss: 0.3540\n",
      "\tVal loss: 0.4510 \n",
      "Epoch 042 | Loss: 0.3347\n",
      "Epoch 043 | Loss: 0.3660\n",
      "Epoch 044 | Loss: 0.3612\n",
      "Epoch 045 | Loss: 0.3254\n",
      "Epoch 046 | Loss: 0.3570\n",
      "\tVal loss: 0.4483 saved!\n",
      "Epoch 047 | Loss: 0.3520\n",
      "Epoch 048 | Loss: 0.3225\n",
      "Epoch 049 | Loss: 0.2948\n",
      "Epoch 050 | Loss: 0.2801\n",
      "Epoch 051 | Loss: 0.2695\n",
      "\tVal loss: 0.4615 \n",
      "Epoch 052 | Loss: 0.2631\n",
      "Epoch 053 | Loss: 0.2686\n",
      "Epoch 054 | Loss: 0.2788\n",
      "Epoch 055 | Loss: 0.2651\n",
      "Epoch 056 | Loss: 0.2539\n",
      "\tVal loss: 0.4560 \n",
      "Epoch 057 | Loss: 0.2718\n",
      "Epoch 058 | Loss: 0.2683\n",
      "Epoch 059 | Loss: 0.2460\n",
      "Epoch 060 | Loss: 0.2398\n",
      "Epoch 061 | Loss: 0.2316\n",
      "\tVal loss: 0.5008 \n",
      "Epoch 062 | Loss: 0.2625\n",
      "Epoch 063 | Loss: 0.2498\n",
      "Epoch 064 | Loss: 0.2725\n",
      "Epoch 065 | Loss: 0.2288\n",
      "Epoch 066 | Loss: 0.2123\n",
      "\tVal loss: 0.4616 \n",
      "Epoch 067 | Loss: 0.2083\n",
      "Epoch 068 | Loss: 0.2136\n",
      "Epoch 069 | Loss: 0.2246\n",
      "Epoch 070 | Loss: 0.2228\n",
      "Epoch 071 | Loss: 0.2116\n",
      "\tVal loss: 0.4593 \n",
      "Epoch 072 | Loss: 0.2159\n",
      "Epoch 073 | Loss: 0.2315\n",
      "Epoch 074 | Loss: 0.2892\n",
      "Epoch 075 | Loss: 0.2105\n",
      "Epoch 076 | Loss: 0.1992\n",
      "\tVal loss: 0.4558 \n",
      "Epoch 077 | Loss: 0.1839\n",
      "Epoch 078 | Loss: 0.1860\n",
      "Epoch 079 | Loss: 0.1956\n",
      "Epoch 080 | Loss: 0.1764\n",
      "Epoch 081 | Loss: 0.2226\n",
      "\tVal loss: 0.5424 \n",
      "Epoch 082 | Loss: 0.2186\n",
      "Epoch 083 | Loss: 0.1764\n",
      "Epoch 084 | Loss: 0.1631\n",
      "Epoch 085 | Loss: 0.1752\n",
      "Epoch 086 | Loss: 0.1662\n",
      "\tVal loss: 0.4826 \n",
      "Epoch 087 | Loss: 0.1563\n",
      "Epoch 088 | Loss: 0.2570\n",
      "Epoch 089 | Loss: 0.2061\n",
      "Epoch 090 | Loss: 0.1666\n",
      "Epoch 091 | Loss: 0.1809\n",
      "\tVal loss: 0.4520 \n",
      "Epoch 092 | Loss: 0.1711\n",
      "Epoch 093 | Loss: 0.1577\n",
      "Epoch 094 | Loss: 0.1977\n",
      "Epoch 095 | Loss: 0.1647\n",
      "Epoch 096 | Loss: 0.2942\n",
      "\tVal loss: 0.5066 \n",
      "Epoch 097 | Loss: 0.2597\n",
      "Epoch 098 | Loss: 0.3100\n",
      "Epoch 099 | Loss: 0.2088\n",
      "Epoch 100 | Loss: 0.1827\n"
     ]
    }
   ],
   "source": [
    "resnet34 = Network(34, 100, 1).to(device)\n",
    "optimiser = optim.Adam(resnet34.parameters())\n",
    "_ = train(resnet34, surge_prediction_metric, optimiser, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-22T07:37:02.350213Z",
     "start_time": "2022-03-22T07:29:54.679593Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | Loss: 0.8894\n",
      "\tVal loss: 0.7828 \n",
      "Epoch 002 | Loss: 0.7014\n",
      "Epoch 003 | Loss: 0.6376\n",
      "Epoch 004 | Loss: 0.6167\n",
      "Epoch 005 | Loss: 0.5860\n",
      "Epoch 006 | Loss: 0.5637\n",
      "\tVal loss: 0.5445 \n",
      "Epoch 007 | Loss: 0.5408\n",
      "Epoch 008 | Loss: 0.5252\n",
      "Epoch 009 | Loss: 0.5099\n",
      "Epoch 010 | Loss: 0.4965\n",
      "Epoch 011 | Loss: 0.4987\n",
      "\tVal loss: 0.5420 \n",
      "Epoch 012 | Loss: 0.4823\n",
      "Epoch 013 | Loss: 0.4729\n",
      "Epoch 014 | Loss: 0.4633\n",
      "Epoch 015 | Loss: 0.4982\n",
      "Epoch 016 | Loss: 0.4586\n",
      "\tVal loss: 0.4371 saved!\n",
      "Epoch 017 | Loss: 0.4475\n",
      "Epoch 018 | Loss: 0.4341\n",
      "Epoch 019 | Loss: 0.4214\n",
      "Epoch 020 | Loss: 0.4157\n",
      "Epoch 021 | Loss: 0.4021\n",
      "\tVal loss: 0.4246 saved!\n",
      "Epoch 022 | Loss: 0.3982\n",
      "Epoch 023 | Loss: 0.3986\n",
      "Epoch 024 | Loss: 0.3900\n",
      "Epoch 025 | Loss: 0.3933\n",
      "Epoch 026 | Loss: 0.3723\n",
      "\tVal loss: 0.4372 saved!\n",
      "Epoch 027 | Loss: 0.3564\n",
      "Epoch 028 | Loss: 0.3370\n",
      "Epoch 029 | Loss: 0.3289\n",
      "Epoch 030 | Loss: 0.3295\n",
      "Epoch 031 | Loss: 0.3450\n",
      "\tVal loss: 0.4403 saved!\n",
      "Epoch 032 | Loss: 0.3312\n",
      "Epoch 033 | Loss: 0.3228\n",
      "Epoch 034 | Loss: 0.3191\n",
      "Epoch 035 | Loss: 0.3171\n",
      "Epoch 036 | Loss: 0.3082\n",
      "\tVal loss: 0.4520 \n",
      "Epoch 037 | Loss: 0.3060\n",
      "Epoch 038 | Loss: 0.2805\n",
      "Epoch 039 | Loss: 0.2751\n",
      "Epoch 040 | Loss: 0.2655\n",
      "Epoch 041 | Loss: 0.2523\n",
      "\tVal loss: 0.4818 \n",
      "Epoch 042 | Loss: 0.2471\n",
      "Epoch 043 | Loss: 0.2479\n",
      "Epoch 044 | Loss: 0.2456\n",
      "Epoch 045 | Loss: 0.2413\n",
      "Epoch 046 | Loss: 0.2419\n",
      "\tVal loss: 0.4882 \n",
      "Epoch 047 | Loss: 0.2705\n",
      "Epoch 048 | Loss: 0.2582\n",
      "Epoch 049 | Loss: 0.3705\n",
      "Epoch 050 | Loss: 0.3467\n",
      "Epoch 051 | Loss: 0.2643\n",
      "\tVal loss: 0.4501 \n",
      "Epoch 052 | Loss: 0.2482\n",
      "Epoch 053 | Loss: 0.2184\n",
      "Epoch 054 | Loss: 0.2060\n",
      "Epoch 055 | Loss: 0.2017\n",
      "Epoch 056 | Loss: 0.1897\n",
      "\tVal loss: 0.4564 \n",
      "Epoch 057 | Loss: 0.1945\n",
      "Epoch 058 | Loss: 0.1906\n",
      "Epoch 059 | Loss: 0.1921\n",
      "Epoch 060 | Loss: 0.1881\n",
      "Epoch 061 | Loss: 0.1952\n",
      "\tVal loss: 0.4548 \n",
      "Epoch 062 | Loss: 0.2495\n",
      "Epoch 063 | Loss: 0.1990\n",
      "Epoch 064 | Loss: 0.1897\n",
      "Epoch 065 | Loss: 0.1871\n",
      "Epoch 066 | Loss: 0.1790\n",
      "\tVal loss: 0.5156 \n",
      "Epoch 067 | Loss: 0.1849\n",
      "Epoch 068 | Loss: 0.1785\n",
      "Epoch 069 | Loss: 0.1830\n",
      "Epoch 070 | Loss: 0.1729\n",
      "Epoch 071 | Loss: 0.1651\n",
      "\tVal loss: 0.4512 \n",
      "Epoch 072 | Loss: 0.1757\n",
      "Epoch 073 | Loss: 0.1717\n",
      "Epoch 074 | Loss: 0.1718\n",
      "Epoch 075 | Loss: 0.1927\n",
      "Epoch 076 | Loss: 0.1603\n",
      "\tVal loss: 0.4508 \n",
      "Epoch 077 | Loss: 0.1612\n",
      "Epoch 078 | Loss: 0.1655\n",
      "Epoch 079 | Loss: 0.1470\n",
      "Epoch 080 | Loss: 0.1633\n",
      "Epoch 081 | Loss: 0.1604\n",
      "\tVal loss: 0.4941 \n",
      "Epoch 082 | Loss: 0.1496\n",
      "Epoch 083 | Loss: 0.1893\n",
      "Epoch 084 | Loss: 0.2230\n",
      "Epoch 085 | Loss: 0.1684\n",
      "Epoch 086 | Loss: 0.1484\n",
      "\tVal loss: 0.4648 \n",
      "Epoch 087 | Loss: 0.1436\n",
      "Epoch 088 | Loss: 0.1407\n",
      "Epoch 089 | Loss: 0.1455\n",
      "Epoch 090 | Loss: 0.1462\n",
      "Epoch 091 | Loss: 0.1547\n",
      "\tVal loss: 0.4770 \n",
      "Epoch 092 | Loss: 0.1588\n",
      "Epoch 093 | Loss: 0.1602\n",
      "Epoch 094 | Loss: 0.1592\n",
      "Epoch 095 | Loss: 0.1847\n",
      "Epoch 096 | Loss: 0.2076\n",
      "\tVal loss: 0.4482 saved!\n",
      "Epoch 097 | Loss: 0.1520\n",
      "Epoch 098 | Loss: 0.1464\n",
      "Epoch 099 | Loss: 0.1468\n",
      "Epoch 100 | Loss: 0.1660\n",
      "Epoch 101 | Loss: 0.1669\n",
      "\tVal loss: 0.4474 saved!\n",
      "Epoch 102 | Loss: 0.1377\n",
      "Epoch 103 | Loss: 0.1307\n",
      "Epoch 104 | Loss: 0.1204\n",
      "Epoch 105 | Loss: 0.1169\n",
      "Epoch 106 | Loss: 0.1182\n",
      "\tVal loss: 0.4511 \n",
      "Epoch 107 | Loss: 0.1200\n",
      "Epoch 108 | Loss: 0.1313\n",
      "Epoch 109 | Loss: 0.1197\n",
      "Epoch 110 | Loss: 0.1138\n",
      "Epoch 111 | Loss: 0.1287\n",
      "\tVal loss: 0.4513 \n",
      "Epoch 112 | Loss: 0.1330\n",
      "Epoch 113 | Loss: 0.1214\n",
      "Epoch 114 | Loss: 0.1377\n",
      "Epoch 115 | Loss: 0.1245\n",
      "Epoch 116 | Loss: 0.1186\n",
      "\tVal loss: 0.4504 \n",
      "Epoch 117 | Loss: 0.1144\n",
      "Epoch 118 | Loss: 0.1490\n",
      "Epoch 119 | Loss: 0.1412\n",
      "Epoch 120 | Loss: 0.1246\n",
      "Epoch 121 | Loss: 0.1316\n",
      "\tVal loss: 0.4561 \n",
      "Epoch 122 | Loss: 0.1392\n",
      "Epoch 123 | Loss: 0.1194\n",
      "Epoch 124 | Loss: 0.1321\n",
      "Epoch 125 | Loss: 0.1111\n",
      "Epoch 126 | Loss: 0.1158\n",
      "\tVal loss: 0.4394 saved!\n",
      "Epoch 127 | Loss: 0.1056\n",
      "Epoch 128 | Loss: 0.1020\n",
      "Epoch 129 | Loss: 0.1112\n",
      "Epoch 130 | Loss: 0.1184\n",
      "Epoch 131 | Loss: 0.1207\n",
      "\tVal loss: 0.4424 saved!\n",
      "Epoch 132 | Loss: 0.1247\n",
      "Epoch 133 | Loss: 0.1160\n",
      "Epoch 134 | Loss: 0.1062\n",
      "Epoch 135 | Loss: 0.1075\n",
      "Epoch 136 | Loss: 0.1267\n",
      "\tVal loss: 0.4486 saved!\n",
      "Epoch 137 | Loss: 0.1267\n",
      "Epoch 138 | Loss: 0.1220\n",
      "Epoch 139 | Loss: 0.1176\n",
      "Epoch 140 | Loss: 0.1161\n",
      "Epoch 141 | Loss: 0.1128\n",
      "\tVal loss: 0.5272 \n",
      "Epoch 142 | Loss: 0.1300\n",
      "Epoch 143 | Loss: 0.1226\n",
      "Epoch 144 | Loss: 0.1235\n",
      "Epoch 145 | Loss: 0.1088\n",
      "Epoch 146 | Loss: 0.1142\n",
      "\tVal loss: 0.4373 saved!\n",
      "Epoch 147 | Loss: 0.1038\n",
      "Epoch 148 | Loss: 0.1008\n",
      "Epoch 149 | Loss: 0.1014\n",
      "Epoch 150 | Loss: 0.1042\n",
      "Epoch 151 | Loss: 0.1080\n",
      "\tVal loss: 0.4680 \n",
      "Epoch 152 | Loss: 0.1166\n",
      "Epoch 153 | Loss: 0.1084\n",
      "Epoch 154 | Loss: 0.1121\n",
      "Epoch 155 | Loss: 0.1347\n",
      "Epoch 156 | Loss: 0.1047\n",
      "\tVal loss: 0.4734 \n",
      "Epoch 157 | Loss: 0.1056\n",
      "Epoch 158 | Loss: 0.1372\n",
      "Epoch 159 | Loss: 0.1474\n",
      "Epoch 160 | Loss: 0.1048\n",
      "Epoch 161 | Loss: 0.0969\n",
      "\tVal loss: 0.4885 \n",
      "Epoch 162 | Loss: 0.1004\n",
      "Epoch 163 | Loss: 0.1057\n",
      "Epoch 164 | Loss: 0.1040\n",
      "Epoch 165 | Loss: 0.1091\n",
      "Epoch 166 | Loss: 0.1065\n",
      "\tVal loss: 0.4628 \n",
      "Epoch 167 | Loss: 0.0993\n",
      "Epoch 168 | Loss: 0.0989\n",
      "Epoch 169 | Loss: 0.1016\n",
      "Epoch 170 | Loss: 0.1025\n",
      "Epoch 171 | Loss: 0.1000\n",
      "\tVal loss: 0.4785 \n",
      "Epoch 172 | Loss: 0.0990\n",
      "Epoch 173 | Loss: 0.1225\n",
      "Epoch 174 | Loss: 0.1037\n",
      "Epoch 175 | Loss: 0.1180\n",
      "Epoch 176 | Loss: 0.1279\n",
      "\tVal loss: 0.4398 saved!\n",
      "Epoch 177 | Loss: 0.0945\n",
      "Epoch 178 | Loss: 0.0862\n",
      "Epoch 179 | Loss: 0.0897\n",
      "Epoch 180 | Loss: 0.0850\n",
      "Epoch 181 | Loss: 0.0810\n",
      "\tVal loss: 0.4400 saved!\n",
      "Epoch 182 | Loss: 0.0827\n",
      "Epoch 183 | Loss: 0.0857\n",
      "Epoch 184 | Loss: 0.0938\n",
      "Epoch 185 | Loss: 0.1014\n",
      "Epoch 186 | Loss: 0.1015\n",
      "\tVal loss: 0.4982 \n",
      "Epoch 187 | Loss: 0.0974\n",
      "Epoch 188 | Loss: 0.0955\n",
      "Epoch 189 | Loss: 0.1183\n",
      "Epoch 190 | Loss: 0.1087\n",
      "Epoch 191 | Loss: 0.1076\n",
      "\tVal loss: 0.4670 \n",
      "Epoch 192 | Loss: 0.1330\n",
      "Epoch 193 | Loss: 0.0898\n",
      "Epoch 194 | Loss: 0.0811\n",
      "Epoch 195 | Loss: 0.0975\n",
      "Epoch 196 | Loss: 0.0901\n",
      "\tVal loss: 0.4427 saved!\n",
      "Epoch 197 | Loss: 0.0816\n",
      "Epoch 198 | Loss: 0.0795\n",
      "Epoch 199 | Loss: 0.0761\n",
      "Epoch 200 | Loss: 0.0755\n"
     ]
    }
   ],
   "source": [
    "resnet34 = Network(34, 100, 2).to(device)\n",
    "optimiser = optim.Adam(resnet34.parameters())\n",
    "_ = train(resnet34, surge_prediction_metric, optimiser, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-22T07:44:26.012239Z",
     "start_time": "2022-03-22T07:37:02.351390Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | Loss: 0.8998\n",
      "\tVal loss: 0.7035 \n",
      "Epoch 002 | Loss: 0.6945\n",
      "Epoch 003 | Loss: 0.6601\n",
      "Epoch 004 | Loss: 0.6271\n",
      "Epoch 005 | Loss: 0.6099\n",
      "Epoch 006 | Loss: 0.6107\n",
      "\tVal loss: 0.5601 \n",
      "Epoch 007 | Loss: 0.5913\n",
      "Epoch 008 | Loss: 0.5803\n",
      "Epoch 009 | Loss: 0.5692\n",
      "Epoch 010 | Loss: 0.5508\n",
      "Epoch 011 | Loss: 0.5321\n",
      "\tVal loss: 0.4750 \n",
      "Epoch 012 | Loss: 0.5431\n",
      "Epoch 013 | Loss: 0.5410\n",
      "Epoch 014 | Loss: 0.5149\n",
      "Epoch 015 | Loss: 0.5043\n",
      "Epoch 016 | Loss: 0.4893\n",
      "\tVal loss: 0.4589 \n",
      "Epoch 017 | Loss: 0.5016\n",
      "Epoch 018 | Loss: 0.5056\n",
      "Epoch 019 | Loss: 0.4964\n",
      "Epoch 020 | Loss: 0.4883\n",
      "Epoch 021 | Loss: 0.4793\n",
      "\tVal loss: 0.4497 saved!\n",
      "Epoch 022 | Loss: 0.4734\n",
      "Epoch 023 | Loss: 0.4638\n",
      "Epoch 024 | Loss: 0.4589\n",
      "Epoch 025 | Loss: 0.4747\n",
      "Epoch 026 | Loss: 0.4715\n",
      "\tVal loss: 0.5075 \n",
      "Epoch 027 | Loss: 0.4728\n",
      "Epoch 028 | Loss: 0.4654\n",
      "Epoch 029 | Loss: 0.4485\n",
      "Epoch 030 | Loss: 0.4390\n",
      "Epoch 031 | Loss: 0.4337\n",
      "\tVal loss: 0.4535 \n",
      "Epoch 032 | Loss: 0.4253\n",
      "Epoch 033 | Loss: 0.4559\n",
      "Epoch 034 | Loss: 0.4734\n",
      "Epoch 035 | Loss: 0.4536\n",
      "Epoch 036 | Loss: 0.4422\n",
      "\tVal loss: 0.4325 saved!\n",
      "Epoch 037 | Loss: 0.4231\n",
      "Epoch 038 | Loss: 0.4623\n",
      "Epoch 039 | Loss: 0.4579\n",
      "Epoch 040 | Loss: 0.4610\n",
      "Epoch 041 | Loss: 0.4526\n",
      "\tVal loss: 0.4666 \n",
      "Epoch 042 | Loss: 0.4965\n",
      "Epoch 043 | Loss: 0.4756\n",
      "Epoch 044 | Loss: 0.4651\n",
      "Epoch 045 | Loss: 0.4740\n",
      "Epoch 046 | Loss: 0.4845\n",
      "\tVal loss: 0.4270 saved!\n",
      "Epoch 047 | Loss: 0.4480\n",
      "Epoch 048 | Loss: 0.4420\n",
      "Epoch 049 | Loss: 0.4361\n",
      "Epoch 050 | Loss: 0.4463\n",
      "Epoch 051 | Loss: 0.4207\n",
      "\tVal loss: 0.4261 saved!\n",
      "Epoch 052 | Loss: 0.4183\n",
      "Epoch 053 | Loss: 0.4065\n",
      "Epoch 054 | Loss: 0.4165\n",
      "Epoch 055 | Loss: 0.4688\n",
      "Epoch 056 | Loss: 0.4545\n",
      "\tVal loss: 0.4479 saved!\n",
      "Epoch 057 | Loss: 0.4279\n",
      "Epoch 058 | Loss: 0.4208\n",
      "Epoch 059 | Loss: 0.4039\n",
      "Epoch 060 | Loss: 0.3895\n",
      "Epoch 061 | Loss: 0.3770\n",
      "\tVal loss: 0.4207 saved!\n",
      "Epoch 062 | Loss: 0.3664\n",
      "Epoch 063 | Loss: 0.3504\n",
      "Epoch 064 | Loss: 0.3418\n",
      "Epoch 065 | Loss: 0.3994\n",
      "Epoch 066 | Loss: 0.3829\n",
      "\tVal loss: 0.4374 saved!\n",
      "Epoch 067 | Loss: 0.3437\n",
      "Epoch 068 | Loss: 0.3299\n",
      "Epoch 069 | Loss: 0.3133\n",
      "Epoch 070 | Loss: 0.2891\n",
      "Epoch 071 | Loss: 0.3808\n",
      "\tVal loss: 0.4674 \n",
      "Epoch 072 | Loss: 0.4401\n",
      "Epoch 073 | Loss: 0.4534\n",
      "Epoch 074 | Loss: 0.5019\n",
      "Epoch 075 | Loss: 0.4669\n",
      "Epoch 076 | Loss: 0.4236\n",
      "\tVal loss: 0.4224 saved!\n",
      "Epoch 077 | Loss: 0.4615\n",
      "Epoch 078 | Loss: 0.4077\n",
      "Epoch 079 | Loss: 0.3734\n",
      "Epoch 080 | Loss: 0.3537\n",
      "Epoch 081 | Loss: 0.3284\n",
      "\tVal loss: 0.4484 saved!\n",
      "Epoch 082 | Loss: 0.3128\n",
      "Epoch 083 | Loss: 0.2979\n",
      "Epoch 084 | Loss: 0.2763\n",
      "Epoch 085 | Loss: 0.2700\n",
      "Epoch 086 | Loss: 0.2690\n",
      "\tVal loss: 0.4921 \n",
      "Epoch 087 | Loss: 0.2637\n",
      "Epoch 088 | Loss: 0.2541\n",
      "Epoch 089 | Loss: 0.2463\n",
      "Epoch 090 | Loss: 0.2495\n",
      "Epoch 091 | Loss: 0.2445\n",
      "\tVal loss: 0.4688 \n",
      "Epoch 092 | Loss: 0.2401\n",
      "Epoch 093 | Loss: 0.2449\n",
      "Epoch 094 | Loss: 0.2525\n",
      "Epoch 095 | Loss: 0.2416\n",
      "Epoch 096 | Loss: 0.2267\n",
      "\tVal loss: 0.4903 \n",
      "Epoch 097 | Loss: 0.2138\n",
      "Epoch 098 | Loss: 0.2032\n",
      "Epoch 099 | Loss: 0.1956\n",
      "Epoch 100 | Loss: 0.1932\n",
      "Epoch 101 | Loss: 0.1924\n",
      "\tVal loss: 0.5136 \n",
      "Epoch 102 | Loss: 0.1957\n",
      "Epoch 103 | Loss: 0.2010\n",
      "Epoch 104 | Loss: 0.1989\n",
      "Epoch 105 | Loss: 0.2004\n",
      "Epoch 106 | Loss: 0.1968\n",
      "\tVal loss: 0.4822 \n",
      "Epoch 107 | Loss: 0.2071\n",
      "Epoch 108 | Loss: 0.2044\n",
      "Epoch 109 | Loss: 0.1963\n",
      "Epoch 110 | Loss: 0.1956\n",
      "Epoch 111 | Loss: 0.2046\n",
      "\tVal loss: 0.4735 \n",
      "Epoch 112 | Loss: 0.1861\n",
      "Epoch 113 | Loss: 0.1806\n",
      "Epoch 114 | Loss: 0.1881\n",
      "Epoch 115 | Loss: 0.1838\n",
      "Epoch 116 | Loss: 0.1785\n",
      "\tVal loss: 0.4838 \n",
      "Epoch 117 | Loss: 0.1885\n",
      "Epoch 118 | Loss: 0.1947\n",
      "Epoch 119 | Loss: 0.1719\n",
      "Epoch 120 | Loss: 0.1797\n",
      "Epoch 121 | Loss: 0.1816\n",
      "\tVal loss: 0.5070 \n",
      "Epoch 122 | Loss: 0.1757\n",
      "Epoch 123 | Loss: 0.1731\n",
      "Epoch 124 | Loss: 0.2560\n",
      "Epoch 125 | Loss: 0.2613\n",
      "Epoch 126 | Loss: 0.2073\n",
      "\tVal loss: 0.5066 \n",
      "Epoch 127 | Loss: 0.2532\n",
      "Epoch 128 | Loss: 0.2023\n",
      "Epoch 129 | Loss: 0.2051\n",
      "Epoch 130 | Loss: 0.1923\n",
      "Epoch 131 | Loss: 0.1718\n",
      "\tVal loss: 0.4737 \n",
      "Epoch 132 | Loss: 0.1693\n",
      "Epoch 133 | Loss: 0.1714\n",
      "Epoch 134 | Loss: 0.1906\n",
      "Epoch 135 | Loss: 0.1883\n",
      "Epoch 136 | Loss: 0.1800\n",
      "\tVal loss: 0.4681 \n",
      "Epoch 137 | Loss: 0.1970\n",
      "Epoch 138 | Loss: 0.1890\n",
      "Epoch 139 | Loss: 0.1845\n",
      "Epoch 140 | Loss: 0.1811\n",
      "Epoch 141 | Loss: 0.1737\n",
      "\tVal loss: 0.4620 \n",
      "Epoch 142 | Loss: 0.1708\n",
      "Epoch 143 | Loss: 0.1662\n",
      "Epoch 144 | Loss: 0.1665\n",
      "Epoch 145 | Loss: 0.1818\n",
      "Epoch 146 | Loss: 0.1638\n",
      "\tVal loss: 0.4717 \n",
      "Epoch 147 | Loss: 0.3122\n",
      "Epoch 148 | Loss: 0.2091\n",
      "Epoch 149 | Loss: 0.1752\n",
      "Epoch 150 | Loss: 0.1774\n",
      "Epoch 151 | Loss: 0.1540\n",
      "\tVal loss: 0.4626 \n",
      "Epoch 152 | Loss: 0.1511\n",
      "Epoch 153 | Loss: 0.1506\n",
      "Epoch 154 | Loss: 0.1520\n",
      "Epoch 155 | Loss: 0.1597\n",
      "Epoch 156 | Loss: 0.1564\n",
      "\tVal loss: 0.4712 \n",
      "Epoch 157 | Loss: 0.1515\n",
      "Epoch 158 | Loss: 0.1550\n",
      "Epoch 159 | Loss: 0.1543\n",
      "Epoch 160 | Loss: 0.1475\n",
      "Epoch 161 | Loss: 0.1436\n",
      "\tVal loss: 0.4599 \n",
      "Epoch 162 | Loss: 0.2335\n",
      "Epoch 163 | Loss: 0.2007\n",
      "Epoch 164 | Loss: 0.1586\n",
      "Epoch 165 | Loss: 0.1529\n",
      "Epoch 166 | Loss: 0.1447\n",
      "\tVal loss: 0.4657 \n",
      "Epoch 167 | Loss: 0.1403\n",
      "Epoch 168 | Loss: 0.1410\n",
      "Epoch 169 | Loss: 0.1456\n",
      "Epoch 170 | Loss: 0.1508\n",
      "Epoch 171 | Loss: 0.1545\n",
      "\tVal loss: 0.5037 \n",
      "Epoch 172 | Loss: 0.1535\n",
      "Epoch 173 | Loss: 0.1518\n",
      "Epoch 174 | Loss: 0.1543\n",
      "Epoch 175 | Loss: 0.1545\n",
      "Epoch 176 | Loss: 0.1557\n",
      "\tVal loss: 0.4677 \n",
      "Epoch 177 | Loss: 0.1587\n",
      "Epoch 178 | Loss: 0.1566\n",
      "Epoch 179 | Loss: 0.1523\n",
      "Epoch 180 | Loss: 0.1432\n",
      "Epoch 181 | Loss: 0.1379\n",
      "\tVal loss: 0.4723 \n",
      "Epoch 182 | Loss: 0.1342\n",
      "Epoch 183 | Loss: 0.1312\n",
      "Epoch 184 | Loss: 0.1313\n",
      "Epoch 185 | Loss: 0.1313\n",
      "Epoch 186 | Loss: 0.1316\n",
      "\tVal loss: 0.4661 \n",
      "Epoch 187 | Loss: 0.1311\n",
      "Epoch 188 | Loss: 0.1296\n",
      "Epoch 189 | Loss: 0.1292\n",
      "Epoch 190 | Loss: 0.1308\n",
      "Epoch 191 | Loss: 0.1339\n",
      "\tVal loss: 0.4575 \n",
      "Epoch 192 | Loss: 0.1351\n",
      "Epoch 193 | Loss: 0.1379\n",
      "Epoch 194 | Loss: 0.1409\n",
      "Epoch 195 | Loss: 0.1420\n",
      "Epoch 196 | Loss: 0.1438\n",
      "\tVal loss: 0.4974 \n",
      "Epoch 197 | Loss: 0.1410\n",
      "Epoch 198 | Loss: 0.1405\n",
      "Epoch 199 | Loss: 0.1388\n",
      "Epoch 200 | Loss: 0.1346\n"
     ]
    }
   ],
   "source": [
    "resnet34 = Network(34, 100, 3).to(device)\n",
    "optimiser = optim.Adam(resnet34.parameters())\n",
    "_ = train(resnet34, surge_prediction_metric, optimiser, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-22T08:34:24.919638Z",
     "start_time": "2022-03-22T08:30:52.602688Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | Loss: 0.9919\n",
      "\tVal loss: 0.8521 \n",
      "Epoch 002 | Loss: 0.8527\n",
      "Epoch 003 | Loss: 0.7272\n",
      "Epoch 004 | Loss: 0.6936\n",
      "Epoch 005 | Loss: 0.6756\n",
      "Epoch 006 | Loss: 0.6442\n",
      "\tVal loss: 0.6101 \n",
      "Epoch 007 | Loss: 0.6248\n",
      "Epoch 008 | Loss: 0.6072\n",
      "Epoch 009 | Loss: 0.5948\n",
      "Epoch 010 | Loss: 0.6106\n",
      "Epoch 011 | Loss: 0.6109\n",
      "\tVal loss: 0.5679 \n",
      "Epoch 012 | Loss: 0.6033\n",
      "Epoch 013 | Loss: 0.5683\n",
      "Epoch 014 | Loss: 0.5599\n",
      "Epoch 015 | Loss: 0.5534\n",
      "Epoch 016 | Loss: 0.5392\n",
      "\tVal loss: 0.4883 \n",
      "Epoch 017 | Loss: 0.5592\n",
      "Epoch 018 | Loss: 0.5532\n",
      "Epoch 019 | Loss: 0.5410\n",
      "Epoch 020 | Loss: 0.5216\n",
      "Epoch 021 | Loss: 0.5146\n",
      "\tVal loss: 0.4685 \n",
      "Epoch 022 | Loss: 0.5043\n",
      "Epoch 023 | Loss: 0.4939\n",
      "Epoch 024 | Loss: 0.4849\n",
      "Epoch 025 | Loss: 0.5029\n",
      "Epoch 026 | Loss: 0.4859\n",
      "\tVal loss: 0.4949 \n",
      "Epoch 027 | Loss: 0.4845\n",
      "Epoch 028 | Loss: 0.4726\n",
      "Epoch 029 | Loss: 0.4669\n",
      "Epoch 030 | Loss: 0.4565\n",
      "Epoch 031 | Loss: 0.4496\n",
      "\tVal loss: 0.4780 \n",
      "Epoch 032 | Loss: 0.4507\n",
      "Epoch 033 | Loss: 0.4630\n",
      "Epoch 034 | Loss: 0.4467\n",
      "Epoch 035 | Loss: 0.4357\n",
      "Epoch 036 | Loss: 0.4791\n",
      "\tVal loss: 0.5372 \n",
      "Epoch 037 | Loss: 0.5510\n",
      "Epoch 038 | Loss: 0.4861\n",
      "Epoch 039 | Loss: 0.4696\n",
      "Epoch 040 | Loss: 0.4543\n",
      "Epoch 041 | Loss: 0.4403\n",
      "\tVal loss: 0.4553 \n",
      "Epoch 042 | Loss: 0.4375\n",
      "Epoch 043 | Loss: 0.4339\n",
      "Epoch 044 | Loss: 0.4417\n",
      "Epoch 045 | Loss: 0.4328\n",
      "Epoch 046 | Loss: 0.4232\n",
      "\tVal loss: 0.4586 \n",
      "Epoch 047 | Loss: 0.4159\n",
      "Epoch 048 | Loss: 0.4026\n",
      "Epoch 049 | Loss: 0.3921\n",
      "Epoch 050 | Loss: 0.4037\n",
      "Epoch 051 | Loss: 0.3814\n",
      "\tVal loss: 0.4635 \n",
      "Epoch 052 | Loss: 0.3649\n",
      "Epoch 053 | Loss: 0.3540\n",
      "Epoch 054 | Loss: 0.3442\n",
      "Epoch 055 | Loss: 0.3396\n",
      "Epoch 056 | Loss: 0.3357\n",
      "\tVal loss: 0.4974 \n",
      "Epoch 057 | Loss: 0.3347\n",
      "Epoch 058 | Loss: 0.3418\n",
      "Epoch 059 | Loss: 0.3383\n",
      "Epoch 060 | Loss: 0.3229\n",
      "Epoch 061 | Loss: 0.3146\n",
      "\tVal loss: 0.4821 \n",
      "Epoch 062 | Loss: 0.3150\n",
      "Epoch 063 | Loss: 0.3276\n",
      "Epoch 064 | Loss: 0.3275\n",
      "Epoch 065 | Loss: 0.3204\n",
      "Epoch 066 | Loss: 0.3342\n",
      "\tVal loss: 0.4731 \n",
      "Epoch 067 | Loss: 0.4053\n",
      "Epoch 068 | Loss: 0.3339\n",
      "Epoch 069 | Loss: 0.2990\n",
      "Epoch 070 | Loss: 0.2836\n",
      "Epoch 071 | Loss: 0.2904\n",
      "\tVal loss: 0.4778 \n",
      "Epoch 072 | Loss: 0.3654\n",
      "Epoch 073 | Loss: 0.3665\n",
      "Epoch 074 | Loss: 0.3302\n",
      "Epoch 075 | Loss: 0.3110\n",
      "Epoch 076 | Loss: 0.3913\n",
      "\tVal loss: 0.4495 saved!\n",
      "Epoch 077 | Loss: 0.3161\n",
      "Epoch 078 | Loss: 0.2983\n",
      "Epoch 079 | Loss: 0.5216\n",
      "Epoch 080 | Loss: 0.5642\n",
      "Epoch 081 | Loss: 0.5092\n",
      "\tVal loss: 0.4824 \n",
      "Epoch 082 | Loss: 0.4742\n",
      "Epoch 083 | Loss: 0.4456\n",
      "Epoch 084 | Loss: 0.4199\n",
      "Epoch 085 | Loss: 0.3939\n",
      "Epoch 086 | Loss: 0.3659\n",
      "\tVal loss: 0.4665 \n",
      "Epoch 087 | Loss: 0.3679\n",
      "Epoch 088 | Loss: 0.3382\n",
      "Epoch 089 | Loss: 0.3889\n",
      "Epoch 090 | Loss: 0.4189\n",
      "Epoch 091 | Loss: 0.3532\n",
      "\tVal loss: 0.4643 \n",
      "Epoch 092 | Loss: 0.3162\n",
      "Epoch 093 | Loss: 0.2996\n",
      "Epoch 094 | Loss: 0.2939\n",
      "Epoch 095 | Loss: 0.2861\n",
      "Epoch 096 | Loss: 0.2760\n",
      "\tVal loss: 0.5141 \n",
      "Epoch 097 | Loss: 0.2840\n",
      "Epoch 098 | Loss: 0.2815\n",
      "Epoch 099 | Loss: 0.2777\n",
      "Epoch 100 | Loss: 0.2638\n"
     ]
    }
   ],
   "source": [
    "resnet34 = Network(34, 10, 3).to(device)\n",
    "optimiser = optim.Adam(resnet34.parameters())\n",
    "_ = train(resnet34, surge_prediction_metric, optimiser, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-22T09:40:30.286235Z",
     "start_time": "2022-03-22T09:36:49.997537Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | Loss: 0.9670\n",
      "\tVal loss: 0.8073 \n",
      "Epoch 002 | Loss: 0.7867\n",
      "Epoch 003 | Loss: 0.6985\n",
      "Epoch 004 | Loss: 0.6636\n",
      "Epoch 005 | Loss: 0.6426\n",
      "Epoch 006 | Loss: 0.6159\n",
      "\tVal loss: 0.5447 \n",
      "Epoch 007 | Loss: 0.6004\n",
      "Epoch 008 | Loss: 0.5935\n",
      "Epoch 009 | Loss: 0.5690\n",
      "Epoch 010 | Loss: 0.5628\n",
      "Epoch 011 | Loss: 0.5745\n",
      "\tVal loss: 0.6313 \n",
      "Epoch 012 | Loss: 0.5706\n",
      "Epoch 013 | Loss: 0.5700\n",
      "Epoch 014 | Loss: 0.5410\n",
      "Epoch 015 | Loss: 0.5338\n",
      "Epoch 016 | Loss: 0.5428\n",
      "\tVal loss: 0.4800 \n",
      "Epoch 017 | Loss: 0.5235\n",
      "Epoch 018 | Loss: 0.5112\n",
      "Epoch 019 | Loss: 0.5112\n",
      "Epoch 020 | Loss: 0.4926\n",
      "Epoch 021 | Loss: 0.4878\n",
      "\tVal loss: 0.4461 saved!\n",
      "Epoch 022 | Loss: 0.5719\n",
      "Epoch 023 | Loss: 0.5248\n",
      "Epoch 024 | Loss: 0.5034\n",
      "Epoch 025 | Loss: 0.4951\n",
      "Epoch 026 | Loss: 0.5231\n",
      "\tVal loss: 0.5081 \n",
      "Epoch 027 | Loss: 0.5061\n",
      "Epoch 028 | Loss: 0.4921\n",
      "Epoch 029 | Loss: 0.5225\n",
      "Epoch 030 | Loss: 0.4921\n",
      "Epoch 031 | Loss: 0.4809\n",
      "\tVal loss: 0.4742 \n",
      "Epoch 032 | Loss: 0.5592\n",
      "Epoch 033 | Loss: 0.5776\n",
      "Epoch 034 | Loss: 0.5222\n",
      "Epoch 035 | Loss: 0.4987\n",
      "Epoch 036 | Loss: 0.4878\n",
      "\tVal loss: 0.4684 \n",
      "Epoch 037 | Loss: 0.4778\n",
      "Epoch 038 | Loss: 0.4700\n",
      "Epoch 039 | Loss: 0.5307\n",
      "Epoch 040 | Loss: 0.5579\n",
      "Epoch 041 | Loss: 0.5519\n",
      "\tVal loss: 0.5602 \n",
      "Epoch 042 | Loss: 0.5976\n",
      "Epoch 043 | Loss: 0.5586\n",
      "Epoch 044 | Loss: 0.5336\n",
      "Epoch 045 | Loss: 0.5167\n",
      "Epoch 046 | Loss: 0.4896\n",
      "\tVal loss: 0.4621 \n",
      "Epoch 047 | Loss: 0.4794\n",
      "Epoch 048 | Loss: 0.4720\n",
      "Epoch 049 | Loss: 0.4756\n",
      "Epoch 050 | Loss: 0.4676\n",
      "Epoch 051 | Loss: 0.4569\n",
      "\tVal loss: 0.4482 saved!\n",
      "Epoch 052 | Loss: 0.4483\n",
      "Epoch 053 | Loss: 0.4895\n",
      "Epoch 054 | Loss: 0.4682\n",
      "Epoch 055 | Loss: 0.4667\n",
      "Epoch 056 | Loss: 0.4830\n",
      "\tVal loss: 0.4509 \n",
      "Epoch 057 | Loss: 0.4597\n",
      "Epoch 058 | Loss: 0.4493\n",
      "Epoch 059 | Loss: 0.4442\n",
      "Epoch 060 | Loss: 0.4430\n",
      "Epoch 061 | Loss: 0.4683\n",
      "\tVal loss: 0.4490 saved!\n",
      "Epoch 062 | Loss: 0.4792\n",
      "Epoch 063 | Loss: 0.4733\n",
      "Epoch 064 | Loss: 0.5404\n",
      "Epoch 065 | Loss: 0.5096\n",
      "Epoch 066 | Loss: 0.4955\n",
      "\tVal loss: 0.4699 \n",
      "Epoch 067 | Loss: 0.4672\n",
      "Epoch 068 | Loss: 0.4523\n",
      "Epoch 069 | Loss: 0.4422\n",
      "Epoch 070 | Loss: 0.4598\n",
      "Epoch 071 | Loss: 0.4591\n",
      "\tVal loss: 0.4473 saved!\n",
      "Epoch 072 | Loss: 0.6280\n",
      "Epoch 073 | Loss: 0.5796\n",
      "Epoch 074 | Loss: 0.5153\n",
      "Epoch 075 | Loss: 0.4963\n",
      "Epoch 076 | Loss: 0.4834\n",
      "\tVal loss: 0.4609 \n",
      "Epoch 077 | Loss: 0.4615\n",
      "Epoch 078 | Loss: 0.4588\n",
      "Epoch 079 | Loss: 0.4516\n",
      "Epoch 080 | Loss: 0.4394\n",
      "Epoch 081 | Loss: 0.4366\n",
      "\tVal loss: 0.4351 saved!\n",
      "Epoch 082 | Loss: 0.4341\n",
      "Epoch 083 | Loss: 0.4294\n",
      "Epoch 084 | Loss: 0.4240\n",
      "Epoch 085 | Loss: 0.4157\n",
      "Epoch 086 | Loss: 0.4271\n",
      "\tVal loss: 0.4681 \n",
      "Epoch 087 | Loss: 0.4553\n",
      "Epoch 088 | Loss: 0.4263\n",
      "Epoch 089 | Loss: 0.4236\n",
      "Epoch 090 | Loss: 0.4131\n",
      "Epoch 091 | Loss: 0.4008\n",
      "\tVal loss: 0.4192 saved!\n",
      "Epoch 092 | Loss: 0.3956\n",
      "Epoch 093 | Loss: 0.4405\n",
      "Epoch 094 | Loss: 0.4199\n",
      "Epoch 095 | Loss: 0.4524\n",
      "Epoch 096 | Loss: 0.4195\n",
      "\tVal loss: 0.4314 saved!\n",
      "Epoch 097 | Loss: 0.4268\n",
      "Epoch 098 | Loss: 0.4257\n",
      "Epoch 099 | Loss: 0.4396\n",
      "Epoch 100 | Loss: 0.5102\n"
     ]
    }
   ],
   "source": [
    "resnet34 = Network(34, 20, 3).to(device)\n",
    "optimiser = optim.Adam(resnet34.parameters())\n",
    "_ = train(resnet34, surge_prediction_metric, optimiser, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-22T08:10:38.975836Z",
     "start_time": "2022-03-22T08:06:58.899928Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | Loss: 0.8644\n",
      "\tVal loss: 0.8311 \n",
      "Epoch 002 | Loss: 0.6837\n",
      "Epoch 003 | Loss: 0.6483\n",
      "Epoch 004 | Loss: 0.6150\n",
      "Epoch 005 | Loss: 0.6093\n",
      "Epoch 006 | Loss: 0.5878\n",
      "\tVal loss: 0.5418 \n",
      "Epoch 007 | Loss: 0.5778\n",
      "Epoch 008 | Loss: 0.5672\n",
      "Epoch 009 | Loss: 0.5600\n",
      "Epoch 010 | Loss: 0.5383\n",
      "Epoch 011 | Loss: 0.5285\n",
      "\tVal loss: 0.4987 \n",
      "Epoch 012 | Loss: 0.5283\n",
      "Epoch 013 | Loss: 0.5206\n",
      "Epoch 014 | Loss: 0.5302\n",
      "Epoch 015 | Loss: 0.5027\n",
      "Epoch 016 | Loss: 0.4898\n",
      "\tVal loss: 0.5239 \n",
      "Epoch 017 | Loss: 0.4845\n",
      "Epoch 018 | Loss: 0.4718\n",
      "Epoch 019 | Loss: 0.4642\n",
      "Epoch 020 | Loss: 0.5069\n",
      "Epoch 021 | Loss: 0.4727\n",
      "\tVal loss: 0.4618 \n",
      "Epoch 022 | Loss: 0.4945\n",
      "Epoch 023 | Loss: 0.4947\n",
      "Epoch 024 | Loss: 0.4868\n",
      "Epoch 025 | Loss: 0.4714\n",
      "Epoch 026 | Loss: 0.4647\n",
      "\tVal loss: 0.4426 saved!\n",
      "Epoch 027 | Loss: 0.4575\n",
      "Epoch 028 | Loss: 0.5048\n",
      "Epoch 029 | Loss: 0.4846\n",
      "Epoch 030 | Loss: 0.4633\n",
      "Epoch 031 | Loss: 0.4801\n",
      "\tVal loss: 0.4666 \n",
      "Epoch 032 | Loss: 0.4531\n",
      "Epoch 033 | Loss: 0.4366\n",
      "Epoch 034 | Loss: 0.5267\n",
      "Epoch 035 | Loss: 0.5259\n",
      "Epoch 036 | Loss: 0.4959\n",
      "\tVal loss: 0.4599 \n",
      "Epoch 037 | Loss: 0.4742\n",
      "Epoch 038 | Loss: 0.4636\n",
      "Epoch 039 | Loss: 0.5021\n",
      "Epoch 040 | Loss: 0.5811\n",
      "Epoch 041 | Loss: 0.5168\n",
      "\tVal loss: 0.4731 \n",
      "Epoch 042 | Loss: 0.4846\n",
      "Epoch 043 | Loss: 0.4811\n",
      "Epoch 044 | Loss: 0.4980\n",
      "Epoch 045 | Loss: 0.4708\n",
      "Epoch 046 | Loss: 0.4558\n",
      "\tVal loss: 0.4310 saved!\n",
      "Epoch 047 | Loss: 0.4498\n",
      "Epoch 048 | Loss: 0.4479\n",
      "Epoch 049 | Loss: 0.4766\n",
      "Epoch 050 | Loss: 0.4497\n",
      "Epoch 051 | Loss: 0.4395\n",
      "\tVal loss: 0.4475 saved!\n",
      "Epoch 052 | Loss: 0.4313\n",
      "Epoch 053 | Loss: 0.4259\n",
      "Epoch 054 | Loss: 0.4412\n",
      "Epoch 055 | Loss: 0.4258\n",
      "Epoch 056 | Loss: 0.4208\n",
      "\tVal loss: 0.4310 saved!\n",
      "Epoch 057 | Loss: 0.4193\n",
      "Epoch 058 | Loss: 0.4152\n",
      "Epoch 059 | Loss: 0.4443\n",
      "Epoch 060 | Loss: 0.4205\n",
      "Epoch 061 | Loss: 0.4086\n",
      "\tVal loss: 0.4539 \n",
      "Epoch 062 | Loss: 0.4074\n",
      "Epoch 063 | Loss: 0.4119\n",
      "Epoch 064 | Loss: 0.3966\n",
      "Epoch 065 | Loss: 0.3790\n",
      "Epoch 066 | Loss: 0.3718\n",
      "\tVal loss: 0.4355 saved!\n",
      "Epoch 067 | Loss: 0.3751\n",
      "Epoch 068 | Loss: 0.4620\n",
      "Epoch 069 | Loss: 0.4256\n",
      "Epoch 070 | Loss: 0.4025\n",
      "Epoch 071 | Loss: 0.5369\n",
      "\tVal loss: 0.4509 \n",
      "Epoch 072 | Loss: 0.4844\n",
      "Epoch 073 | Loss: 0.4849\n",
      "Epoch 074 | Loss: 0.4637\n",
      "Epoch 075 | Loss: 0.4632\n",
      "Epoch 076 | Loss: 0.4417\n",
      "\tVal loss: 0.4370 saved!\n",
      "Epoch 077 | Loss: 0.4212\n",
      "Epoch 078 | Loss: 0.4174\n",
      "Epoch 079 | Loss: 0.4286\n",
      "Epoch 080 | Loss: 0.4383\n",
      "Epoch 081 | Loss: 0.4282\n",
      "\tVal loss: 0.4401 saved!\n",
      "Epoch 082 | Loss: 0.4081\n",
      "Epoch 083 | Loss: 0.4203\n",
      "Epoch 084 | Loss: 0.4528\n",
      "Epoch 085 | Loss: 0.4330\n",
      "Epoch 086 | Loss: 0.4213\n",
      "\tVal loss: 0.4298 saved!\n",
      "Epoch 087 | Loss: 0.4122\n",
      "Epoch 088 | Loss: 0.4047\n",
      "Epoch 089 | Loss: 0.4175\n",
      "Epoch 090 | Loss: 0.4062\n",
      "Epoch 091 | Loss: 0.3754\n",
      "\tVal loss: 0.4515 \n",
      "Epoch 092 | Loss: 0.4766\n",
      "Epoch 093 | Loss: 0.5144\n",
      "Epoch 094 | Loss: 0.4731\n",
      "Epoch 095 | Loss: 0.4504\n",
      "Epoch 096 | Loss: 0.4312\n",
      "\tVal loss: 0.4427 saved!\n",
      "Epoch 097 | Loss: 0.4233\n",
      "Epoch 098 | Loss: 0.4044\n",
      "Epoch 099 | Loss: 0.4626\n",
      "Epoch 100 | Loss: 0.4144\n"
     ]
    }
   ],
   "source": [
    "resnet34 = Network(34, 200, 3).to(device)\n",
    "optimiser = optim.Adam(resnet34.parameters())\n",
    "_ = train(resnet34, surge_prediction_metric, optimiser, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-22T04:08:52.159523Z",
     "start_time": "2022-03-22T04:03:24.355202Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | Loss: 1.0308\n",
      "\tVal loss: 0.8777 \n",
      "Epoch 002 | Loss: 0.8113\n",
      "Epoch 003 | Loss: 0.7012\n",
      "Epoch 004 | Loss: 0.6576\n",
      "Epoch 005 | Loss: 0.6321\n",
      "Epoch 006 | Loss: 0.6288\n",
      "\tVal loss: 0.5834 \n",
      "Epoch 007 | Loss: 0.6115\n",
      "Epoch 008 | Loss: 0.6023\n",
      "Epoch 009 | Loss: 0.6059\n",
      "Epoch 010 | Loss: 0.6204\n",
      "Epoch 011 | Loss: 0.6071\n",
      "\tVal loss: 0.5749 \n",
      "Epoch 012 | Loss: 0.5898\n",
      "Epoch 013 | Loss: 0.5818\n",
      "Epoch 014 | Loss: 0.5839\n",
      "Epoch 015 | Loss: 0.5724\n",
      "Epoch 016 | Loss: 0.5579\n",
      "\tVal loss: 0.5337 \n",
      "Epoch 017 | Loss: 0.5519\n",
      "Epoch 018 | Loss: 0.5438\n",
      "Epoch 019 | Loss: 0.5401\n",
      "Epoch 020 | Loss: 0.5246\n",
      "Epoch 021 | Loss: 0.5132\n",
      "\tVal loss: 0.6028 \n",
      "Epoch 022 | Loss: 0.5152\n",
      "Epoch 023 | Loss: 0.5400\n",
      "Epoch 024 | Loss: 0.5886\n",
      "Epoch 025 | Loss: 0.5581\n",
      "Epoch 026 | Loss: 0.5496\n",
      "\tVal loss: 0.5551 \n",
      "Epoch 027 | Loss: 0.5437\n",
      "Epoch 028 | Loss: 0.5293\n",
      "Epoch 029 | Loss: 0.5289\n",
      "Epoch 030 | Loss: 0.6016\n",
      "Epoch 031 | Loss: 0.5634\n",
      "\tVal loss: 0.5577 \n",
      "Epoch 032 | Loss: 0.5501\n",
      "Epoch 033 | Loss: 0.5268\n",
      "Epoch 034 | Loss: 0.5263\n",
      "Epoch 035 | Loss: 0.5383\n",
      "Epoch 036 | Loss: 0.5440\n",
      "\tVal loss: 0.5237 \n",
      "Epoch 037 | Loss: 0.5167\n",
      "Epoch 038 | Loss: 0.5162\n",
      "Epoch 039 | Loss: 0.5284\n",
      "Epoch 040 | Loss: 0.5009\n",
      "Epoch 041 | Loss: 0.4857\n",
      "\tVal loss: 0.5490 \n",
      "Epoch 042 | Loss: 0.4918\n",
      "Epoch 043 | Loss: 0.4848\n",
      "Epoch 044 | Loss: 0.4659\n",
      "Epoch 045 | Loss: 0.5015\n",
      "Epoch 046 | Loss: 0.5563\n",
      "\tVal loss: 0.5427 \n",
      "Epoch 047 | Loss: 0.5431\n",
      "Epoch 048 | Loss: 0.5129\n",
      "Epoch 049 | Loss: 0.5012\n",
      "Epoch 050 | Loss: 0.4869\n",
      "Epoch 051 | Loss: 0.4658\n",
      "\tVal loss: 0.5251 \n",
      "Epoch 052 | Loss: 0.4576\n",
      "Epoch 053 | Loss: 0.4526\n",
      "Epoch 054 | Loss: 0.4597\n",
      "Epoch 055 | Loss: 0.4394\n",
      "Epoch 056 | Loss: 0.4313\n",
      "\tVal loss: 0.5425 \n",
      "Epoch 057 | Loss: 0.4277\n",
      "Epoch 058 | Loss: 0.4230\n",
      "Epoch 059 | Loss: 0.4140\n",
      "Epoch 060 | Loss: 0.4121\n",
      "Epoch 061 | Loss: 0.4039\n",
      "\tVal loss: 0.5444 \n",
      "Epoch 062 | Loss: 0.4131\n",
      "Epoch 063 | Loss: 0.5716\n",
      "Epoch 064 | Loss: 0.5187\n",
      "Epoch 065 | Loss: 0.4793\n",
      "Epoch 066 | Loss: 0.5159\n",
      "\tVal loss: 0.5332 \n",
      "Epoch 067 | Loss: 0.5065\n",
      "Epoch 068 | Loss: 0.4658\n",
      "Epoch 069 | Loss: 0.4445\n",
      "Epoch 070 | Loss: 0.4270\n",
      "Epoch 071 | Loss: 0.4332\n",
      "\tVal loss: 0.5417 \n",
      "Epoch 072 | Loss: 0.4402\n",
      "Epoch 073 | Loss: 0.4500\n",
      "Epoch 074 | Loss: 0.4248\n",
      "Epoch 075 | Loss: 0.4191\n",
      "Epoch 076 | Loss: 0.3950\n",
      "\tVal loss: 0.5549 \n",
      "Epoch 077 | Loss: 0.3854\n",
      "Epoch 078 | Loss: 0.3838\n",
      "Epoch 079 | Loss: 0.4034\n",
      "Epoch 080 | Loss: 0.3823\n",
      "Epoch 081 | Loss: 0.3810\n",
      "\tVal loss: 0.6061 \n",
      "Epoch 082 | Loss: 0.3913\n",
      "Epoch 083 | Loss: 0.5169\n",
      "Epoch 084 | Loss: 0.4705\n",
      "Epoch 085 | Loss: 0.4414\n",
      "Epoch 086 | Loss: 0.4810\n",
      "\tVal loss: 0.5538 \n",
      "Epoch 087 | Loss: 0.4560\n",
      "Epoch 088 | Loss: 0.4130\n",
      "Epoch 089 | Loss: 0.3888\n",
      "Epoch 090 | Loss: 0.3714\n",
      "Epoch 091 | Loss: 0.3739\n",
      "\tVal loss: 0.5578 \n",
      "Epoch 092 | Loss: 0.3720\n",
      "Epoch 093 | Loss: 0.3782\n",
      "Epoch 094 | Loss: 0.3846\n",
      "Epoch 095 | Loss: 0.3747\n",
      "Epoch 096 | Loss: 0.3753\n",
      "\tVal loss: 0.5586 \n",
      "Epoch 097 | Loss: 0.3699\n",
      "Epoch 098 | Loss: 0.3767\n",
      "Epoch 099 | Loss: 0.3709\n",
      "Epoch 100 | Loss: 0.3394\n"
     ]
    }
   ],
   "source": [
    "resnet50 = Network(50, 100, 1).to(device)\n",
    "optimiser = optim.Adam(resnet50.parameters())\n",
    "_ = train(resnet50, surge_prediction_metric, optimiser, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-22T07:50:01.715893Z",
     "start_time": "2022-03-22T07:50:01.711881Z"
    }
   },
   "outputs": [],
   "source": [
    "def write_submission(model, fn='submission.csv'):\n",
    "    COLUMNS = [\n",
    "        'surge1_t0', 'surge1_t1', 'surge1_t2', 'surge1_t3', 'surge1_t4',\n",
    "        'surge1_t5', 'surge1_t6', 'surge1_t7', 'surge1_t8', 'surge1_t9',\n",
    "        'surge2_t0', 'surge2_t1', 'surge2_t2', 'surge2_t3', 'surge2_t4',\n",
    "        'surge2_t5', 'surge2_t6', 'surge2_t7', 'surge2_t8', 'surge2_t9' ]\n",
    "    surge1_output = []\n",
    "    surge2_output = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for slp, t_surge1_in, surge1_in, t_surge2_in, surge2_in, t_surge1_out, t_surge2_out in test_dataloader:\n",
    "            slp = slp.to(device)\n",
    "            t_surge1_in = t_surge1_in.to(device)\n",
    "            surge1_in = surge1_in.to(device)\n",
    "            t_surge2_in = t_surge2_in.to(device)\n",
    "            surge2_in = surge2_in.to(device)\n",
    "            t_surge1_out = t_surge1_out.to(device)\n",
    "            t_surge2_out = t_surge2_out.to(device)\n",
    "            surge1_out_pred, surge2_out_pred = model(slp, t_surge1_in, surge1_in, t_surge2_in, surge2_in, t_surge1_out, t_surge2_out)\n",
    "            surge1_output.append(surge1_out_pred)\n",
    "            surge2_output.append(surge2_out_pred)\n",
    "\n",
    "    surge1_output = torch.cat(surge1_output, dim=0)\n",
    "    surge2_output = torch.cat(surge2_output, dim=0)\n",
    "    test_outputs = torch.cat([surge1_output, surge2_output], dim=1).detach().cpu().numpy()\n",
    "\n",
    "    test_df = pd.DataFrame(test_outputs, index=test_id_seq, columns=COLUMNS)\n",
    "    test_df.index.name = 'id_sequence'\n",
    "    test_df.to_csv(fn)\n",
    "    return test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-22T08:35:15.976997Z",
     "start_time": "2022-03-22T08:35:15.684722Z"
    }
   },
   "outputs": [],
   "source": [
    "model_fn = 'resnet34_20_3_60.pth' # 'resnet34_50_3_40.pth'\n",
    "model = Network(34, 20, 3)\n",
    "model.load_state_dict(torch.load(model_fn, map_location=device))\n",
    "model = model.to(device)\n",
    "df = write_submission(model, 'submission6.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-22T02:06:05.809337Z",
     "start_time": "2022-03-22T02:06:05.789324Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>surge1_t0</th>\n",
       "      <th>surge1_t1</th>\n",
       "      <th>surge1_t2</th>\n",
       "      <th>surge1_t3</th>\n",
       "      <th>surge1_t4</th>\n",
       "      <th>surge1_t5</th>\n",
       "      <th>surge1_t6</th>\n",
       "      <th>surge1_t7</th>\n",
       "      <th>surge1_t8</th>\n",
       "      <th>surge1_t9</th>\n",
       "      <th>surge2_t0</th>\n",
       "      <th>surge2_t1</th>\n",
       "      <th>surge2_t2</th>\n",
       "      <th>surge2_t3</th>\n",
       "      <th>surge2_t4</th>\n",
       "      <th>surge2_t5</th>\n",
       "      <th>surge2_t6</th>\n",
       "      <th>surge2_t7</th>\n",
       "      <th>surge2_t8</th>\n",
       "      <th>surge2_t9</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_sequence</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5600</th>\n",
       "      <td>-1.522672</td>\n",
       "      <td>-1.640056</td>\n",
       "      <td>-1.812372</td>\n",
       "      <td>-1.873276</td>\n",
       "      <td>-1.911106</td>\n",
       "      <td>-1.892778</td>\n",
       "      <td>-1.814209</td>\n",
       "      <td>-1.698588</td>\n",
       "      <td>-1.570127</td>\n",
       "      <td>-1.445986</td>\n",
       "      <td>-1.077388</td>\n",
       "      <td>-1.206931</td>\n",
       "      <td>-1.322765</td>\n",
       "      <td>-1.355515</td>\n",
       "      <td>-1.416860</td>\n",
       "      <td>-1.485878</td>\n",
       "      <td>-1.503089</td>\n",
       "      <td>-1.474643</td>\n",
       "      <td>-1.412876</td>\n",
       "      <td>-1.315216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5601</th>\n",
       "      <td>-0.172018</td>\n",
       "      <td>-0.244204</td>\n",
       "      <td>-0.333128</td>\n",
       "      <td>-0.247466</td>\n",
       "      <td>0.008339</td>\n",
       "      <td>0.311038</td>\n",
       "      <td>0.669055</td>\n",
       "      <td>1.009192</td>\n",
       "      <td>1.198229</td>\n",
       "      <td>1.189028</td>\n",
       "      <td>0.016760</td>\n",
       "      <td>-0.052519</td>\n",
       "      <td>-0.104194</td>\n",
       "      <td>-0.077168</td>\n",
       "      <td>0.124576</td>\n",
       "      <td>0.411364</td>\n",
       "      <td>0.781786</td>\n",
       "      <td>1.110761</td>\n",
       "      <td>1.277642</td>\n",
       "      <td>1.251851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5602</th>\n",
       "      <td>-0.728413</td>\n",
       "      <td>-1.103798</td>\n",
       "      <td>-1.081307</td>\n",
       "      <td>-0.869735</td>\n",
       "      <td>-0.684275</td>\n",
       "      <td>-0.531688</td>\n",
       "      <td>-0.410095</td>\n",
       "      <td>-0.311700</td>\n",
       "      <td>-0.223303</td>\n",
       "      <td>-0.145472</td>\n",
       "      <td>1.353438</td>\n",
       "      <td>1.628443</td>\n",
       "      <td>1.170808</td>\n",
       "      <td>0.442155</td>\n",
       "      <td>0.039545</td>\n",
       "      <td>-0.142167</td>\n",
       "      <td>-0.216019</td>\n",
       "      <td>-0.203616</td>\n",
       "      <td>-0.161684</td>\n",
       "      <td>-0.113963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5603</th>\n",
       "      <td>-0.214031</td>\n",
       "      <td>0.022230</td>\n",
       "      <td>0.161170</td>\n",
       "      <td>0.047790</td>\n",
       "      <td>-0.173768</td>\n",
       "      <td>-0.312626</td>\n",
       "      <td>-0.312222</td>\n",
       "      <td>-0.243562</td>\n",
       "      <td>-0.164647</td>\n",
       "      <td>-0.095614</td>\n",
       "      <td>1.032095</td>\n",
       "      <td>1.588513</td>\n",
       "      <td>1.518492</td>\n",
       "      <td>0.934075</td>\n",
       "      <td>0.517910</td>\n",
       "      <td>0.386632</td>\n",
       "      <td>0.364263</td>\n",
       "      <td>0.341955</td>\n",
       "      <td>0.313909</td>\n",
       "      <td>0.286394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5604</th>\n",
       "      <td>0.188806</td>\n",
       "      <td>0.256014</td>\n",
       "      <td>0.136325</td>\n",
       "      <td>-0.055788</td>\n",
       "      <td>-0.255821</td>\n",
       "      <td>-0.438868</td>\n",
       "      <td>-0.531611</td>\n",
       "      <td>-0.504565</td>\n",
       "      <td>-0.409072</td>\n",
       "      <td>-0.300028</td>\n",
       "      <td>-0.038583</td>\n",
       "      <td>0.077969</td>\n",
       "      <td>-0.076706</td>\n",
       "      <td>-0.310813</td>\n",
       "      <td>-0.561307</td>\n",
       "      <td>-0.757425</td>\n",
       "      <td>-0.837745</td>\n",
       "      <td>-0.798326</td>\n",
       "      <td>-0.687479</td>\n",
       "      <td>-0.554552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6104</th>\n",
       "      <td>-0.202868</td>\n",
       "      <td>-0.458387</td>\n",
       "      <td>-0.676621</td>\n",
       "      <td>-0.850127</td>\n",
       "      <td>-0.801504</td>\n",
       "      <td>-0.661781</td>\n",
       "      <td>-0.550004</td>\n",
       "      <td>-0.469796</td>\n",
       "      <td>-0.375337</td>\n",
       "      <td>-0.267382</td>\n",
       "      <td>0.679106</td>\n",
       "      <td>-0.081019</td>\n",
       "      <td>-0.426764</td>\n",
       "      <td>-0.630051</td>\n",
       "      <td>-0.645582</td>\n",
       "      <td>-0.500996</td>\n",
       "      <td>-0.302407</td>\n",
       "      <td>-0.138851</td>\n",
       "      <td>-0.036838</td>\n",
       "      <td>0.022590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6105</th>\n",
       "      <td>0.004178</td>\n",
       "      <td>-0.027268</td>\n",
       "      <td>-0.104875</td>\n",
       "      <td>-0.129565</td>\n",
       "      <td>-0.042405</td>\n",
       "      <td>0.124772</td>\n",
       "      <td>0.274499</td>\n",
       "      <td>0.309280</td>\n",
       "      <td>0.270955</td>\n",
       "      <td>0.226377</td>\n",
       "      <td>0.155758</td>\n",
       "      <td>-0.343349</td>\n",
       "      <td>-0.494503</td>\n",
       "      <td>-0.406895</td>\n",
       "      <td>-0.211891</td>\n",
       "      <td>0.009162</td>\n",
       "      <td>0.194007</td>\n",
       "      <td>0.265217</td>\n",
       "      <td>0.251194</td>\n",
       "      <td>0.218806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6106</th>\n",
       "      <td>-0.429599</td>\n",
       "      <td>-0.720932</td>\n",
       "      <td>-0.728523</td>\n",
       "      <td>-0.629140</td>\n",
       "      <td>-0.443044</td>\n",
       "      <td>-0.221718</td>\n",
       "      <td>0.056978</td>\n",
       "      <td>0.360464</td>\n",
       "      <td>0.566990</td>\n",
       "      <td>0.590129</td>\n",
       "      <td>0.005821</td>\n",
       "      <td>-0.308064</td>\n",
       "      <td>-0.262538</td>\n",
       "      <td>-0.246235</td>\n",
       "      <td>-0.120654</td>\n",
       "      <td>0.120976</td>\n",
       "      <td>0.452356</td>\n",
       "      <td>0.784378</td>\n",
       "      <td>0.954932</td>\n",
       "      <td>0.906470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6107</th>\n",
       "      <td>0.218414</td>\n",
       "      <td>-0.065912</td>\n",
       "      <td>-0.073831</td>\n",
       "      <td>-0.228745</td>\n",
       "      <td>-0.546522</td>\n",
       "      <td>-0.754496</td>\n",
       "      <td>-0.782437</td>\n",
       "      <td>-0.702246</td>\n",
       "      <td>-0.584819</td>\n",
       "      <td>-0.462281</td>\n",
       "      <td>0.360878</td>\n",
       "      <td>-0.268439</td>\n",
       "      <td>-0.302901</td>\n",
       "      <td>-0.340486</td>\n",
       "      <td>-0.570672</td>\n",
       "      <td>-0.755228</td>\n",
       "      <td>-0.782008</td>\n",
       "      <td>-0.705261</td>\n",
       "      <td>-0.590581</td>\n",
       "      <td>-0.468862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6108</th>\n",
       "      <td>-0.082970</td>\n",
       "      <td>-0.123427</td>\n",
       "      <td>0.101262</td>\n",
       "      <td>0.405648</td>\n",
       "      <td>0.578660</td>\n",
       "      <td>0.476890</td>\n",
       "      <td>0.271017</td>\n",
       "      <td>0.211466</td>\n",
       "      <td>0.257518</td>\n",
       "      <td>0.282207</td>\n",
       "      <td>0.226513</td>\n",
       "      <td>-0.025818</td>\n",
       "      <td>0.269845</td>\n",
       "      <td>0.735363</td>\n",
       "      <td>1.123745</td>\n",
       "      <td>1.118167</td>\n",
       "      <td>0.872491</td>\n",
       "      <td>0.779338</td>\n",
       "      <td>0.723321</td>\n",
       "      <td>0.621802</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>509 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             surge1_t0  surge1_t1  surge1_t2  surge1_t3  surge1_t4  surge1_t5  \\\n",
       "id_sequence                                                                     \n",
       "5600         -1.522672  -1.640056  -1.812372  -1.873276  -1.911106  -1.892778   \n",
       "5601         -0.172018  -0.244204  -0.333128  -0.247466   0.008339   0.311038   \n",
       "5602         -0.728413  -1.103798  -1.081307  -0.869735  -0.684275  -0.531688   \n",
       "5603         -0.214031   0.022230   0.161170   0.047790  -0.173768  -0.312626   \n",
       "5604          0.188806   0.256014   0.136325  -0.055788  -0.255821  -0.438868   \n",
       "...                ...        ...        ...        ...        ...        ...   \n",
       "6104         -0.202868  -0.458387  -0.676621  -0.850127  -0.801504  -0.661781   \n",
       "6105          0.004178  -0.027268  -0.104875  -0.129565  -0.042405   0.124772   \n",
       "6106         -0.429599  -0.720932  -0.728523  -0.629140  -0.443044  -0.221718   \n",
       "6107          0.218414  -0.065912  -0.073831  -0.228745  -0.546522  -0.754496   \n",
       "6108         -0.082970  -0.123427   0.101262   0.405648   0.578660   0.476890   \n",
       "\n",
       "             surge1_t6  surge1_t7  surge1_t8  surge1_t9  surge2_t0  surge2_t1  \\\n",
       "id_sequence                                                                     \n",
       "5600         -1.814209  -1.698588  -1.570127  -1.445986  -1.077388  -1.206931   \n",
       "5601          0.669055   1.009192   1.198229   1.189028   0.016760  -0.052519   \n",
       "5602         -0.410095  -0.311700  -0.223303  -0.145472   1.353438   1.628443   \n",
       "5603         -0.312222  -0.243562  -0.164647  -0.095614   1.032095   1.588513   \n",
       "5604         -0.531611  -0.504565  -0.409072  -0.300028  -0.038583   0.077969   \n",
       "...                ...        ...        ...        ...        ...        ...   \n",
       "6104         -0.550004  -0.469796  -0.375337  -0.267382   0.679106  -0.081019   \n",
       "6105          0.274499   0.309280   0.270955   0.226377   0.155758  -0.343349   \n",
       "6106          0.056978   0.360464   0.566990   0.590129   0.005821  -0.308064   \n",
       "6107         -0.782437  -0.702246  -0.584819  -0.462281   0.360878  -0.268439   \n",
       "6108          0.271017   0.211466   0.257518   0.282207   0.226513  -0.025818   \n",
       "\n",
       "             surge2_t2  surge2_t3  surge2_t4  surge2_t5  surge2_t6  surge2_t7  \\\n",
       "id_sequence                                                                     \n",
       "5600         -1.322765  -1.355515  -1.416860  -1.485878  -1.503089  -1.474643   \n",
       "5601         -0.104194  -0.077168   0.124576   0.411364   0.781786   1.110761   \n",
       "5602          1.170808   0.442155   0.039545  -0.142167  -0.216019  -0.203616   \n",
       "5603          1.518492   0.934075   0.517910   0.386632   0.364263   0.341955   \n",
       "5604         -0.076706  -0.310813  -0.561307  -0.757425  -0.837745  -0.798326   \n",
       "...                ...        ...        ...        ...        ...        ...   \n",
       "6104         -0.426764  -0.630051  -0.645582  -0.500996  -0.302407  -0.138851   \n",
       "6105         -0.494503  -0.406895  -0.211891   0.009162   0.194007   0.265217   \n",
       "6106         -0.262538  -0.246235  -0.120654   0.120976   0.452356   0.784378   \n",
       "6107         -0.302901  -0.340486  -0.570672  -0.755228  -0.782008  -0.705261   \n",
       "6108          0.269845   0.735363   1.123745   1.118167   0.872491   0.779338   \n",
       "\n",
       "             surge2_t8  surge2_t9  \n",
       "id_sequence                        \n",
       "5600         -1.412876  -1.315216  \n",
       "5601          1.277642   1.251851  \n",
       "5602         -0.161684  -0.113963  \n",
       "5603          0.313909   0.286394  \n",
       "5604         -0.687479  -0.554552  \n",
       "...                ...        ...  \n",
       "6104         -0.036838   0.022590  \n",
       "6105          0.251194   0.218806  \n",
       "6106          0.954932   0.906470  \n",
       "6107         -0.590581  -0.468862  \n",
       "6108          0.723321   0.621802  \n",
       "\n",
       "[509 rows x 20 columns]"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-22T04:25:42.510131Z",
     "start_time": "2022-03-22T04:25:42.499432Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>surge1_t0</th>\n",
       "      <th>surge1_t1</th>\n",
       "      <th>surge1_t2</th>\n",
       "      <th>surge1_t3</th>\n",
       "      <th>surge1_t4</th>\n",
       "      <th>surge1_t5</th>\n",
       "      <th>surge1_t6</th>\n",
       "      <th>surge1_t7</th>\n",
       "      <th>surge1_t8</th>\n",
       "      <th>surge1_t9</th>\n",
       "      <th>surge2_t0</th>\n",
       "      <th>surge2_t1</th>\n",
       "      <th>surge2_t2</th>\n",
       "      <th>surge2_t3</th>\n",
       "      <th>surge2_t4</th>\n",
       "      <th>surge2_t5</th>\n",
       "      <th>surge2_t6</th>\n",
       "      <th>surge2_t7</th>\n",
       "      <th>surge2_t8</th>\n",
       "      <th>surge2_t9</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_sequence</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5600</th>\n",
       "      <td>-1.199508</td>\n",
       "      <td>-1.171856</td>\n",
       "      <td>-1.131088</td>\n",
       "      <td>-1.066332</td>\n",
       "      <td>-0.990663</td>\n",
       "      <td>-0.864091</td>\n",
       "      <td>-0.801858</td>\n",
       "      <td>-0.746576</td>\n",
       "      <td>-0.696449</td>\n",
       "      <td>-0.651072</td>\n",
       "      <td>-0.726454</td>\n",
       "      <td>-0.739934</td>\n",
       "      <td>-0.704749</td>\n",
       "      <td>-0.652886</td>\n",
       "      <td>-0.602378</td>\n",
       "      <td>-0.553215</td>\n",
       "      <td>-0.507450</td>\n",
       "      <td>-0.465277</td>\n",
       "      <td>-0.426380</td>\n",
       "      <td>-0.390044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5601</th>\n",
       "      <td>-0.189759</td>\n",
       "      <td>-0.080710</td>\n",
       "      <td>0.080381</td>\n",
       "      <td>0.214584</td>\n",
       "      <td>0.307948</td>\n",
       "      <td>0.374662</td>\n",
       "      <td>0.426763</td>\n",
       "      <td>0.470630</td>\n",
       "      <td>0.509728</td>\n",
       "      <td>0.546062</td>\n",
       "      <td>-0.023364</td>\n",
       "      <td>0.019109</td>\n",
       "      <td>0.151435</td>\n",
       "      <td>0.216138</td>\n",
       "      <td>0.207174</td>\n",
       "      <td>0.181843</td>\n",
       "      <td>0.155302</td>\n",
       "      <td>0.228899</td>\n",
       "      <td>0.305990</td>\n",
       "      <td>0.377340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5602</th>\n",
       "      <td>-0.236023</td>\n",
       "      <td>-0.227396</td>\n",
       "      <td>-0.211098</td>\n",
       "      <td>-0.195290</td>\n",
       "      <td>-0.173217</td>\n",
       "      <td>-0.148450</td>\n",
       "      <td>-0.123184</td>\n",
       "      <td>-0.097737</td>\n",
       "      <td>-0.071892</td>\n",
       "      <td>-0.045371</td>\n",
       "      <td>0.345954</td>\n",
       "      <td>0.105727</td>\n",
       "      <td>0.012435</td>\n",
       "      <td>-0.008099</td>\n",
       "      <td>0.004825</td>\n",
       "      <td>0.026204</td>\n",
       "      <td>0.049977</td>\n",
       "      <td>0.044051</td>\n",
       "      <td>0.072038</td>\n",
       "      <td>0.103419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5603</th>\n",
       "      <td>-0.019892</td>\n",
       "      <td>-0.034730</td>\n",
       "      <td>-0.047265</td>\n",
       "      <td>-0.054631</td>\n",
       "      <td>-0.026954</td>\n",
       "      <td>0.002579</td>\n",
       "      <td>0.007071</td>\n",
       "      <td>-0.003261</td>\n",
       "      <td>0.008565</td>\n",
       "      <td>0.019975</td>\n",
       "      <td>0.561087</td>\n",
       "      <td>0.464187</td>\n",
       "      <td>0.381445</td>\n",
       "      <td>0.333398</td>\n",
       "      <td>0.350693</td>\n",
       "      <td>0.344865</td>\n",
       "      <td>0.275331</td>\n",
       "      <td>0.209318</td>\n",
       "      <td>0.188466</td>\n",
       "      <td>0.206171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5604</th>\n",
       "      <td>0.047979</td>\n",
       "      <td>0.138255</td>\n",
       "      <td>0.121141</td>\n",
       "      <td>0.073845</td>\n",
       "      <td>0.050825</td>\n",
       "      <td>0.023558</td>\n",
       "      <td>0.002464</td>\n",
       "      <td>-0.011403</td>\n",
       "      <td>-0.019011</td>\n",
       "      <td>-0.021168</td>\n",
       "      <td>-0.131821</td>\n",
       "      <td>0.035536</td>\n",
       "      <td>0.045335</td>\n",
       "      <td>0.005460</td>\n",
       "      <td>-0.030595</td>\n",
       "      <td>-0.056825</td>\n",
       "      <td>-0.075062</td>\n",
       "      <td>-0.067410</td>\n",
       "      <td>-0.075035</td>\n",
       "      <td>-0.065493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6104</th>\n",
       "      <td>-0.135019</td>\n",
       "      <td>-0.469348</td>\n",
       "      <td>-0.601718</td>\n",
       "      <td>-0.616191</td>\n",
       "      <td>-0.556177</td>\n",
       "      <td>-0.512051</td>\n",
       "      <td>-0.472327</td>\n",
       "      <td>-0.434713</td>\n",
       "      <td>-0.399322</td>\n",
       "      <td>-0.341437</td>\n",
       "      <td>1.137056</td>\n",
       "      <td>0.598249</td>\n",
       "      <td>0.287585</td>\n",
       "      <td>0.139036</td>\n",
       "      <td>0.090300</td>\n",
       "      <td>0.081051</td>\n",
       "      <td>0.086205</td>\n",
       "      <td>0.098205</td>\n",
       "      <td>0.114662</td>\n",
       "      <td>0.097834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6105</th>\n",
       "      <td>0.683089</td>\n",
       "      <td>0.908907</td>\n",
       "      <td>0.897307</td>\n",
       "      <td>0.772712</td>\n",
       "      <td>0.645870</td>\n",
       "      <td>0.534164</td>\n",
       "      <td>0.454687</td>\n",
       "      <td>0.378453</td>\n",
       "      <td>0.335563</td>\n",
       "      <td>0.306968</td>\n",
       "      <td>1.416681</td>\n",
       "      <td>1.488015</td>\n",
       "      <td>1.375669</td>\n",
       "      <td>1.207726</td>\n",
       "      <td>1.055663</td>\n",
       "      <td>0.932966</td>\n",
       "      <td>0.837150</td>\n",
       "      <td>0.763569</td>\n",
       "      <td>0.708463</td>\n",
       "      <td>0.595871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6106</th>\n",
       "      <td>-0.003328</td>\n",
       "      <td>-0.161861</td>\n",
       "      <td>-0.211071</td>\n",
       "      <td>-0.234781</td>\n",
       "      <td>-0.243938</td>\n",
       "      <td>-0.244648</td>\n",
       "      <td>-0.240823</td>\n",
       "      <td>-0.234090</td>\n",
       "      <td>-0.225040</td>\n",
       "      <td>-0.213867</td>\n",
       "      <td>0.487938</td>\n",
       "      <td>0.265363</td>\n",
       "      <td>0.157881</td>\n",
       "      <td>0.095710</td>\n",
       "      <td>0.056991</td>\n",
       "      <td>0.033530</td>\n",
       "      <td>0.024331</td>\n",
       "      <td>0.018640</td>\n",
       "      <td>0.017186</td>\n",
       "      <td>0.020715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6107</th>\n",
       "      <td>0.914033</td>\n",
       "      <td>0.608752</td>\n",
       "      <td>0.300186</td>\n",
       "      <td>0.149531</td>\n",
       "      <td>0.089661</td>\n",
       "      <td>0.026004</td>\n",
       "      <td>0.010477</td>\n",
       "      <td>-0.026670</td>\n",
       "      <td>-0.055958</td>\n",
       "      <td>-0.076653</td>\n",
       "      <td>1.909812</td>\n",
       "      <td>1.394225</td>\n",
       "      <td>0.941408</td>\n",
       "      <td>0.663911</td>\n",
       "      <td>0.512475</td>\n",
       "      <td>0.403015</td>\n",
       "      <td>0.346634</td>\n",
       "      <td>0.284168</td>\n",
       "      <td>0.265617</td>\n",
       "      <td>0.259639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6108</th>\n",
       "      <td>-0.070369</td>\n",
       "      <td>-0.104713</td>\n",
       "      <td>0.071311</td>\n",
       "      <td>0.164608</td>\n",
       "      <td>0.172307</td>\n",
       "      <td>0.137374</td>\n",
       "      <td>0.098209</td>\n",
       "      <td>0.066351</td>\n",
       "      <td>0.042581</td>\n",
       "      <td>0.026450</td>\n",
       "      <td>0.192584</td>\n",
       "      <td>0.116951</td>\n",
       "      <td>0.265056</td>\n",
       "      <td>0.342862</td>\n",
       "      <td>0.323550</td>\n",
       "      <td>0.271288</td>\n",
       "      <td>0.221877</td>\n",
       "      <td>0.182179</td>\n",
       "      <td>0.152361</td>\n",
       "      <td>0.131725</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>509 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             surge1_t0  surge1_t1  surge1_t2  surge1_t3  surge1_t4  surge1_t5  \\\n",
       "id_sequence                                                                     \n",
       "5600         -1.199508  -1.171856  -1.131088  -1.066332  -0.990663  -0.864091   \n",
       "5601         -0.189759  -0.080710   0.080381   0.214584   0.307948   0.374662   \n",
       "5602         -0.236023  -0.227396  -0.211098  -0.195290  -0.173217  -0.148450   \n",
       "5603         -0.019892  -0.034730  -0.047265  -0.054631  -0.026954   0.002579   \n",
       "5604          0.047979   0.138255   0.121141   0.073845   0.050825   0.023558   \n",
       "...                ...        ...        ...        ...        ...        ...   \n",
       "6104         -0.135019  -0.469348  -0.601718  -0.616191  -0.556177  -0.512051   \n",
       "6105          0.683089   0.908907   0.897307   0.772712   0.645870   0.534164   \n",
       "6106         -0.003328  -0.161861  -0.211071  -0.234781  -0.243938  -0.244648   \n",
       "6107          0.914033   0.608752   0.300186   0.149531   0.089661   0.026004   \n",
       "6108         -0.070369  -0.104713   0.071311   0.164608   0.172307   0.137374   \n",
       "\n",
       "             surge1_t6  surge1_t7  surge1_t8  surge1_t9  surge2_t0  surge2_t1  \\\n",
       "id_sequence                                                                     \n",
       "5600         -0.801858  -0.746576  -0.696449  -0.651072  -0.726454  -0.739934   \n",
       "5601          0.426763   0.470630   0.509728   0.546062  -0.023364   0.019109   \n",
       "5602         -0.123184  -0.097737  -0.071892  -0.045371   0.345954   0.105727   \n",
       "5603          0.007071  -0.003261   0.008565   0.019975   0.561087   0.464187   \n",
       "5604          0.002464  -0.011403  -0.019011  -0.021168  -0.131821   0.035536   \n",
       "...                ...        ...        ...        ...        ...        ...   \n",
       "6104         -0.472327  -0.434713  -0.399322  -0.341437   1.137056   0.598249   \n",
       "6105          0.454687   0.378453   0.335563   0.306968   1.416681   1.488015   \n",
       "6106         -0.240823  -0.234090  -0.225040  -0.213867   0.487938   0.265363   \n",
       "6107          0.010477  -0.026670  -0.055958  -0.076653   1.909812   1.394225   \n",
       "6108          0.098209   0.066351   0.042581   0.026450   0.192584   0.116951   \n",
       "\n",
       "             surge2_t2  surge2_t3  surge2_t4  surge2_t5  surge2_t6  surge2_t7  \\\n",
       "id_sequence                                                                     \n",
       "5600         -0.704749  -0.652886  -0.602378  -0.553215  -0.507450  -0.465277   \n",
       "5601          0.151435   0.216138   0.207174   0.181843   0.155302   0.228899   \n",
       "5602          0.012435  -0.008099   0.004825   0.026204   0.049977   0.044051   \n",
       "5603          0.381445   0.333398   0.350693   0.344865   0.275331   0.209318   \n",
       "5604          0.045335   0.005460  -0.030595  -0.056825  -0.075062  -0.067410   \n",
       "...                ...        ...        ...        ...        ...        ...   \n",
       "6104          0.287585   0.139036   0.090300   0.081051   0.086205   0.098205   \n",
       "6105          1.375669   1.207726   1.055663   0.932966   0.837150   0.763569   \n",
       "6106          0.157881   0.095710   0.056991   0.033530   0.024331   0.018640   \n",
       "6107          0.941408   0.663911   0.512475   0.403015   0.346634   0.284168   \n",
       "6108          0.265056   0.342862   0.323550   0.271288   0.221877   0.182179   \n",
       "\n",
       "             surge2_t8  surge2_t9  \n",
       "id_sequence                        \n",
       "5600         -0.426380  -0.390044  \n",
       "5601          0.305990   0.377340  \n",
       "5602          0.072038   0.103419  \n",
       "5603          0.188466   0.206171  \n",
       "5604         -0.075035  -0.065493  \n",
       "...                ...        ...  \n",
       "6104          0.114662   0.097834  \n",
       "6105          0.708463   0.595871  \n",
       "6106          0.017186   0.020715  \n",
       "6107          0.265617   0.259639  \n",
       "6108          0.152361   0.131725  \n",
       "\n",
       "[509 rows x 20 columns]"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark\n",
    "Train using kNN of pressure fields at two instants in time, with 40 neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def surge_prediction_metric(dataframe_y_true, dataframe_y_pred):\n",
    "    weights = np.linspace(1, 0.1, 10)[np.newaxis]\n",
    "    surge1_columns = [\n",
    "        'surge1_t0', 'surge1_t1', 'surge1_t2', 'surge1_t3', 'surge1_t4',\n",
    "        'surge1_t5', 'surge1_t6', 'surge1_t7', 'surge1_t8', 'surge1_t9' ]\n",
    "    surge2_columns = [\n",
    "        'surge2_t0', 'surge2_t1', 'surge2_t2', 'surge2_t3', 'surge2_t4',\n",
    "        'surge2_t5', 'surge2_t6', 'surge2_t7', 'surge2_t8', 'surge2_t9' ]\n",
    "    surge1_score = (weights * (dataframe_y_true[surge1_columns].values - dataframe_y_pred[surge1_columns].values)**2).mean()\n",
    "    surge2_score = (weights * (dataframe_y_true[surge2_columns].values - dataframe_y_pred[surge2_columns].values)**2).mean()\n",
    "\n",
    "    return surge1_score + surge2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfields = 2; time_step_slp = 8\n",
    "slp_train = []\n",
    "slp_all = X_train['slp']\n",
    "for i in range(5559):\n",
    "    slp_train.append(np.ndarray.flatten(slp_all[i,-1]))\n",
    "    for j in range(1,nfields):\n",
    "        slp_train[-1] = np.concatenate( ( slp_train[-1], np.ndarray.flatten(slp_all[i,-1-j*time_step_slp]) ) )\n",
    "slp_train = np.array(slp_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "slp_test = []\n",
    "slp_all_test = X_test['slp']\n",
    "for i in range(509):\n",
    "    slp_test.append(np.ndarray.flatten(slp_all_test[i,-1]))\n",
    "    for j in range(1,nfields):\n",
    "        slp_test[-1] = np.concatenate( ( slp_test[-1], np.ndarray.flatten(slp_all_test[i,-1-j*time_step_slp]) ) )\n",
    "slp_test = np.array(slp_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = BallTree(slp_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "surge_test_benchmark = []; k = 40\n",
    "for i in range(509):\n",
    "    dist, ind = tree.query([slp_test[i]], k=k)\n",
    "    surge_test_benchmark.append(np.mean(surge_train[ind[0]], axis=0))\n",
    "surge_test_benchmark = np.array(surge_test_benchmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_columns = [f'surge1_t{i}' for i in range(10)] + [f'surge2_t{i}' for i in range(10)]\n",
    "y_test_benchmark = pd.DataFrame(data=surge_test_benchmark, columns=y_columns, index=X_test['id_sequence'])\n",
    "y_test_benchmark.to_csv('Y_test_benchmark.csv', index_label='id_sequence', sep=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
