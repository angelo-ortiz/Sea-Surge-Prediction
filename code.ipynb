{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports, data load, metric function definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-22T06:57:38.110735Z",
     "start_time": "2022-03-22T06:57:37.640740Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import BallTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-22T06:57:39.038683Z",
     "start_time": "2022-03-22T06:57:39.007102Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = np.load('X_train_surge.npz')\n",
    "Y_train = pd.read_csv('Y_train_surge.csv')\n",
    "X_test = np.load('X_test_surge.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-22T06:57:42.769417Z",
     "start_time": "2022-03-22T06:57:39.877082Z"
    }
   },
   "outputs": [],
   "source": [
    "def split_train_set(X, Y, val_size=0.2, seed=None):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    \n",
    "    nb_examples = len(Y)\n",
    "    val_examples = int(val_size * nb_examples)\n",
    "    \n",
    "    val_indices = rng.choice(nb_examples, size=val_examples, replace=False)\n",
    "    \n",
    "    train_indices = np.setdiff1d(\n",
    "        np.arange(nb_examples),\n",
    "        val_indices,\n",
    "        assume_unique=True\n",
    "    )\n",
    "    train_indices = rng.permutation(train_indices)\n",
    "    \n",
    "    X_train = {}\n",
    "    X_val = {}\n",
    "    for feat in X.files:\n",
    "        X_train[feat] = X[feat][train_indices]\n",
    "        X_val[feat] = X[feat][val_indices]\n",
    "    \n",
    "    return X_train, Y.iloc[train_indices], \\\n",
    "            X_val, Y.iloc[val_indices]\n",
    "\n",
    "X_train, Y_train, X_val, Y_val = split_train_set(X_train, Y_train, val_size=0.091, seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-22T06:57:43.770623Z",
     "start_time": "2022-03-22T06:57:43.755876Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_slp = X_train['t_slp'] / 3600\n",
    "t_slp_delta = t_slp - t_slp[:, 0].reshape(-1, 1)\n",
    "np.allclose(np.round(t_slp_delta), np.round(t_slp_delta)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-22T06:57:48.728017Z",
     "start_time": "2022-03-22T06:57:46.425196Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-22T06:57:51.169259Z",
     "start_time": "2022-03-22T06:57:51.164177Z"
    }
   },
   "outputs": [],
   "source": [
    "SLP_HEIGHT = SLP_WIDTH = 41\n",
    "SLP_PER_EX = 40\n",
    "T_SURGE_NORMALISATION = 240.\n",
    "SLP_REL_TIMESTAMPS = np.arange(24*5, step=3) / T_SURGE_NORMALISATION\n",
    "\n",
    "\n",
    "def normalised_tensor(array, mean, std):\n",
    "        return torch.from_numpy((array - mean) / std)\n",
    "\n",
    "def preprocessing(X, slp_mean=None, slp_std=None, t_slp_mean=None, t_slp_std=None,\n",
    "                 surge_mean=None, surge_std=None):\n",
    "    slp = X['slp'].reshape(-1, SLP_PER_EX, SLP_HEIGHT, SLP_WIDTH)\n",
    "    slp = np.roll(slp, shift=-11, axis=3)\n",
    "    if slp_mean is None:\n",
    "        slp_mean = np.mean(slp)\n",
    "    if slp_std is None:\n",
    "        slp_std = np.std(slp)\n",
    "    slp = normalised_tensor(slp, slp_mean, slp_std)\n",
    "    \n",
    "    fst_slp = X['t_slp'][:, 0] / 3600\n",
    "    fst_slp_tmp = fst_slp.reshape(-1, 1)\n",
    "    \n",
    "    def rel_surge_time(index):\n",
    "        t_surge = X[index] / 3600\n",
    "        t_surge -= fst_slp_tmp\n",
    "        return torch.from_numpy(t_surge / T_SURGE_NORMALISATION)\n",
    "\n",
    "    t_surge1_in = rel_surge_time('t_surge1_input')\n",
    "    t_surge2_in = rel_surge_time('t_surge2_input')\n",
    "    t_surge1_out = rel_surge_time('t_surge1_output')\n",
    "    t_surge2_out = rel_surge_time('t_surge2_output')\n",
    "    \n",
    "    if t_slp_mean is None:\n",
    "        t_slp_mean = np.mean(fst_slp)\n",
    "    if t_slp_std is None:\n",
    "        t_slp_std = np.std(fst_slp)\n",
    "    fst_slp = normalised_tensor(fst_slp, t_slp_mean, t_slp_std)\n",
    "    \n",
    "    surge1 = X['surge1_input']\n",
    "    surge2 = X['surge2_input']\n",
    "    if surge_mean is None or surge_std is None:\n",
    "        surges = np.concatenate([surge1, surge2], axis=None)\n",
    "        surge_mean = np.mean(surges)\n",
    "        surge_std = np.std(surges)\n",
    "    surge1_in = normalised_tensor(surge1, surge_mean, surge_std)\n",
    "    surge2_in = normalised_tensor(surge2, surge_mean, surge_std)\n",
    "    \n",
    "    return X['id_sequence'], slp, slp_mean, slp_std, \\\n",
    "            fst_slp, t_slp_mean, t_slp_std, \\\n",
    "            t_surge1_in, t_surge2_in, t_surge1_out, t_surge2_out, \\\n",
    "            surge1_in, surge2_in, surge_mean, surge_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-22T06:57:53.560037Z",
     "start_time": "2022-03-22T06:57:52.394226Z"
    }
   },
   "outputs": [],
   "source": [
    "train_id_seq, train_slp, slp_mean, slp_std, \\\n",
    "train_fst_slp, t_slp_mean, t_slp_std, \\\n",
    "train_t_surge1_in, train_t_surge2_in, train_t_surge1_out, train_t_surge2_out, \\\n",
    "train_surge1_in, train_surge2_in, surge_mean, surge_std = preprocessing(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-22T06:57:53.627279Z",
     "start_time": "2022-03-22T06:57:53.561203Z"
    }
   },
   "outputs": [],
   "source": [
    "val_id_seq, val_slp, _, _, \\\n",
    "val_fst_slp, _, _, \\\n",
    "val_t_surge1_in, val_t_surge2_in, val_t_surge1_out, val_t_surge2_out, \\\n",
    "val_surge1_in, val_surge2_in, _, _ = preprocessing(X_val, slp_mean, slp_std, t_slp_mean, t_slp_std, surge_mean, surge_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-22T06:57:53.834508Z",
     "start_time": "2022-03-22T06:57:53.628231Z"
    }
   },
   "outputs": [],
   "source": [
    "test_id_seq, test_slp, _, _, \\\n",
    "test_fst_slp, _, _, \\\n",
    "test_t_surge1_in, test_t_surge2_in, test_t_surge1_out, test_t_surge2_out, \\\n",
    "test_surge1_in, test_surge2_in, _, _ = preprocessing(X_test, slp_mean, slp_std, t_slp_mean, t_slp_std, surge_mean, surge_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-22T06:57:54.568335Z",
     "start_time": "2022-03-22T06:57:54.555829Z"
    }
   },
   "outputs": [],
   "source": [
    "train_Y_id_seq = Y_train['id_sequence'].values\n",
    "assert np.alltrue(train_Y_id_seq == train_id_seq), 'Data/label index mismatch'\n",
    "\n",
    "train_surge1_out = Y_train.iloc[:, 1:11].values\n",
    "train_surge1_out = normalised_tensor(train_surge1_out, surge_mean, surge_std)\n",
    "train_surge2_out = Y_train.iloc[:, 11:].values\n",
    "train_surge2_out = normalised_tensor(train_surge2_out, surge_mean, surge_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-22T06:57:55.323690Z",
     "start_time": "2022-03-22T06:57:55.320668Z"
    }
   },
   "outputs": [],
   "source": [
    "val_Y_id_seq = Y_val['id_sequence'].values\n",
    "assert np.alltrue(val_Y_id_seq == val_id_seq), 'Data/label index mismatch'\n",
    "\n",
    "val_surge1_out = Y_val.iloc[:, 1:11].values\n",
    "val_surge1_out = normalised_tensor(val_surge1_out, surge_mean, surge_std)\n",
    "val_surge2_out = Y_val.iloc[:, 11:].values\n",
    "val_surge2_out = normalised_tensor(val_surge2_out, surge_mean, surge_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-22T04:22:09.128857Z",
     "start_time": "2022-03-22T04:22:09.114659Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "resnet_versions = {\n",
    "    18 : models.resnet18,\n",
    "    34 : models.resnet34,\n",
    "    50 : models.resnet50,\n",
    "}\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_dataset = TensorDataset(\n",
    "    train_slp,\n",
    "#     train_fst_slp,\n",
    "    train_t_surge1_in,\n",
    "    train_surge1_in,\n",
    "    train_t_surge2_in,\n",
    "    train_surge2_in,\n",
    "    train_t_surge1_out,\n",
    "    train_surge1_out,\n",
    "    train_t_surge2_out,\n",
    "    train_surge2_out,\n",
    ")\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "val_dataset = TensorDataset(\n",
    "    val_slp,\n",
    "#     val_fst_slp,\n",
    "    val_t_surge1_in,\n",
    "    val_surge1_in,\n",
    "    val_t_surge2_in,\n",
    "    val_surge2_in,\n",
    "    val_t_surge1_out,\n",
    "    val_surge1_out,\n",
    "    val_t_surge2_out,\n",
    "    val_surge2_out,\n",
    ")\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "test_dataset = TensorDataset(\n",
    "    test_slp,\n",
    "#     test_fst_slp,\n",
    "    test_t_surge1_in,\n",
    "    test_surge1_in,\n",
    "    test_t_surge2_in,\n",
    "    test_surge2_in,\n",
    "    test_t_surge1_out,\n",
    "    test_t_surge2_out\n",
    ")\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "class Network(nn.Module):\n",
    "    \n",
    "    def __init__(self, resnet_layers, lstm_hidden_size, lstm_layers=1, dropout=.2):\n",
    "        super().__init__()\n",
    "        self.resnet = resnet_versions[resnet_layers]()\n",
    "        self.resnet_layers = resnet_layers\n",
    "        self.resnet.conv1 = nn.Conv2d(SLP_PER_EX, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "#         self.resnet.fc = nn.Linear(self.resnet.fc.in_features, lstm_hidden_size)\n",
    "\n",
    "        self.lstm_hidden_size = lstm_hidden_size\n",
    "        self.lstm_layers = lstm_layers\n",
    "        self.fc0 = nn.Linear(self.resnet.fc.out_features, lstm_hidden_size)\n",
    "        self.fc1 = nn.Linear(20, 1)\n",
    "        self.fc2 = nn.Linear(20, 1)\n",
    "        self.lstm = nn.LSTM(input_size=1, hidden_size=lstm_hidden_size, num_layers=lstm_layers, bias=True, batch_first=True, \\\n",
    "                            dropout=dropout, bidirectional=False, proj_size=1)\n",
    "        \n",
    "    def __str__(self):\n",
    "        return f'resnet{self.resnet_layers}_{self.lstm_hidden_size}_{self.lstm_layers}'\n",
    "        \n",
    "    def forward(self, slp, t_surge1_in, surge1_in, t_surge2_in, surge2_in, t_surge1_out, t_surge2_out):\n",
    "        cell = self.resnet(slp)\n",
    "        cell = self.fc0(cell)\n",
    "        cell = torch.cat([cell, cell], dim=0).unsqueeze(0)\n",
    "        c_0 = torch.cat([cell]*self.lstm_layers, dim=0)\n",
    "        \n",
    "        surge1 = torch.cat([t_surge1_in, surge1_in], dim=1)\n",
    "        hidden1 = self.fc1(surge1)\n",
    "        surge2 = torch.cat([t_surge2_in, surge2_in], dim=1)\n",
    "        hidden2 = self.fc2(surge2)\n",
    "        hidden = torch.cat([hidden1, hidden2], dim=0).unsqueeze(0)\n",
    "        h_0 = torch.cat([hidden]*self.lstm_layers, dim=0)\n",
    "        \n",
    "        lstm_input = torch.cat([t_surge1_out, t_surge2_out], dim=0).unsqueeze(2)\n",
    "        output, _ = self.lstm(lstm_input, (h_0, c_0))\n",
    "        output = output.squeeze()\n",
    "        return output[:len(slp)], output[len(slp):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-22T07:04:09.280780Z",
     "start_time": "2022-03-22T07:04:09.235748Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "resnet_versions = {\n",
    "    18 : models.resnet18,\n",
    "    34 : models.resnet34,\n",
    "    50 : models.resnet50,\n",
    "}\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_dataset = TensorDataset(\n",
    "    train_slp,\n",
    "#     train_fst_slp,\n",
    "    train_t_surge1_in,\n",
    "    train_surge1_in,\n",
    "    train_t_surge2_in,\n",
    "    train_surge2_in,\n",
    "    train_t_surge1_out,\n",
    "    train_surge1_out,\n",
    "    train_t_surge2_out,\n",
    "    train_surge2_out,\n",
    ")\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "val_dataset = TensorDataset(\n",
    "    val_slp,\n",
    "#     val_fst_slp,\n",
    "    val_t_surge1_in,\n",
    "    val_surge1_in,\n",
    "    val_t_surge2_in,\n",
    "    val_surge2_in,\n",
    "    val_t_surge1_out,\n",
    "    val_surge1_out,\n",
    "    val_t_surge2_out,\n",
    "    val_surge2_out,\n",
    ")\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "test_dataset = TensorDataset(\n",
    "    test_slp,\n",
    "#     test_fst_slp,\n",
    "    test_t_surge1_in,\n",
    "    test_surge1_in,\n",
    "    test_t_surge2_in,\n",
    "    test_surge2_in,\n",
    "    test_t_surge1_out,\n",
    "    test_t_surge2_out\n",
    ")\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "class Network(nn.Module):\n",
    "    \n",
    "    def __init__(self, resnet_layers, lstm_hidden_size, lstm_layers=1, dropout=.2):\n",
    "        super().__init__()\n",
    "        self.resnet = resnet_versions[resnet_layers]()\n",
    "        self.resnet_layers = resnet_layers\n",
    "        self.resnet.conv1 = nn.Conv2d(SLP_PER_EX, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "#         self.resnet.fc = nn.Linear(self.resnet.fc.in_features, lstm_hidden_size)\n",
    "\n",
    "        self.lstm_hidden_size = lstm_hidden_size\n",
    "        self.lstm_layers = lstm_layers\n",
    "        self.fc0 = nn.Linear(self.resnet.fc.out_features, lstm_hidden_size)\n",
    "        self.fc1 = nn.Linear(20, lstm_hidden_size)\n",
    "        self.fc2 = nn.Linear(20, lstm_hidden_size)\n",
    "        self.lstm = nn.LSTM(input_size=1, hidden_size=lstm_hidden_size, num_layers=lstm_layers, bias=True, batch_first=True, \\\n",
    "                            dropout=dropout, bidirectional=False, proj_size=1)\n",
    "        \n",
    "    def __str__(self):\n",
    "        return f'resnet{self.resnet_layers}_{self.lstm_hidden_size}_{self.lstm_layers}'\n",
    "        \n",
    "    def forward(self, slp, t_surge1_in, surge1_in, t_surge2_in, surge2_in, t_surge1_out, t_surge2_out):\n",
    "        cell = self.resnet(slp)\n",
    "        cell = self.fc0(cell)\n",
    "        cell = torch.cat([cell, cell], dim=0).unsqueeze(0)\n",
    "        \n",
    "        surge1 = torch.cat([t_surge1_in, surge1_in], dim=1)\n",
    "        hidden1 = self.fc1(surge1)\n",
    "        surge2 = torch.cat([t_surge2_in, surge2_in], dim=1)\n",
    "        hidden2 = self.fc2(surge2)\n",
    "        hidden = torch.cat([hidden1, hidden2], dim=0).unsqueeze(0)\n",
    "        cell = cell + hidden\n",
    "        c_0 = torch.cat([cell]*self.lstm_layers, dim=0)\n",
    "        h_0 = torch.randn(self.lstm_layers, c_0.shape[1], 1)\n",
    "        \n",
    "        lstm_input = torch.cat([t_surge1_out, t_surge2_out], dim=0).unsqueeze(2)\n",
    "        output, _ = self.lstm(lstm_input, (h_0, c_0))\n",
    "        output = output.squeeze()\n",
    "        return output[:len(slp)], output[len(slp):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-22T07:04:12.309452Z",
     "start_time": "2022-03-22T07:04:09.948689Z"
    }
   },
   "outputs": [],
   "source": [
    "loss_weights = torch.linspace(1, 0.1, 10, requires_grad=False).to(device)\n",
    "    \n",
    "def surge_prediction_metric(surge1_true, surge2_true, surge1_pred, surge2_pred):\n",
    "    surge1_score = torch.mean(torch.square(surge1_true - surge1_pred) * loss_weights)\n",
    "    surge2_score = torch.mean(torch.square(surge2_true - surge2_pred) * loss_weights)\n",
    "    return surge1_score + surge2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-22T07:04:12.317644Z",
     "start_time": "2022-03-22T07:04:12.310467Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, loss_fn, optmiser, epochs):\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "    \n",
    "    for epoch_num in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = []\n",
    "\n",
    "        for batch, (slp, t_surge1_in, surge1_in, t_surge2_in, surge2_in, t_surge1_out, surge1_out, t_surge2_out, surge2_out) in enumerate(train_dataloader):\n",
    "            slp = slp.to(device)\n",
    "            t_surge1_in = t_surge1_in.to(device)\n",
    "            surge1_in = surge1_in.to(device)\n",
    "            t_surge2_in = t_surge2_in.to(device)\n",
    "            surge2_in = surge2_in.to(device)\n",
    "            t_surge1_out = t_surge1_out.to(device)\n",
    "            surge1_out = surge1_out.to(device)\n",
    "            t_surge2_out = t_surge2_out.to(device)\n",
    "            surge2_out = surge2_out.to(device)\n",
    "            surge1_out_pred, surge2_out_pred = model(slp, t_surge1_in, surge1_in, t_surge2_in, surge2_in, t_surge1_out, t_surge2_out)\n",
    "            \n",
    "            loss = loss_fn(surge1_out, surge2_out, surge1_out_pred, surge2_out_pred)\n",
    "            running_loss.append(loss.item())\n",
    "            \n",
    "            optimiser.zero_grad()\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            \n",
    "        epoch_loss = np.mean(running_loss)\n",
    "        train_loss.append(epoch_loss)\n",
    "        print(f'Epoch {epoch_num+1:03d} | Loss: {epoch_loss:.4f}')\n",
    "        \n",
    "        if epoch_num % 5 == 0:\n",
    "            val_loss = evaluate(model, loss_fn)\n",
    "            print(f'\\tVal loss: {val_loss:.4f}', end=' ')\n",
    "            if val_loss < .52:\n",
    "                torch.save(model.state_dict(), str(model) + f'_{epoch_num}.pth')\n",
    "                print('saved!')\n",
    "            else:\n",
    "                print()\n",
    "            \n",
    "    return train_loss\n",
    "\n",
    "def evaluate(model, loss_fn):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        losses = []\n",
    "        for slp, t_surge1_in, surge1_in, t_surge2_in, surge2_in, t_surge1_out, surge1_out, t_surge2_out, surge2_out in val_dataloader:\n",
    "            slp = slp.to(device)\n",
    "            t_surge1_in = t_surge1_in.to(device)\n",
    "            surge1_in = surge1_in.to(device)\n",
    "            t_surge2_in = t_surge2_in.to(device)\n",
    "            surge2_in = surge2_in.to(device)\n",
    "            t_surge1_out = t_surge1_out.to(device)\n",
    "            surge1_out = surge1_out.to(device)\n",
    "            t_surge2_out = t_surge2_out.to(device)\n",
    "            surge2_out = surge2_out.to(device)\n",
    "            surge1_out_pred, surge2_out_pred = model(slp, t_surge1_in, surge1_in, t_surge2_in, surge2_in, t_surge1_out, t_surge2_out)\n",
    "            \n",
    "            loss = loss_fn(surge1_out, surge2_out, surge1_out_pred, surge2_out_pred)\n",
    "            losses.append(loss.item())\n",
    "        return np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-22T03:42:33.549718Z",
     "start_time": "2022-03-22T03:40:17.819700Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | Loss: 0.9974\n",
      "\tVal loss: 0.8707 \n",
      "Epoch 002 | Loss: 0.8157\n",
      "Epoch 003 | Loss: 0.7021\n",
      "Epoch 004 | Loss: 0.6705\n",
      "Epoch 005 | Loss: 0.6458\n",
      "Epoch 006 | Loss: 0.6238\n",
      "\tVal loss: 0.6041 \n",
      "Epoch 007 | Loss: 0.6120\n",
      "Epoch 008 | Loss: 0.5934\n",
      "Epoch 009 | Loss: 0.5767\n",
      "Epoch 010 | Loss: 0.5643\n",
      "Epoch 011 | Loss: 0.5542\n",
      "\tVal loss: 0.5512 \n",
      "Epoch 012 | Loss: 0.5388\n",
      "Epoch 013 | Loss: 0.5298\n",
      "Epoch 014 | Loss: 0.5188\n",
      "Epoch 015 | Loss: 0.5061\n",
      "Epoch 016 | Loss: 0.5006\n",
      "\tVal loss: 0.5787 \n",
      "Epoch 017 | Loss: 0.4950\n",
      "Epoch 018 | Loss: 0.4706\n",
      "Epoch 019 | Loss: 0.4635\n",
      "Epoch 020 | Loss: 0.4596\n",
      "Epoch 021 | Loss: 0.4483\n",
      "\tVal loss: 0.5558 \n",
      "Epoch 022 | Loss: 0.4345\n",
      "Epoch 023 | Loss: 0.4370\n",
      "Epoch 024 | Loss: 0.4279\n",
      "Epoch 025 | Loss: 0.4355\n",
      "Epoch 026 | Loss: 0.4284\n",
      "\tVal loss: 0.5327 \n",
      "Epoch 027 | Loss: 0.4245\n",
      "Epoch 028 | Loss: 0.4101\n",
      "Epoch 029 | Loss: 0.3909\n",
      "Epoch 030 | Loss: 0.4134\n",
      "Epoch 031 | Loss: 0.3911\n",
      "\tVal loss: 0.5426 \n",
      "Epoch 032 | Loss: 0.3980\n",
      "Epoch 033 | Loss: 0.4332\n",
      "Epoch 034 | Loss: 0.4151\n",
      "Epoch 035 | Loss: 0.3686\n",
      "Epoch 036 | Loss: 0.3488\n",
      "\tVal loss: 0.5533 \n",
      "Epoch 037 | Loss: 0.3427\n",
      "Epoch 038 | Loss: 0.3493\n",
      "Epoch 039 | Loss: 0.3473\n",
      "Epoch 040 | Loss: 0.3446\n",
      "Epoch 041 | Loss: 0.3505\n",
      "\tVal loss: 0.5274 \n",
      "Epoch 042 | Loss: 0.3585\n",
      "Epoch 043 | Loss: 0.3599\n",
      "Epoch 044 | Loss: 0.3572\n",
      "Epoch 045 | Loss: 0.3587\n",
      "Epoch 046 | Loss: 0.3745\n",
      "\tVal loss: 0.6419 \n",
      "Epoch 047 | Loss: 0.3729\n",
      "Epoch 048 | Loss: 0.3636\n",
      "Epoch 049 | Loss: 0.3295\n",
      "Epoch 050 | Loss: 0.3118\n",
      "Epoch 051 | Loss: 0.3134\n",
      "\tVal loss: 0.5415 \n",
      "Epoch 052 | Loss: 0.3222\n",
      "Epoch 053 | Loss: 0.3368\n",
      "Epoch 054 | Loss: 0.3448\n",
      "Epoch 055 | Loss: 0.3278\n",
      "Epoch 056 | Loss: 0.3163\n",
      "\tVal loss: 0.5475 \n",
      "Epoch 057 | Loss: 0.3043\n",
      "Epoch 058 | Loss: 0.3026\n",
      "Epoch 059 | Loss: 0.3130\n",
      "Epoch 060 | Loss: 0.3294\n",
      "Epoch 061 | Loss: 0.3271\n",
      "\tVal loss: 0.5475 \n",
      "Epoch 062 | Loss: 0.3143\n",
      "Epoch 063 | Loss: 0.3060\n",
      "Epoch 064 | Loss: 0.3187\n",
      "Epoch 065 | Loss: 0.3204\n",
      "Epoch 066 | Loss: 0.3250\n",
      "\tVal loss: 0.6440 \n",
      "Epoch 067 | Loss: 0.3322\n",
      "Epoch 068 | Loss: 0.3421\n",
      "Epoch 069 | Loss: 0.3429\n",
      "Epoch 070 | Loss: 0.3084\n",
      "Epoch 071 | Loss: 0.2940\n",
      "\tVal loss: 0.5464 \n",
      "Epoch 072 | Loss: 0.2904\n",
      "Epoch 073 | Loss: 0.2905\n",
      "Epoch 074 | Loss: 0.2892\n",
      "Epoch 075 | Loss: 0.2949\n",
      "Epoch 076 | Loss: 0.3095\n",
      "\tVal loss: 0.5720 \n",
      "Epoch 077 | Loss: 0.3125\n",
      "Epoch 078 | Loss: 0.3063\n",
      "Epoch 079 | Loss: 0.2969\n",
      "Epoch 080 | Loss: 0.2853\n",
      "Epoch 081 | Loss: 0.2792\n",
      "\tVal loss: 0.5598 \n",
      "Epoch 082 | Loss: 0.2777\n",
      "Epoch 083 | Loss: 0.2898\n",
      "Epoch 084 | Loss: 0.2828\n",
      "Epoch 085 | Loss: 0.2783\n",
      "Epoch 086 | Loss: 0.2854\n",
      "\tVal loss: 0.5638 \n",
      "Epoch 087 | Loss: 0.3030\n",
      "Epoch 088 | Loss: 0.2980\n",
      "Epoch 089 | Loss: 0.2823\n",
      "Epoch 090 | Loss: 0.2792\n",
      "Epoch 091 | Loss: 0.2851\n",
      "\tVal loss: 0.5916 \n",
      "Epoch 092 | Loss: 0.2879\n",
      "Epoch 093 | Loss: 0.2887\n",
      "Epoch 094 | Loss: 0.2928\n",
      "Epoch 095 | Loss: 0.2811\n",
      "Epoch 096 | Loss: 0.2702\n",
      "\tVal loss: 0.6126 \n",
      "Epoch 097 | Loss: 0.2701\n",
      "Epoch 098 | Loss: 0.2676\n",
      "Epoch 099 | Loss: 0.2643\n",
      "Epoch 100 | Loss: 0.2598\n"
     ]
    }
   ],
   "source": [
    "resnet18 = Network(18, 100, 1).to(device)\n",
    "optimiser = optim.Adam(resnet18.parameters())\n",
    "_ = train(resnet18, surge_prediction_metric, optimiser, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-22T02:34:32.316958Z",
     "start_time": "2022-03-22T02:31:00.746544Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | Loss: 1.0114\n",
      "\tVal loss: 0.8565\n",
      "Epoch 002 | Loss: 0.7872\n",
      "Epoch 003 | Loss: 0.6887\n",
      "Epoch 004 | Loss: 0.6610\n",
      "Epoch 005 | Loss: 0.6327\n",
      "Epoch 006 | Loss: 0.6371\n",
      "\tVal loss: 0.6249\n",
      "Epoch 007 | Loss: 0.6212\n",
      "Epoch 008 | Loss: 0.6068\n",
      "Epoch 009 | Loss: 0.6165\n",
      "Epoch 010 | Loss: 0.6224\n",
      "Epoch 011 | Loss: 0.6105\n",
      "\tVal loss: 0.5638\n",
      "Epoch 012 | Loss: 0.5944\n",
      "Epoch 013 | Loss: 0.5824\n",
      "Epoch 014 | Loss: 0.5961\n",
      "Epoch 015 | Loss: 0.6069\n",
      "Epoch 016 | Loss: 0.6043\n",
      "\tVal loss: 0.5853\n",
      "Epoch 017 | Loss: 0.6250\n",
      "Epoch 018 | Loss: 0.6136\n",
      "Epoch 019 | Loss: 0.5964\n",
      "Epoch 020 | Loss: 0.5831\n",
      "Epoch 021 | Loss: 0.5742\n",
      "\tVal loss: 0.5372\n",
      "Epoch 022 | Loss: 0.5690\n",
      "Epoch 023 | Loss: 0.5690\n",
      "Epoch 024 | Loss: 0.5611\n",
      "Epoch 025 | Loss: 0.5555\n",
      "Epoch 026 | Loss: 0.5572\n",
      "\tVal loss: 0.5285\n",
      "Epoch 027 | Loss: 0.5504\n",
      "Epoch 028 | Loss: 0.5456\n",
      "Epoch 029 | Loss: 0.5428\n",
      "Epoch 030 | Loss: 0.5438\n",
      "Epoch 031 | Loss: 0.5359\n",
      "\tVal loss: 0.5194\n",
      "Epoch 032 | Loss: 0.5350\n",
      "Epoch 033 | Loss: 0.5391\n",
      "Epoch 034 | Loss: 0.5378\n",
      "Epoch 035 | Loss: 0.5353\n",
      "Epoch 036 | Loss: 0.5236\n",
      "\tVal loss: 0.5091\n",
      "Epoch 037 | Loss: 0.5343\n",
      "Epoch 038 | Loss: 0.5228\n",
      "Epoch 039 | Loss: 0.5134\n",
      "Epoch 040 | Loss: 0.5038\n",
      "Epoch 041 | Loss: 0.5091\n",
      "\tVal loss: 0.5087\n",
      "Epoch 042 | Loss: 0.5048\n",
      "Epoch 043 | Loss: 0.5029\n",
      "Epoch 044 | Loss: 0.5069\n",
      "Epoch 045 | Loss: 0.5176\n",
      "Epoch 046 | Loss: 0.5063\n",
      "\tVal loss: 0.5022\n",
      "Epoch 047 | Loss: 0.4927\n",
      "Epoch 048 | Loss: 0.4902\n",
      "Epoch 049 | Loss: 0.4756\n",
      "Epoch 050 | Loss: 0.4757\n",
      "Epoch 051 | Loss: 0.4811\n",
      "\tVal loss: 0.5174\n",
      "Epoch 052 | Loss: 0.4613\n",
      "Epoch 053 | Loss: 0.4551\n",
      "Epoch 054 | Loss: 0.4453\n",
      "Epoch 055 | Loss: 0.4454\n",
      "Epoch 056 | Loss: 0.4699\n",
      "\tVal loss: 0.5159\n",
      "Epoch 057 | Loss: 0.4451\n",
      "Epoch 058 | Loss: 0.4295\n",
      "Epoch 059 | Loss: 0.4250\n",
      "Epoch 060 | Loss: 0.4188\n",
      "Epoch 061 | Loss: 0.4235\n",
      "\tVal loss: 0.5258\n",
      "Epoch 062 | Loss: 0.4097\n",
      "Epoch 063 | Loss: 0.4041\n",
      "Epoch 064 | Loss: 0.4080\n",
      "Epoch 065 | Loss: 0.4053\n",
      "Epoch 066 | Loss: 0.4031\n",
      "\tVal loss: 0.5450\n",
      "Epoch 067 | Loss: 0.3965\n",
      "Epoch 068 | Loss: 0.4024\n",
      "Epoch 069 | Loss: 0.4346\n",
      "Epoch 070 | Loss: 0.4192\n",
      "Epoch 071 | Loss: 0.3962\n",
      "\tVal loss: 0.5357\n",
      "Epoch 072 | Loss: 0.3880\n",
      "Epoch 073 | Loss: 0.3841\n",
      "Epoch 074 | Loss: 0.3869\n",
      "Epoch 075 | Loss: 0.3659\n",
      "Epoch 076 | Loss: 0.3641\n",
      "\tVal loss: 0.5695\n",
      "Epoch 077 | Loss: 0.3711\n",
      "Epoch 078 | Loss: 0.3568\n",
      "Epoch 079 | Loss: 0.3532\n",
      "Epoch 080 | Loss: 0.3711\n",
      "Epoch 081 | Loss: 0.3694\n",
      "\tVal loss: 0.6356\n",
      "Epoch 082 | Loss: 0.3563\n",
      "Epoch 083 | Loss: 0.3570\n",
      "Epoch 084 | Loss: 0.3613\n",
      "Epoch 085 | Loss: 0.3595\n",
      "Epoch 086 | Loss: 0.3692\n",
      "\tVal loss: 0.5543\n",
      "Epoch 087 | Loss: 0.4021\n",
      "Epoch 088 | Loss: 0.3746\n",
      "Epoch 089 | Loss: 0.3617\n",
      "Epoch 090 | Loss: 0.3596\n",
      "Epoch 091 | Loss: 0.3416\n",
      "\tVal loss: 0.5611\n",
      "Epoch 092 | Loss: 0.3381\n",
      "Epoch 093 | Loss: 0.3265\n",
      "Epoch 094 | Loss: 0.3658\n",
      "Epoch 095 | Loss: 0.3371\n",
      "Epoch 096 | Loss: 0.3251\n",
      "\tVal loss: 0.5682\n",
      "Epoch 097 | Loss: 0.3363\n",
      "Epoch 098 | Loss: 0.3341\n",
      "Epoch 099 | Loss: 0.3149\n",
      "Epoch 100 | Loss: 0.2960\n"
     ]
    }
   ],
   "source": [
    "resnet34 = Network(34, 100).to(device)\n",
    "optimiser = optim.Adam(resnet34.parameters())\n",
    "_ = train(resnet34, surge_prediction_metric, optimiser, epochs=100)\n",
    "torch.save(resnet34.state_dict(), 'resnet34_100.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-22T04:21:49.283406Z",
     "start_time": "2022-03-22T04:20:52.703228Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | Loss: 1.0465\n",
      "\tVal loss: 0.9183 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [328]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m resnet34 \u001b[38;5;241m=\u001b[39m Network(\u001b[38;5;241m34\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      2\u001b[0m optimiser \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(resnet34\u001b[38;5;241m.\u001b[39mparameters())\n\u001b[0;32m----> 3\u001b[0m _ \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresnet34\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msurge_prediction_metric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimiser\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [315]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, loss_fn, optmiser, epochs)\u001b[0m\n\u001b[1;32m     19\u001b[0m surge1_out_pred, surge2_out_pred \u001b[38;5;241m=\u001b[39m model(slp, t_surge1_in, surge1_in, t_surge2_in, surge2_in, t_surge1_out, t_surge2_out)\n\u001b[1;32m     21\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(surge1_out, surge2_out, surge1_out_pred, surge2_out_pred)\n\u001b[0;32m---> 22\u001b[0m running_loss\u001b[38;5;241m.\u001b[39mappend(\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     24\u001b[0m optimiser\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     25\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "resnet34 = Network(34, 100, 1).to(device)\n",
    "optimiser = optim.Adam(resnet34.parameters())\n",
    "_ = train(resnet34, surge_prediction_metric, optimiser, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-22T04:18:05.646869Z",
     "start_time": "2022-03-22T04:14:33.804021Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | Loss: 1.0118\n",
      "\tVal loss: 0.8851 \n",
      "Epoch 002 | Loss: 0.9092\n",
      "Epoch 003 | Loss: 0.8304\n",
      "Epoch 004 | Loss: 0.7378\n",
      "Epoch 005 | Loss: 0.6844\n",
      "Epoch 006 | Loss: 0.6760\n",
      "\tVal loss: 0.7541 \n",
      "Epoch 007 | Loss: 0.7078\n",
      "Epoch 008 | Loss: 0.6507\n",
      "Epoch 009 | Loss: 0.6355\n",
      "Epoch 010 | Loss: 0.6295\n",
      "Epoch 011 | Loss: 0.6365\n",
      "\tVal loss: 0.6262 \n",
      "Epoch 012 | Loss: 0.6342\n",
      "Epoch 013 | Loss: 0.6089\n",
      "Epoch 014 | Loss: 0.6008\n",
      "Epoch 015 | Loss: 0.6169\n",
      "Epoch 016 | Loss: 0.6034\n",
      "\tVal loss: 0.5728 \n",
      "Epoch 017 | Loss: 0.5911\n",
      "Epoch 018 | Loss: 0.5845\n",
      "Epoch 019 | Loss: 0.5910\n",
      "Epoch 020 | Loss: 0.5797\n",
      "Epoch 021 | Loss: 0.5702\n",
      "\tVal loss: 0.5586 \n",
      "Epoch 022 | Loss: 0.5746\n",
      "Epoch 023 | Loss: 0.5759\n",
      "Epoch 024 | Loss: 0.5822\n",
      "Epoch 025 | Loss: 0.5663\n",
      "Epoch 026 | Loss: 0.5722\n",
      "\tVal loss: 0.5673 \n",
      "Epoch 027 | Loss: 0.5856\n",
      "Epoch 028 | Loss: 0.5588\n",
      "Epoch 029 | Loss: 0.5605\n",
      "Epoch 030 | Loss: 0.5775\n",
      "Epoch 031 | Loss: 0.5729\n",
      "\tVal loss: 0.5425 \n",
      "Epoch 032 | Loss: 0.5576\n",
      "Epoch 033 | Loss: 0.5556\n",
      "Epoch 034 | Loss: 0.5493\n",
      "Epoch 035 | Loss: 0.5513\n",
      "Epoch 036 | Loss: 0.5396\n",
      "\tVal loss: 0.5484 \n",
      "Epoch 037 | Loss: 0.5471\n",
      "Epoch 038 | Loss: 0.5407\n",
      "Epoch 039 | Loss: 0.5390\n",
      "Epoch 040 | Loss: 0.5251\n",
      "Epoch 041 | Loss: 0.5410\n",
      "\tVal loss: 0.5481 \n",
      "Epoch 042 | Loss: 0.5380\n",
      "Epoch 043 | Loss: 0.5321\n",
      "Epoch 044 | Loss: 0.5156\n",
      "Epoch 045 | Loss: 0.5052\n",
      "Epoch 046 | Loss: 0.5075\n",
      "\tVal loss: 0.5223 \n",
      "Epoch 047 | Loss: 0.5001\n",
      "Epoch 048 | Loss: 0.4957\n",
      "Epoch 049 | Loss: 0.4896\n",
      "Epoch 050 | Loss: 0.4814\n",
      "Epoch 051 | Loss: 0.5226\n",
      "\tVal loss: 0.5318 \n",
      "Epoch 052 | Loss: 0.5302\n",
      "Epoch 053 | Loss: 0.5042\n",
      "Epoch 054 | Loss: 0.4843\n",
      "Epoch 055 | Loss: 0.4914\n",
      "Epoch 056 | Loss: 0.4713\n",
      "\tVal loss: 0.5453 \n",
      "Epoch 057 | Loss: 0.5073\n",
      "Epoch 058 | Loss: 0.4781\n",
      "Epoch 059 | Loss: 0.4607\n",
      "Epoch 060 | Loss: 0.4627\n",
      "Epoch 061 | Loss: 0.4447\n",
      "\tVal loss: 0.5362 \n",
      "Epoch 062 | Loss: 0.4307\n",
      "Epoch 063 | Loss: 0.4182\n",
      "Epoch 064 | Loss: 0.4110\n",
      "Epoch 065 | Loss: 0.4009\n",
      "Epoch 066 | Loss: 0.3933\n",
      "\tVal loss: 0.5544 \n",
      "Epoch 067 | Loss: 0.3851\n",
      "Epoch 068 | Loss: 0.3808\n",
      "Epoch 069 | Loss: 0.3778\n",
      "Epoch 070 | Loss: 0.3745\n",
      "Epoch 071 | Loss: 0.3723\n",
      "\tVal loss: 0.5713 \n",
      "Epoch 072 | Loss: 0.3693\n",
      "Epoch 073 | Loss: 0.3635\n",
      "Epoch 074 | Loss: 0.3601\n",
      "Epoch 075 | Loss: 0.3611\n",
      "Epoch 076 | Loss: 0.3685\n",
      "\tVal loss: 0.5661 \n",
      "Epoch 077 | Loss: 0.3670\n",
      "Epoch 078 | Loss: 0.3570\n",
      "Epoch 079 | Loss: 0.3395\n",
      "Epoch 080 | Loss: 0.3292\n",
      "Epoch 081 | Loss: 0.3245\n",
      "\tVal loss: 0.5753 \n",
      "Epoch 082 | Loss: 0.3297\n",
      "Epoch 083 | Loss: 0.3315\n",
      "Epoch 084 | Loss: 0.3212\n",
      "Epoch 085 | Loss: 0.3174\n",
      "Epoch 086 | Loss: 0.3157\n",
      "\tVal loss: 0.5761 \n",
      "Epoch 087 | Loss: 0.3217\n",
      "Epoch 088 | Loss: 0.3267\n",
      "Epoch 089 | Loss: 0.3298\n",
      "Epoch 090 | Loss: 0.3207\n",
      "Epoch 091 | Loss: 0.3043\n",
      "\tVal loss: 0.5781 \n",
      "Epoch 092 | Loss: 0.2998\n",
      "Epoch 093 | Loss: 0.3002\n",
      "Epoch 094 | Loss: 0.3016\n",
      "Epoch 095 | Loss: 0.2954\n",
      "Epoch 096 | Loss: 0.2900\n",
      "\tVal loss: 0.5705 \n",
      "Epoch 097 | Loss: 0.3606\n",
      "Epoch 098 | Loss: 0.3893\n",
      "Epoch 099 | Loss: 0.3448\n",
      "Epoch 100 | Loss: 0.3374\n"
     ]
    }
   ],
   "source": [
    "resnet34 = Network(34, 100, 1).to(device)\n",
    "optimiser = optim.Adam(resnet34.parameters())\n",
    "_ = train(resnet34, surge_prediction_metric, optimiser, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-22T03:52:23.951291Z",
     "start_time": "2022-03-22T03:48:47.527408Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | Loss: 0.9736\n",
      "\tVal loss: 0.7503 \n",
      "Epoch 002 | Loss: 0.7425\n",
      "Epoch 003 | Loss: 0.6894\n",
      "Epoch 004 | Loss: 0.6611\n",
      "Epoch 005 | Loss: 0.6381\n",
      "Epoch 006 | Loss: 0.6233\n",
      "\tVal loss: 0.5656 \n",
      "Epoch 007 | Loss: 0.6172\n",
      "Epoch 008 | Loss: 0.5983\n",
      "Epoch 009 | Loss: 0.5917\n",
      "Epoch 010 | Loss: 0.5795\n",
      "Epoch 011 | Loss: 0.5689\n",
      "\tVal loss: 0.5315 \n",
      "Epoch 012 | Loss: 0.5609\n",
      "Epoch 013 | Loss: 0.5613\n",
      "Epoch 014 | Loss: 0.5471\n",
      "Epoch 015 | Loss: 0.5461\n",
      "Epoch 016 | Loss: 0.5674\n",
      "\tVal loss: 0.5252 \n",
      "Epoch 017 | Loss: 0.5407\n",
      "Epoch 018 | Loss: 0.5311\n",
      "Epoch 019 | Loss: 0.5210\n",
      "Epoch 020 | Loss: 0.5128\n",
      "Epoch 021 | Loss: 0.5005\n",
      "\tVal loss: 0.5267 \n",
      "Epoch 022 | Loss: 0.4945\n",
      "Epoch 023 | Loss: 0.4838\n",
      "Epoch 024 | Loss: 0.4846\n",
      "Epoch 025 | Loss: 0.4796\n",
      "Epoch 026 | Loss: 0.4699\n",
      "\tVal loss: 0.5222 \n",
      "Epoch 027 | Loss: 0.4653\n",
      "Epoch 028 | Loss: 0.4508\n",
      "Epoch 029 | Loss: 0.4373\n",
      "Epoch 030 | Loss: 0.4390\n",
      "Epoch 031 | Loss: 0.4390\n",
      "\tVal loss: 0.5146 saved!\n",
      "Epoch 032 | Loss: 0.4343\n",
      "Epoch 033 | Loss: 0.4401\n",
      "Epoch 034 | Loss: 0.4990\n",
      "Epoch 035 | Loss: 0.5042\n",
      "Epoch 036 | Loss: 0.4562\n",
      "\tVal loss: 0.5232 \n",
      "Epoch 037 | Loss: 0.4699\n",
      "Epoch 038 | Loss: 0.4339\n",
      "Epoch 039 | Loss: 0.4411\n",
      "Epoch 040 | Loss: 0.5050\n",
      "Epoch 041 | Loss: 0.4561\n",
      "\tVal loss: 0.5236 \n",
      "Epoch 042 | Loss: 0.4226\n",
      "Epoch 043 | Loss: 0.4139\n",
      "Epoch 044 | Loss: 0.4138\n",
      "Epoch 045 | Loss: 0.4005\n",
      "Epoch 046 | Loss: 0.3832\n",
      "\tVal loss: 0.5198 saved!\n",
      "Epoch 047 | Loss: 0.3856\n",
      "Epoch 048 | Loss: 0.4156\n",
      "Epoch 049 | Loss: 0.5061\n",
      "Epoch 050 | Loss: 0.4344\n",
      "Epoch 051 | Loss: 0.3926\n",
      "\tVal loss: 0.5294 \n",
      "Epoch 052 | Loss: 0.3760\n",
      "Epoch 053 | Loss: 0.4004\n",
      "Epoch 054 | Loss: 0.3624\n",
      "Epoch 055 | Loss: 0.3468\n",
      "Epoch 056 | Loss: 0.3442\n",
      "\tVal loss: 0.5713 \n",
      "Epoch 057 | Loss: 0.3484\n",
      "Epoch 058 | Loss: 0.3471\n",
      "Epoch 059 | Loss: 0.3529\n",
      "Epoch 060 | Loss: 0.3548\n",
      "Epoch 061 | Loss: 0.3579\n",
      "\tVal loss: 0.5626 \n",
      "Epoch 062 | Loss: 0.3439\n",
      "Epoch 063 | Loss: 0.3676\n",
      "Epoch 064 | Loss: 0.3432\n",
      "Epoch 065 | Loss: 0.3188\n",
      "Epoch 066 | Loss: 0.3123\n",
      "\tVal loss: 0.5883 \n",
      "Epoch 067 | Loss: 0.3768\n",
      "Epoch 068 | Loss: 0.3983\n",
      "Epoch 069 | Loss: 0.5352\n",
      "Epoch 070 | Loss: 0.4472\n",
      "Epoch 071 | Loss: 0.3756\n",
      "\tVal loss: 0.5376 \n",
      "Epoch 072 | Loss: 0.3368\n",
      "Epoch 073 | Loss: 0.3213\n",
      "Epoch 074 | Loss: 0.3180\n",
      "Epoch 075 | Loss: 0.3052\n",
      "Epoch 076 | Loss: 0.3018\n",
      "\tVal loss: 0.5750 \n",
      "Epoch 077 | Loss: 0.3031\n",
      "Epoch 078 | Loss: 0.3221\n",
      "Epoch 079 | Loss: 0.3169\n",
      "Epoch 080 | Loss: 0.3147\n",
      "Epoch 081 | Loss: 0.3708\n",
      "\tVal loss: 0.5624 \n",
      "Epoch 082 | Loss: 0.3436\n",
      "Epoch 083 | Loss: 0.3178\n",
      "Epoch 084 | Loss: 0.3130\n",
      "Epoch 085 | Loss: 0.3270\n",
      "Epoch 086 | Loss: 0.3305\n",
      "\tVal loss: 0.5747 \n",
      "Epoch 087 | Loss: 0.2954\n",
      "Epoch 088 | Loss: 0.3053\n",
      "Epoch 089 | Loss: 0.2881\n",
      "Epoch 090 | Loss: 0.2718\n",
      "Epoch 091 | Loss: 0.2708\n",
      "\tVal loss: 0.5847 \n",
      "Epoch 092 | Loss: 0.2794\n",
      "Epoch 093 | Loss: 0.3146\n",
      "Epoch 094 | Loss: 0.2885\n",
      "Epoch 095 | Loss: 0.2902\n",
      "Epoch 096 | Loss: 0.2817\n",
      "\tVal loss: 0.5818 \n",
      "Epoch 097 | Loss: 0.2785\n",
      "Epoch 098 | Loss: 0.2898\n",
      "Epoch 099 | Loss: 0.2988\n",
      "Epoch 100 | Loss: 0.2862\n"
     ]
    }
   ],
   "source": [
    "resnet34 = Network(34, 100, 2).to(device)\n",
    "optimiser = optim.Adam(resnet34.parameters())\n",
    "_ = train(resnet34, surge_prediction_metric, optimiser, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-22T03:56:08.085820Z",
     "start_time": "2022-03-22T03:52:23.952481Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | Loss: 1.0223\n",
      "\tVal loss: 0.8374 \n",
      "Epoch 002 | Loss: 0.7569\n",
      "Epoch 003 | Loss: 0.6864\n",
      "Epoch 004 | Loss: 0.6630\n",
      "Epoch 005 | Loss: 0.6383\n",
      "Epoch 006 | Loss: 0.6348\n",
      "\tVal loss: 0.6230 \n",
      "Epoch 007 | Loss: 0.6353\n",
      "Epoch 008 | Loss: 0.6172\n",
      "Epoch 009 | Loss: 0.6092\n",
      "Epoch 010 | Loss: 0.6021\n",
      "Epoch 011 | Loss: 0.5911\n",
      "\tVal loss: 0.5639 \n",
      "Epoch 012 | Loss: 0.5861\n",
      "Epoch 013 | Loss: 0.5794\n",
      "Epoch 014 | Loss: 0.5680\n",
      "Epoch 015 | Loss: 0.5596\n",
      "Epoch 016 | Loss: 0.5542\n",
      "\tVal loss: 0.5353 \n",
      "Epoch 017 | Loss: 0.5504\n",
      "Epoch 018 | Loss: 0.5418\n",
      "Epoch 019 | Loss: 0.5330\n",
      "Epoch 020 | Loss: 0.5321\n",
      "Epoch 021 | Loss: 0.5249\n",
      "\tVal loss: 0.5283 \n",
      "Epoch 022 | Loss: 0.5157\n",
      "Epoch 023 | Loss: 0.5065\n",
      "Epoch 024 | Loss: 0.4972\n",
      "Epoch 025 | Loss: 0.5113\n",
      "Epoch 026 | Loss: 0.5004\n",
      "\tVal loss: 0.5200 \n",
      "Epoch 027 | Loss: 0.4843\n",
      "Epoch 028 | Loss: 0.4913\n",
      "Epoch 029 | Loss: 0.5162\n",
      "Epoch 030 | Loss: 0.4952\n",
      "Epoch 031 | Loss: 0.4721\n",
      "\tVal loss: 0.5544 \n",
      "Epoch 032 | Loss: 0.4640\n",
      "Epoch 033 | Loss: 0.4625\n",
      "Epoch 034 | Loss: 0.4629\n",
      "Epoch 035 | Loss: 0.4658\n",
      "Epoch 036 | Loss: 0.4544\n",
      "\tVal loss: 0.5361 \n",
      "Epoch 037 | Loss: 0.4308\n",
      "Epoch 038 | Loss: 0.4281\n",
      "Epoch 039 | Loss: 0.4305\n",
      "Epoch 040 | Loss: 0.4140\n",
      "Epoch 041 | Loss: 0.4100\n",
      "\tVal loss: 0.5347 \n",
      "Epoch 042 | Loss: 0.4042\n",
      "Epoch 043 | Loss: 0.4015\n",
      "Epoch 044 | Loss: 0.3912\n",
      "Epoch 045 | Loss: 0.4047\n",
      "Epoch 046 | Loss: 0.3955\n",
      "\tVal loss: 0.5509 \n",
      "Epoch 047 | Loss: 0.4081\n",
      "Epoch 048 | Loss: 0.3853\n",
      "Epoch 049 | Loss: 0.3985\n",
      "Epoch 050 | Loss: 0.3990\n",
      "Epoch 051 | Loss: 0.4738\n",
      "\tVal loss: 0.5438 \n",
      "Epoch 052 | Loss: 0.4474\n",
      "Epoch 053 | Loss: 0.4337\n",
      "Epoch 054 | Loss: 0.4006\n",
      "Epoch 055 | Loss: 0.3777\n",
      "Epoch 056 | Loss: 0.3655\n",
      "\tVal loss: 0.5623 \n",
      "Epoch 057 | Loss: 0.3873\n",
      "Epoch 058 | Loss: 0.3822\n",
      "Epoch 059 | Loss: 0.4854\n",
      "Epoch 060 | Loss: 0.4948\n",
      "Epoch 061 | Loss: 0.4304\n",
      "\tVal loss: 0.5254 \n",
      "Epoch 062 | Loss: 0.4090\n",
      "Epoch 063 | Loss: 0.3648\n",
      "Epoch 064 | Loss: 0.4378\n",
      "Epoch 065 | Loss: 0.5360\n",
      "Epoch 066 | Loss: 0.4916\n",
      "\tVal loss: 0.5273 \n",
      "Epoch 067 | Loss: 0.4567\n",
      "Epoch 068 | Loss: 0.4221\n",
      "Epoch 069 | Loss: 0.3966\n",
      "Epoch 070 | Loss: 0.3777\n",
      "Epoch 071 | Loss: 0.3671\n",
      "\tVal loss: 0.5730 \n",
      "Epoch 072 | Loss: 0.3574\n",
      "Epoch 073 | Loss: 0.3477\n",
      "Epoch 074 | Loss: 0.3766\n",
      "Epoch 075 | Loss: 0.3592\n",
      "Epoch 076 | Loss: 0.3616\n",
      "\tVal loss: 0.5801 \n",
      "Epoch 077 | Loss: 0.3680\n",
      "Epoch 078 | Loss: 0.3405\n",
      "Epoch 079 | Loss: 0.3302\n",
      "Epoch 080 | Loss: 0.3262\n",
      "Epoch 081 | Loss: 0.3184\n",
      "\tVal loss: 0.6146 \n",
      "Epoch 082 | Loss: 0.3169\n",
      "Epoch 083 | Loss: 0.3110\n",
      "Epoch 084 | Loss: 0.3308\n",
      "Epoch 085 | Loss: 0.3131\n",
      "Epoch 086 | Loss: 0.3132\n",
      "\tVal loss: 0.6092 \n",
      "Epoch 087 | Loss: 0.3651\n",
      "Epoch 088 | Loss: 0.3360\n",
      "Epoch 089 | Loss: 0.3162\n",
      "Epoch 090 | Loss: 0.3097\n",
      "Epoch 091 | Loss: 0.3020\n",
      "\tVal loss: 0.6044 \n",
      "Epoch 092 | Loss: 0.3003\n",
      "Epoch 093 | Loss: 0.3136\n",
      "Epoch 094 | Loss: 0.4528\n",
      "Epoch 095 | Loss: 0.3657\n",
      "Epoch 096 | Loss: 0.3532\n",
      "\tVal loss: 0.5701 \n",
      "Epoch 097 | Loss: 0.3196\n",
      "Epoch 098 | Loss: 0.2986\n",
      "Epoch 099 | Loss: 0.2827\n",
      "Epoch 100 | Loss: 0.2738\n"
     ]
    }
   ],
   "source": [
    "resnet34 = Network(34, 100, 3).to(device)\n",
    "optimiser = optim.Adam(resnet34.parameters())\n",
    "_ = train(resnet34, surge_prediction_metric, optimiser, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-22T03:59:45.747658Z",
     "start_time": "2022-03-22T03:56:08.087185Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | Loss: 1.0296\n",
      "\tVal loss: 0.9139 \n",
      "Epoch 002 | Loss: 0.9456\n",
      "Epoch 003 | Loss: 0.8634\n",
      "Epoch 004 | Loss: 0.7983\n",
      "Epoch 005 | Loss: 0.7284\n",
      "Epoch 006 | Loss: 0.7102\n",
      "\tVal loss: 0.8165 \n",
      "Epoch 007 | Loss: 0.7362\n",
      "Epoch 008 | Loss: 0.6817\n",
      "Epoch 009 | Loss: 0.6596\n",
      "Epoch 010 | Loss: 0.6511\n",
      "Epoch 011 | Loss: 0.6476\n",
      "\tVal loss: 0.5899 \n",
      "Epoch 012 | Loss: 0.6237\n",
      "Epoch 013 | Loss: 0.6120\n",
      "Epoch 014 | Loss: 0.6107\n",
      "Epoch 015 | Loss: 0.6139\n",
      "Epoch 016 | Loss: 0.5983\n",
      "\tVal loss: 0.5733 \n",
      "Epoch 017 | Loss: 0.5945\n",
      "Epoch 018 | Loss: 0.5961\n",
      "Epoch 019 | Loss: 0.5841\n",
      "Epoch 020 | Loss: 0.5795\n",
      "Epoch 021 | Loss: 0.5814\n",
      "\tVal loss: 0.5545 \n",
      "Epoch 022 | Loss: 0.5734\n",
      "Epoch 023 | Loss: 0.5656\n",
      "Epoch 024 | Loss: 0.5635\n",
      "Epoch 025 | Loss: 0.6119\n",
      "Epoch 026 | Loss: 0.5896\n",
      "\tVal loss: 0.5537 \n",
      "Epoch 027 | Loss: 0.5729\n",
      "Epoch 028 | Loss: 0.5640\n",
      "Epoch 029 | Loss: 0.5544\n",
      "Epoch 030 | Loss: 0.5503\n",
      "Epoch 031 | Loss: 0.5496\n",
      "\tVal loss: 0.5503 \n",
      "Epoch 032 | Loss: 0.5456\n",
      "Epoch 033 | Loss: 0.5358\n",
      "Epoch 034 | Loss: 0.5536\n",
      "Epoch 035 | Loss: 0.5502\n",
      "Epoch 036 | Loss: 0.5611\n",
      "\tVal loss: 0.7068 \n",
      "Epoch 037 | Loss: 0.5790\n",
      "Epoch 038 | Loss: 0.5559\n",
      "Epoch 039 | Loss: 0.5427\n",
      "Epoch 040 | Loss: 0.5319\n",
      "Epoch 041 | Loss: 0.5233\n",
      "\tVal loss: 0.5308 \n",
      "Epoch 042 | Loss: 0.5124\n",
      "Epoch 043 | Loss: 0.5044\n",
      "Epoch 044 | Loss: 0.4925\n",
      "Epoch 045 | Loss: 0.4866\n",
      "Epoch 046 | Loss: 0.4847\n",
      "\tVal loss: 0.5429 \n",
      "Epoch 047 | Loss: 0.4790\n",
      "Epoch 048 | Loss: 0.4753\n",
      "Epoch 049 | Loss: 0.4657\n",
      "Epoch 050 | Loss: 0.4616\n",
      "Epoch 051 | Loss: 0.4774\n",
      "\tVal loss: 0.5401 \n",
      "Epoch 052 | Loss: 0.4949\n",
      "Epoch 053 | Loss: 0.4679\n",
      "Epoch 054 | Loss: 0.4522\n",
      "Epoch 055 | Loss: 0.4638\n",
      "Epoch 056 | Loss: 0.5161\n",
      "\tVal loss: 0.5292 \n",
      "Epoch 057 | Loss: 0.4759\n",
      "Epoch 058 | Loss: 0.4545\n",
      "Epoch 059 | Loss: 0.4323\n",
      "Epoch 060 | Loss: 0.4289\n",
      "Epoch 061 | Loss: 0.4221\n",
      "\tVal loss: 0.5629 \n",
      "Epoch 062 | Loss: 0.4116\n",
      "Epoch 063 | Loss: 0.4019\n",
      "Epoch 064 | Loss: 0.4059\n",
      "Epoch 065 | Loss: 0.4111\n",
      "Epoch 066 | Loss: 0.4055\n",
      "\tVal loss: 0.5641 \n",
      "Epoch 067 | Loss: 0.4751\n",
      "Epoch 068 | Loss: 0.4297\n",
      "Epoch 069 | Loss: 0.4012\n",
      "Epoch 070 | Loss: 0.3857\n",
      "Epoch 071 | Loss: 0.3859\n",
      "\tVal loss: 0.5369 \n",
      "Epoch 072 | Loss: 0.3804\n",
      "Epoch 073 | Loss: 0.3947\n",
      "Epoch 074 | Loss: 0.3868\n",
      "Epoch 075 | Loss: 0.3764\n",
      "Epoch 076 | Loss: 0.3772\n",
      "\tVal loss: 0.5684 \n",
      "Epoch 077 | Loss: 0.3661\n",
      "Epoch 078 | Loss: 0.3663\n",
      "Epoch 079 | Loss: 0.3695\n",
      "Epoch 080 | Loss: 0.3762\n",
      "Epoch 081 | Loss: 0.3793\n",
      "\tVal loss: 0.5499 \n",
      "Epoch 082 | Loss: 0.3813\n",
      "Epoch 083 | Loss: 0.3962\n",
      "Epoch 084 | Loss: 0.4428\n",
      "Epoch 085 | Loss: 0.3966\n",
      "Epoch 086 | Loss: 0.4718\n",
      "\tVal loss: 0.5284 \n",
      "Epoch 087 | Loss: 0.3957\n",
      "Epoch 088 | Loss: 0.3996\n",
      "Epoch 089 | Loss: 0.5619\n",
      "Epoch 090 | Loss: 0.5429\n",
      "Epoch 091 | Loss: 0.4738\n",
      "\tVal loss: 0.5452 \n",
      "Epoch 092 | Loss: 0.4172\n",
      "Epoch 093 | Loss: 0.3876\n",
      "Epoch 094 | Loss: 0.4336\n",
      "Epoch 095 | Loss: 0.3893\n",
      "Epoch 096 | Loss: 0.3802\n",
      "\tVal loss: 0.5492 \n",
      "Epoch 097 | Loss: 0.3630\n",
      "Epoch 098 | Loss: 0.3476\n",
      "Epoch 099 | Loss: 0.3333\n",
      "Epoch 100 | Loss: 0.3328\n"
     ]
    }
   ],
   "source": [
    "resnet34 = Network(34, 50, 1).to(device)\n",
    "optimiser = optim.Adam(resnet34.parameters())\n",
    "_ = train(resnet34, surge_prediction_metric, optimiser, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-22T04:03:24.353585Z",
     "start_time": "2022-03-22T03:59:45.748654Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | Loss: 1.0229\n",
      "\tVal loss: 0.8867 \n",
      "Epoch 002 | Loss: 0.9117\n",
      "Epoch 003 | Loss: 0.8263\n",
      "Epoch 004 | Loss: 0.7479\n",
      "Epoch 005 | Loss: 0.6765\n",
      "Epoch 006 | Loss: 0.6641\n",
      "\tVal loss: 0.5946 \n",
      "Epoch 007 | Loss: 0.6396\n",
      "Epoch 008 | Loss: 0.6184\n",
      "Epoch 009 | Loss: 0.6027\n",
      "Epoch 010 | Loss: 0.5961\n",
      "Epoch 011 | Loss: 0.5866\n",
      "\tVal loss: 0.5634 \n",
      "Epoch 012 | Loss: 0.5791\n",
      "Epoch 013 | Loss: 0.5643\n",
      "Epoch 014 | Loss: 0.5540\n",
      "Epoch 015 | Loss: 0.5434\n",
      "Epoch 016 | Loss: 0.5384\n",
      "\tVal loss: 0.5388 \n",
      "Epoch 017 | Loss: 0.5298\n",
      "Epoch 018 | Loss: 0.5143\n",
      "Epoch 019 | Loss: 0.5047\n",
      "Epoch 020 | Loss: 0.4959\n",
      "Epoch 021 | Loss: 0.4907\n",
      "\tVal loss: 0.5695 \n",
      "Epoch 022 | Loss: 0.4836\n",
      "Epoch 023 | Loss: 0.4832\n",
      "Epoch 024 | Loss: 0.4649\n",
      "Epoch 025 | Loss: 0.4545\n",
      "Epoch 026 | Loss: 0.4457\n",
      "\tVal loss: 0.5440 \n",
      "Epoch 027 | Loss: 0.4385\n",
      "Epoch 028 | Loss: 0.4377\n",
      "Epoch 029 | Loss: 0.4466\n",
      "Epoch 030 | Loss: 0.4211\n",
      "Epoch 031 | Loss: 0.4123\n",
      "\tVal loss: 0.5603 \n",
      "Epoch 032 | Loss: 0.4073\n",
      "Epoch 033 | Loss: 0.4031\n",
      "Epoch 034 | Loss: 0.3929\n",
      "Epoch 035 | Loss: 0.3879\n",
      "Epoch 036 | Loss: 0.3867\n",
      "\tVal loss: 0.5298 \n",
      "Epoch 037 | Loss: 0.3839\n",
      "Epoch 038 | Loss: 0.3760\n",
      "Epoch 039 | Loss: 0.3716\n",
      "Epoch 040 | Loss: 0.3716\n",
      "Epoch 041 | Loss: 0.3622\n",
      "\tVal loss: 0.5431 \n",
      "Epoch 042 | Loss: 0.3558\n",
      "Epoch 043 | Loss: 0.3563\n",
      "Epoch 044 | Loss: 0.3449\n",
      "Epoch 045 | Loss: 0.3380\n",
      "Epoch 046 | Loss: 0.3349\n",
      "\tVal loss: 0.5543 \n",
      "Epoch 047 | Loss: 0.3351\n",
      "Epoch 048 | Loss: 0.3355\n",
      "Epoch 049 | Loss: 0.3424\n",
      "Epoch 050 | Loss: 0.3376\n",
      "Epoch 051 | Loss: 0.3329\n",
      "\tVal loss: 0.5500 \n",
      "Epoch 052 | Loss: 0.3648\n",
      "Epoch 053 | Loss: 0.3264\n",
      "Epoch 054 | Loss: 0.3128\n",
      "Epoch 055 | Loss: 0.3074\n",
      "Epoch 056 | Loss: 0.3146\n",
      "\tVal loss: 0.5570 \n",
      "Epoch 057 | Loss: 0.3125\n",
      "Epoch 058 | Loss: 0.3071\n",
      "Epoch 059 | Loss: 0.3096\n",
      "Epoch 060 | Loss: 0.3095\n",
      "Epoch 061 | Loss: 0.3036\n",
      "\tVal loss: 0.5312 \n",
      "Epoch 062 | Loss: 0.2990\n",
      "Epoch 063 | Loss: 0.2950\n",
      "Epoch 064 | Loss: 0.3022\n",
      "Epoch 065 | Loss: 0.2981\n",
      "Epoch 066 | Loss: 0.3036\n",
      "\tVal loss: 0.5557 \n",
      "Epoch 067 | Loss: 0.3003\n",
      "Epoch 068 | Loss: 0.2939\n",
      "Epoch 069 | Loss: 0.2918\n",
      "Epoch 070 | Loss: 0.2822\n",
      "Epoch 071 | Loss: 0.2839\n",
      "\tVal loss: 0.5582 \n",
      "Epoch 072 | Loss: 0.2896\n",
      "Epoch 073 | Loss: 0.2899\n",
      "Epoch 074 | Loss: 0.2805\n",
      "Epoch 075 | Loss: 0.2744\n",
      "Epoch 076 | Loss: 0.2754\n",
      "\tVal loss: 0.5519 \n",
      "Epoch 077 | Loss: 0.2735\n",
      "Epoch 078 | Loss: 0.2759\n",
      "Epoch 079 | Loss: 0.2948\n",
      "Epoch 080 | Loss: 0.2824\n",
      "Epoch 081 | Loss: 0.3158\n",
      "\tVal loss: 0.5561 \n",
      "Epoch 082 | Loss: 0.2848\n",
      "Epoch 083 | Loss: 0.2861\n",
      "Epoch 084 | Loss: 0.3048\n",
      "Epoch 085 | Loss: 0.2976\n",
      "Epoch 086 | Loss: 0.2802\n",
      "\tVal loss: 0.5744 \n",
      "Epoch 087 | Loss: 0.2858\n",
      "Epoch 088 | Loss: 0.3124\n",
      "Epoch 089 | Loss: 0.3113\n",
      "Epoch 090 | Loss: 0.2753\n",
      "Epoch 091 | Loss: 0.2790\n",
      "\tVal loss: 0.5691 \n",
      "Epoch 092 | Loss: 0.2817\n",
      "Epoch 093 | Loss: 0.3454\n",
      "Epoch 094 | Loss: 0.2738\n",
      "Epoch 095 | Loss: 0.2520\n",
      "Epoch 096 | Loss: 0.2429\n",
      "\tVal loss: 0.5676 \n",
      "Epoch 097 | Loss: 0.2476\n",
      "Epoch 098 | Loss: 0.3057\n",
      "Epoch 099 | Loss: 0.2793\n",
      "Epoch 100 | Loss: 0.3384\n"
     ]
    }
   ],
   "source": [
    "resnet34 = Network(34, 200, 1).to(device)\n",
    "optimiser = optim.Adam(resnet34.parameters())\n",
    "_ = train(resnet34, surge_prediction_metric, optimiser, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-22T04:08:52.159523Z",
     "start_time": "2022-03-22T04:03:24.355202Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | Loss: 1.0308\n",
      "\tVal loss: 0.8777 \n",
      "Epoch 002 | Loss: 0.8113\n",
      "Epoch 003 | Loss: 0.7012\n",
      "Epoch 004 | Loss: 0.6576\n",
      "Epoch 005 | Loss: 0.6321\n",
      "Epoch 006 | Loss: 0.6288\n",
      "\tVal loss: 0.5834 \n",
      "Epoch 007 | Loss: 0.6115\n",
      "Epoch 008 | Loss: 0.6023\n",
      "Epoch 009 | Loss: 0.6059\n",
      "Epoch 010 | Loss: 0.6204\n",
      "Epoch 011 | Loss: 0.6071\n",
      "\tVal loss: 0.5749 \n",
      "Epoch 012 | Loss: 0.5898\n",
      "Epoch 013 | Loss: 0.5818\n",
      "Epoch 014 | Loss: 0.5839\n",
      "Epoch 015 | Loss: 0.5724\n",
      "Epoch 016 | Loss: 0.5579\n",
      "\tVal loss: 0.5337 \n",
      "Epoch 017 | Loss: 0.5519\n",
      "Epoch 018 | Loss: 0.5438\n",
      "Epoch 019 | Loss: 0.5401\n",
      "Epoch 020 | Loss: 0.5246\n",
      "Epoch 021 | Loss: 0.5132\n",
      "\tVal loss: 0.6028 \n",
      "Epoch 022 | Loss: 0.5152\n",
      "Epoch 023 | Loss: 0.5400\n",
      "Epoch 024 | Loss: 0.5886\n",
      "Epoch 025 | Loss: 0.5581\n",
      "Epoch 026 | Loss: 0.5496\n",
      "\tVal loss: 0.5551 \n",
      "Epoch 027 | Loss: 0.5437\n",
      "Epoch 028 | Loss: 0.5293\n",
      "Epoch 029 | Loss: 0.5289\n",
      "Epoch 030 | Loss: 0.6016\n",
      "Epoch 031 | Loss: 0.5634\n",
      "\tVal loss: 0.5577 \n",
      "Epoch 032 | Loss: 0.5501\n",
      "Epoch 033 | Loss: 0.5268\n",
      "Epoch 034 | Loss: 0.5263\n",
      "Epoch 035 | Loss: 0.5383\n",
      "Epoch 036 | Loss: 0.5440\n",
      "\tVal loss: 0.5237 \n",
      "Epoch 037 | Loss: 0.5167\n",
      "Epoch 038 | Loss: 0.5162\n",
      "Epoch 039 | Loss: 0.5284\n",
      "Epoch 040 | Loss: 0.5009\n",
      "Epoch 041 | Loss: 0.4857\n",
      "\tVal loss: 0.5490 \n",
      "Epoch 042 | Loss: 0.4918\n",
      "Epoch 043 | Loss: 0.4848\n",
      "Epoch 044 | Loss: 0.4659\n",
      "Epoch 045 | Loss: 0.5015\n",
      "Epoch 046 | Loss: 0.5563\n",
      "\tVal loss: 0.5427 \n",
      "Epoch 047 | Loss: 0.5431\n",
      "Epoch 048 | Loss: 0.5129\n",
      "Epoch 049 | Loss: 0.5012\n",
      "Epoch 050 | Loss: 0.4869\n",
      "Epoch 051 | Loss: 0.4658\n",
      "\tVal loss: 0.5251 \n",
      "Epoch 052 | Loss: 0.4576\n",
      "Epoch 053 | Loss: 0.4526\n",
      "Epoch 054 | Loss: 0.4597\n",
      "Epoch 055 | Loss: 0.4394\n",
      "Epoch 056 | Loss: 0.4313\n",
      "\tVal loss: 0.5425 \n",
      "Epoch 057 | Loss: 0.4277\n",
      "Epoch 058 | Loss: 0.4230\n",
      "Epoch 059 | Loss: 0.4140\n",
      "Epoch 060 | Loss: 0.4121\n",
      "Epoch 061 | Loss: 0.4039\n",
      "\tVal loss: 0.5444 \n",
      "Epoch 062 | Loss: 0.4131\n",
      "Epoch 063 | Loss: 0.5716\n",
      "Epoch 064 | Loss: 0.5187\n",
      "Epoch 065 | Loss: 0.4793\n",
      "Epoch 066 | Loss: 0.5159\n",
      "\tVal loss: 0.5332 \n",
      "Epoch 067 | Loss: 0.5065\n",
      "Epoch 068 | Loss: 0.4658\n",
      "Epoch 069 | Loss: 0.4445\n",
      "Epoch 070 | Loss: 0.4270\n",
      "Epoch 071 | Loss: 0.4332\n",
      "\tVal loss: 0.5417 \n",
      "Epoch 072 | Loss: 0.4402\n",
      "Epoch 073 | Loss: 0.4500\n",
      "Epoch 074 | Loss: 0.4248\n",
      "Epoch 075 | Loss: 0.4191\n",
      "Epoch 076 | Loss: 0.3950\n",
      "\tVal loss: 0.5549 \n",
      "Epoch 077 | Loss: 0.3854\n",
      "Epoch 078 | Loss: 0.3838\n",
      "Epoch 079 | Loss: 0.4034\n",
      "Epoch 080 | Loss: 0.3823\n",
      "Epoch 081 | Loss: 0.3810\n",
      "\tVal loss: 0.6061 \n",
      "Epoch 082 | Loss: 0.3913\n",
      "Epoch 083 | Loss: 0.5169\n",
      "Epoch 084 | Loss: 0.4705\n",
      "Epoch 085 | Loss: 0.4414\n",
      "Epoch 086 | Loss: 0.4810\n",
      "\tVal loss: 0.5538 \n",
      "Epoch 087 | Loss: 0.4560\n",
      "Epoch 088 | Loss: 0.4130\n",
      "Epoch 089 | Loss: 0.3888\n",
      "Epoch 090 | Loss: 0.3714\n",
      "Epoch 091 | Loss: 0.3739\n",
      "\tVal loss: 0.5578 \n",
      "Epoch 092 | Loss: 0.3720\n",
      "Epoch 093 | Loss: 0.3782\n",
      "Epoch 094 | Loss: 0.3846\n",
      "Epoch 095 | Loss: 0.3747\n",
      "Epoch 096 | Loss: 0.3753\n",
      "\tVal loss: 0.5586 \n",
      "Epoch 097 | Loss: 0.3699\n",
      "Epoch 098 | Loss: 0.3767\n",
      "Epoch 099 | Loss: 0.3709\n",
      "Epoch 100 | Loss: 0.3394\n"
     ]
    }
   ],
   "source": [
    "resnet50 = Network(50, 100, 1).to(device)\n",
    "optimiser = optim.Adam(resnet50.parameters())\n",
    "_ = train(resnet50, surge_prediction_metric, optimiser, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-22T04:23:35.613832Z",
     "start_time": "2022-03-22T04:23:35.609197Z"
    }
   },
   "outputs": [],
   "source": [
    "def write_submission(model, fn='submission.csv'):\n",
    "    COLUMNS = [\n",
    "        'surge1_t0', 'surge1_t1', 'surge1_t2', 'surge1_t3', 'surge1_t4',\n",
    "        'surge1_t5', 'surge1_t6', 'surge1_t7', 'surge1_t8', 'surge1_t9',\n",
    "        'surge2_t0', 'surge2_t1', 'surge2_t2', 'surge2_t3', 'surge2_t4',\n",
    "        'surge2_t5', 'surge2_t6', 'surge2_t7', 'surge2_t8', 'surge2_t9' ]\n",
    "    surge1_output = []\n",
    "    surge2_output = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for slp, t_surge1_in, surge1_in, t_surge2_in, surge2_in, t_surge1_out, t_surge2_out in test_dataloader:\n",
    "            slp = slp.to(device)\n",
    "            t_surge1_in = t_surge1_in.to(device)\n",
    "            surge1_in = surge1_in.to(device)\n",
    "            t_surge2_in = t_surge2_in.to(device)\n",
    "            surge2_in = surge2_in.to(device)\n",
    "            t_surge1_out = t_surge1_out.to(device)\n",
    "            t_surge2_out = t_surge2_out.to(device)\n",
    "            surge1_out_pred, surge2_out_pred = model(slp, t_surge1_in, surge1_in, t_surge2_in, surge2_in, t_surge1_out, t_surge2_out)\n",
    "            surge1_output.append(surge1_out_pred)\n",
    "            surge2_output.append(surge2_out_pred)\n",
    "\n",
    "    surge1_output = torch.cat(surge1_output, dim=0)\n",
    "    surge2_output = torch.cat(surge2_output, dim=0)\n",
    "    test_outputs = torch.cat([surge1_output, surge2_output], dim=1).detach().cpu().numpy()\n",
    "\n",
    "    test_df = pd.DataFrame(test_outputs, index=test_id_seq, columns=COLUMNS)\n",
    "    test_df.index.name = 'id_sequence'\n",
    "    test_df.to_csv(fn)\n",
    "    return test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-22T04:25:34.861678Z",
     "start_time": "2022-03-22T04:25:34.577570Z"
    }
   },
   "outputs": [],
   "source": [
    "model_fn = 'resnet34_100_2_30.pth'\n",
    "model = Network(34, 100, 2)\n",
    "model.load_state_dict(torch.load(model_fn, map_location=device))\n",
    "model = model.to(device)\n",
    "df = write_submission(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-22T02:06:05.809337Z",
     "start_time": "2022-03-22T02:06:05.789324Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>surge1_t0</th>\n",
       "      <th>surge1_t1</th>\n",
       "      <th>surge1_t2</th>\n",
       "      <th>surge1_t3</th>\n",
       "      <th>surge1_t4</th>\n",
       "      <th>surge1_t5</th>\n",
       "      <th>surge1_t6</th>\n",
       "      <th>surge1_t7</th>\n",
       "      <th>surge1_t8</th>\n",
       "      <th>surge1_t9</th>\n",
       "      <th>surge2_t0</th>\n",
       "      <th>surge2_t1</th>\n",
       "      <th>surge2_t2</th>\n",
       "      <th>surge2_t3</th>\n",
       "      <th>surge2_t4</th>\n",
       "      <th>surge2_t5</th>\n",
       "      <th>surge2_t6</th>\n",
       "      <th>surge2_t7</th>\n",
       "      <th>surge2_t8</th>\n",
       "      <th>surge2_t9</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_sequence</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5600</th>\n",
       "      <td>-1.522672</td>\n",
       "      <td>-1.640056</td>\n",
       "      <td>-1.812372</td>\n",
       "      <td>-1.873276</td>\n",
       "      <td>-1.911106</td>\n",
       "      <td>-1.892778</td>\n",
       "      <td>-1.814209</td>\n",
       "      <td>-1.698588</td>\n",
       "      <td>-1.570127</td>\n",
       "      <td>-1.445986</td>\n",
       "      <td>-1.077388</td>\n",
       "      <td>-1.206931</td>\n",
       "      <td>-1.322765</td>\n",
       "      <td>-1.355515</td>\n",
       "      <td>-1.416860</td>\n",
       "      <td>-1.485878</td>\n",
       "      <td>-1.503089</td>\n",
       "      <td>-1.474643</td>\n",
       "      <td>-1.412876</td>\n",
       "      <td>-1.315216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5601</th>\n",
       "      <td>-0.172018</td>\n",
       "      <td>-0.244204</td>\n",
       "      <td>-0.333128</td>\n",
       "      <td>-0.247466</td>\n",
       "      <td>0.008339</td>\n",
       "      <td>0.311038</td>\n",
       "      <td>0.669055</td>\n",
       "      <td>1.009192</td>\n",
       "      <td>1.198229</td>\n",
       "      <td>1.189028</td>\n",
       "      <td>0.016760</td>\n",
       "      <td>-0.052519</td>\n",
       "      <td>-0.104194</td>\n",
       "      <td>-0.077168</td>\n",
       "      <td>0.124576</td>\n",
       "      <td>0.411364</td>\n",
       "      <td>0.781786</td>\n",
       "      <td>1.110761</td>\n",
       "      <td>1.277642</td>\n",
       "      <td>1.251851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5602</th>\n",
       "      <td>-0.728413</td>\n",
       "      <td>-1.103798</td>\n",
       "      <td>-1.081307</td>\n",
       "      <td>-0.869735</td>\n",
       "      <td>-0.684275</td>\n",
       "      <td>-0.531688</td>\n",
       "      <td>-0.410095</td>\n",
       "      <td>-0.311700</td>\n",
       "      <td>-0.223303</td>\n",
       "      <td>-0.145472</td>\n",
       "      <td>1.353438</td>\n",
       "      <td>1.628443</td>\n",
       "      <td>1.170808</td>\n",
       "      <td>0.442155</td>\n",
       "      <td>0.039545</td>\n",
       "      <td>-0.142167</td>\n",
       "      <td>-0.216019</td>\n",
       "      <td>-0.203616</td>\n",
       "      <td>-0.161684</td>\n",
       "      <td>-0.113963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5603</th>\n",
       "      <td>-0.214031</td>\n",
       "      <td>0.022230</td>\n",
       "      <td>0.161170</td>\n",
       "      <td>0.047790</td>\n",
       "      <td>-0.173768</td>\n",
       "      <td>-0.312626</td>\n",
       "      <td>-0.312222</td>\n",
       "      <td>-0.243562</td>\n",
       "      <td>-0.164647</td>\n",
       "      <td>-0.095614</td>\n",
       "      <td>1.032095</td>\n",
       "      <td>1.588513</td>\n",
       "      <td>1.518492</td>\n",
       "      <td>0.934075</td>\n",
       "      <td>0.517910</td>\n",
       "      <td>0.386632</td>\n",
       "      <td>0.364263</td>\n",
       "      <td>0.341955</td>\n",
       "      <td>0.313909</td>\n",
       "      <td>0.286394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5604</th>\n",
       "      <td>0.188806</td>\n",
       "      <td>0.256014</td>\n",
       "      <td>0.136325</td>\n",
       "      <td>-0.055788</td>\n",
       "      <td>-0.255821</td>\n",
       "      <td>-0.438868</td>\n",
       "      <td>-0.531611</td>\n",
       "      <td>-0.504565</td>\n",
       "      <td>-0.409072</td>\n",
       "      <td>-0.300028</td>\n",
       "      <td>-0.038583</td>\n",
       "      <td>0.077969</td>\n",
       "      <td>-0.076706</td>\n",
       "      <td>-0.310813</td>\n",
       "      <td>-0.561307</td>\n",
       "      <td>-0.757425</td>\n",
       "      <td>-0.837745</td>\n",
       "      <td>-0.798326</td>\n",
       "      <td>-0.687479</td>\n",
       "      <td>-0.554552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6104</th>\n",
       "      <td>-0.202868</td>\n",
       "      <td>-0.458387</td>\n",
       "      <td>-0.676621</td>\n",
       "      <td>-0.850127</td>\n",
       "      <td>-0.801504</td>\n",
       "      <td>-0.661781</td>\n",
       "      <td>-0.550004</td>\n",
       "      <td>-0.469796</td>\n",
       "      <td>-0.375337</td>\n",
       "      <td>-0.267382</td>\n",
       "      <td>0.679106</td>\n",
       "      <td>-0.081019</td>\n",
       "      <td>-0.426764</td>\n",
       "      <td>-0.630051</td>\n",
       "      <td>-0.645582</td>\n",
       "      <td>-0.500996</td>\n",
       "      <td>-0.302407</td>\n",
       "      <td>-0.138851</td>\n",
       "      <td>-0.036838</td>\n",
       "      <td>0.022590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6105</th>\n",
       "      <td>0.004178</td>\n",
       "      <td>-0.027268</td>\n",
       "      <td>-0.104875</td>\n",
       "      <td>-0.129565</td>\n",
       "      <td>-0.042405</td>\n",
       "      <td>0.124772</td>\n",
       "      <td>0.274499</td>\n",
       "      <td>0.309280</td>\n",
       "      <td>0.270955</td>\n",
       "      <td>0.226377</td>\n",
       "      <td>0.155758</td>\n",
       "      <td>-0.343349</td>\n",
       "      <td>-0.494503</td>\n",
       "      <td>-0.406895</td>\n",
       "      <td>-0.211891</td>\n",
       "      <td>0.009162</td>\n",
       "      <td>0.194007</td>\n",
       "      <td>0.265217</td>\n",
       "      <td>0.251194</td>\n",
       "      <td>0.218806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6106</th>\n",
       "      <td>-0.429599</td>\n",
       "      <td>-0.720932</td>\n",
       "      <td>-0.728523</td>\n",
       "      <td>-0.629140</td>\n",
       "      <td>-0.443044</td>\n",
       "      <td>-0.221718</td>\n",
       "      <td>0.056978</td>\n",
       "      <td>0.360464</td>\n",
       "      <td>0.566990</td>\n",
       "      <td>0.590129</td>\n",
       "      <td>0.005821</td>\n",
       "      <td>-0.308064</td>\n",
       "      <td>-0.262538</td>\n",
       "      <td>-0.246235</td>\n",
       "      <td>-0.120654</td>\n",
       "      <td>0.120976</td>\n",
       "      <td>0.452356</td>\n",
       "      <td>0.784378</td>\n",
       "      <td>0.954932</td>\n",
       "      <td>0.906470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6107</th>\n",
       "      <td>0.218414</td>\n",
       "      <td>-0.065912</td>\n",
       "      <td>-0.073831</td>\n",
       "      <td>-0.228745</td>\n",
       "      <td>-0.546522</td>\n",
       "      <td>-0.754496</td>\n",
       "      <td>-0.782437</td>\n",
       "      <td>-0.702246</td>\n",
       "      <td>-0.584819</td>\n",
       "      <td>-0.462281</td>\n",
       "      <td>0.360878</td>\n",
       "      <td>-0.268439</td>\n",
       "      <td>-0.302901</td>\n",
       "      <td>-0.340486</td>\n",
       "      <td>-0.570672</td>\n",
       "      <td>-0.755228</td>\n",
       "      <td>-0.782008</td>\n",
       "      <td>-0.705261</td>\n",
       "      <td>-0.590581</td>\n",
       "      <td>-0.468862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6108</th>\n",
       "      <td>-0.082970</td>\n",
       "      <td>-0.123427</td>\n",
       "      <td>0.101262</td>\n",
       "      <td>0.405648</td>\n",
       "      <td>0.578660</td>\n",
       "      <td>0.476890</td>\n",
       "      <td>0.271017</td>\n",
       "      <td>0.211466</td>\n",
       "      <td>0.257518</td>\n",
       "      <td>0.282207</td>\n",
       "      <td>0.226513</td>\n",
       "      <td>-0.025818</td>\n",
       "      <td>0.269845</td>\n",
       "      <td>0.735363</td>\n",
       "      <td>1.123745</td>\n",
       "      <td>1.118167</td>\n",
       "      <td>0.872491</td>\n",
       "      <td>0.779338</td>\n",
       "      <td>0.723321</td>\n",
       "      <td>0.621802</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>509 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             surge1_t0  surge1_t1  surge1_t2  surge1_t3  surge1_t4  surge1_t5  \\\n",
       "id_sequence                                                                     \n",
       "5600         -1.522672  -1.640056  -1.812372  -1.873276  -1.911106  -1.892778   \n",
       "5601         -0.172018  -0.244204  -0.333128  -0.247466   0.008339   0.311038   \n",
       "5602         -0.728413  -1.103798  -1.081307  -0.869735  -0.684275  -0.531688   \n",
       "5603         -0.214031   0.022230   0.161170   0.047790  -0.173768  -0.312626   \n",
       "5604          0.188806   0.256014   0.136325  -0.055788  -0.255821  -0.438868   \n",
       "...                ...        ...        ...        ...        ...        ...   \n",
       "6104         -0.202868  -0.458387  -0.676621  -0.850127  -0.801504  -0.661781   \n",
       "6105          0.004178  -0.027268  -0.104875  -0.129565  -0.042405   0.124772   \n",
       "6106         -0.429599  -0.720932  -0.728523  -0.629140  -0.443044  -0.221718   \n",
       "6107          0.218414  -0.065912  -0.073831  -0.228745  -0.546522  -0.754496   \n",
       "6108         -0.082970  -0.123427   0.101262   0.405648   0.578660   0.476890   \n",
       "\n",
       "             surge1_t6  surge1_t7  surge1_t8  surge1_t9  surge2_t0  surge2_t1  \\\n",
       "id_sequence                                                                     \n",
       "5600         -1.814209  -1.698588  -1.570127  -1.445986  -1.077388  -1.206931   \n",
       "5601          0.669055   1.009192   1.198229   1.189028   0.016760  -0.052519   \n",
       "5602         -0.410095  -0.311700  -0.223303  -0.145472   1.353438   1.628443   \n",
       "5603         -0.312222  -0.243562  -0.164647  -0.095614   1.032095   1.588513   \n",
       "5604         -0.531611  -0.504565  -0.409072  -0.300028  -0.038583   0.077969   \n",
       "...                ...        ...        ...        ...        ...        ...   \n",
       "6104         -0.550004  -0.469796  -0.375337  -0.267382   0.679106  -0.081019   \n",
       "6105          0.274499   0.309280   0.270955   0.226377   0.155758  -0.343349   \n",
       "6106          0.056978   0.360464   0.566990   0.590129   0.005821  -0.308064   \n",
       "6107         -0.782437  -0.702246  -0.584819  -0.462281   0.360878  -0.268439   \n",
       "6108          0.271017   0.211466   0.257518   0.282207   0.226513  -0.025818   \n",
       "\n",
       "             surge2_t2  surge2_t3  surge2_t4  surge2_t5  surge2_t6  surge2_t7  \\\n",
       "id_sequence                                                                     \n",
       "5600         -1.322765  -1.355515  -1.416860  -1.485878  -1.503089  -1.474643   \n",
       "5601         -0.104194  -0.077168   0.124576   0.411364   0.781786   1.110761   \n",
       "5602          1.170808   0.442155   0.039545  -0.142167  -0.216019  -0.203616   \n",
       "5603          1.518492   0.934075   0.517910   0.386632   0.364263   0.341955   \n",
       "5604         -0.076706  -0.310813  -0.561307  -0.757425  -0.837745  -0.798326   \n",
       "...                ...        ...        ...        ...        ...        ...   \n",
       "6104         -0.426764  -0.630051  -0.645582  -0.500996  -0.302407  -0.138851   \n",
       "6105         -0.494503  -0.406895  -0.211891   0.009162   0.194007   0.265217   \n",
       "6106         -0.262538  -0.246235  -0.120654   0.120976   0.452356   0.784378   \n",
       "6107         -0.302901  -0.340486  -0.570672  -0.755228  -0.782008  -0.705261   \n",
       "6108          0.269845   0.735363   1.123745   1.118167   0.872491   0.779338   \n",
       "\n",
       "             surge2_t8  surge2_t9  \n",
       "id_sequence                        \n",
       "5600         -1.412876  -1.315216  \n",
       "5601          1.277642   1.251851  \n",
       "5602         -0.161684  -0.113963  \n",
       "5603          0.313909   0.286394  \n",
       "5604         -0.687479  -0.554552  \n",
       "...                ...        ...  \n",
       "6104         -0.036838   0.022590  \n",
       "6105          0.251194   0.218806  \n",
       "6106          0.954932   0.906470  \n",
       "6107         -0.590581  -0.468862  \n",
       "6108          0.723321   0.621802  \n",
       "\n",
       "[509 rows x 20 columns]"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-22T04:25:42.510131Z",
     "start_time": "2022-03-22T04:25:42.499432Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>surge1_t0</th>\n",
       "      <th>surge1_t1</th>\n",
       "      <th>surge1_t2</th>\n",
       "      <th>surge1_t3</th>\n",
       "      <th>surge1_t4</th>\n",
       "      <th>surge1_t5</th>\n",
       "      <th>surge1_t6</th>\n",
       "      <th>surge1_t7</th>\n",
       "      <th>surge1_t8</th>\n",
       "      <th>surge1_t9</th>\n",
       "      <th>surge2_t0</th>\n",
       "      <th>surge2_t1</th>\n",
       "      <th>surge2_t2</th>\n",
       "      <th>surge2_t3</th>\n",
       "      <th>surge2_t4</th>\n",
       "      <th>surge2_t5</th>\n",
       "      <th>surge2_t6</th>\n",
       "      <th>surge2_t7</th>\n",
       "      <th>surge2_t8</th>\n",
       "      <th>surge2_t9</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_sequence</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5600</th>\n",
       "      <td>-1.199508</td>\n",
       "      <td>-1.171856</td>\n",
       "      <td>-1.131088</td>\n",
       "      <td>-1.066332</td>\n",
       "      <td>-0.990663</td>\n",
       "      <td>-0.864091</td>\n",
       "      <td>-0.801858</td>\n",
       "      <td>-0.746576</td>\n",
       "      <td>-0.696449</td>\n",
       "      <td>-0.651072</td>\n",
       "      <td>-0.726454</td>\n",
       "      <td>-0.739934</td>\n",
       "      <td>-0.704749</td>\n",
       "      <td>-0.652886</td>\n",
       "      <td>-0.602378</td>\n",
       "      <td>-0.553215</td>\n",
       "      <td>-0.507450</td>\n",
       "      <td>-0.465277</td>\n",
       "      <td>-0.426380</td>\n",
       "      <td>-0.390044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5601</th>\n",
       "      <td>-0.189759</td>\n",
       "      <td>-0.080710</td>\n",
       "      <td>0.080381</td>\n",
       "      <td>0.214584</td>\n",
       "      <td>0.307948</td>\n",
       "      <td>0.374662</td>\n",
       "      <td>0.426763</td>\n",
       "      <td>0.470630</td>\n",
       "      <td>0.509728</td>\n",
       "      <td>0.546062</td>\n",
       "      <td>-0.023364</td>\n",
       "      <td>0.019109</td>\n",
       "      <td>0.151435</td>\n",
       "      <td>0.216138</td>\n",
       "      <td>0.207174</td>\n",
       "      <td>0.181843</td>\n",
       "      <td>0.155302</td>\n",
       "      <td>0.228899</td>\n",
       "      <td>0.305990</td>\n",
       "      <td>0.377340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5602</th>\n",
       "      <td>-0.236023</td>\n",
       "      <td>-0.227396</td>\n",
       "      <td>-0.211098</td>\n",
       "      <td>-0.195290</td>\n",
       "      <td>-0.173217</td>\n",
       "      <td>-0.148450</td>\n",
       "      <td>-0.123184</td>\n",
       "      <td>-0.097737</td>\n",
       "      <td>-0.071892</td>\n",
       "      <td>-0.045371</td>\n",
       "      <td>0.345954</td>\n",
       "      <td>0.105727</td>\n",
       "      <td>0.012435</td>\n",
       "      <td>-0.008099</td>\n",
       "      <td>0.004825</td>\n",
       "      <td>0.026204</td>\n",
       "      <td>0.049977</td>\n",
       "      <td>0.044051</td>\n",
       "      <td>0.072038</td>\n",
       "      <td>0.103419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5603</th>\n",
       "      <td>-0.019892</td>\n",
       "      <td>-0.034730</td>\n",
       "      <td>-0.047265</td>\n",
       "      <td>-0.054631</td>\n",
       "      <td>-0.026954</td>\n",
       "      <td>0.002579</td>\n",
       "      <td>0.007071</td>\n",
       "      <td>-0.003261</td>\n",
       "      <td>0.008565</td>\n",
       "      <td>0.019975</td>\n",
       "      <td>0.561087</td>\n",
       "      <td>0.464187</td>\n",
       "      <td>0.381445</td>\n",
       "      <td>0.333398</td>\n",
       "      <td>0.350693</td>\n",
       "      <td>0.344865</td>\n",
       "      <td>0.275331</td>\n",
       "      <td>0.209318</td>\n",
       "      <td>0.188466</td>\n",
       "      <td>0.206171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5604</th>\n",
       "      <td>0.047979</td>\n",
       "      <td>0.138255</td>\n",
       "      <td>0.121141</td>\n",
       "      <td>0.073845</td>\n",
       "      <td>0.050825</td>\n",
       "      <td>0.023558</td>\n",
       "      <td>0.002464</td>\n",
       "      <td>-0.011403</td>\n",
       "      <td>-0.019011</td>\n",
       "      <td>-0.021168</td>\n",
       "      <td>-0.131821</td>\n",
       "      <td>0.035536</td>\n",
       "      <td>0.045335</td>\n",
       "      <td>0.005460</td>\n",
       "      <td>-0.030595</td>\n",
       "      <td>-0.056825</td>\n",
       "      <td>-0.075062</td>\n",
       "      <td>-0.067410</td>\n",
       "      <td>-0.075035</td>\n",
       "      <td>-0.065493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6104</th>\n",
       "      <td>-0.135019</td>\n",
       "      <td>-0.469348</td>\n",
       "      <td>-0.601718</td>\n",
       "      <td>-0.616191</td>\n",
       "      <td>-0.556177</td>\n",
       "      <td>-0.512051</td>\n",
       "      <td>-0.472327</td>\n",
       "      <td>-0.434713</td>\n",
       "      <td>-0.399322</td>\n",
       "      <td>-0.341437</td>\n",
       "      <td>1.137056</td>\n",
       "      <td>0.598249</td>\n",
       "      <td>0.287585</td>\n",
       "      <td>0.139036</td>\n",
       "      <td>0.090300</td>\n",
       "      <td>0.081051</td>\n",
       "      <td>0.086205</td>\n",
       "      <td>0.098205</td>\n",
       "      <td>0.114662</td>\n",
       "      <td>0.097834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6105</th>\n",
       "      <td>0.683089</td>\n",
       "      <td>0.908907</td>\n",
       "      <td>0.897307</td>\n",
       "      <td>0.772712</td>\n",
       "      <td>0.645870</td>\n",
       "      <td>0.534164</td>\n",
       "      <td>0.454687</td>\n",
       "      <td>0.378453</td>\n",
       "      <td>0.335563</td>\n",
       "      <td>0.306968</td>\n",
       "      <td>1.416681</td>\n",
       "      <td>1.488015</td>\n",
       "      <td>1.375669</td>\n",
       "      <td>1.207726</td>\n",
       "      <td>1.055663</td>\n",
       "      <td>0.932966</td>\n",
       "      <td>0.837150</td>\n",
       "      <td>0.763569</td>\n",
       "      <td>0.708463</td>\n",
       "      <td>0.595871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6106</th>\n",
       "      <td>-0.003328</td>\n",
       "      <td>-0.161861</td>\n",
       "      <td>-0.211071</td>\n",
       "      <td>-0.234781</td>\n",
       "      <td>-0.243938</td>\n",
       "      <td>-0.244648</td>\n",
       "      <td>-0.240823</td>\n",
       "      <td>-0.234090</td>\n",
       "      <td>-0.225040</td>\n",
       "      <td>-0.213867</td>\n",
       "      <td>0.487938</td>\n",
       "      <td>0.265363</td>\n",
       "      <td>0.157881</td>\n",
       "      <td>0.095710</td>\n",
       "      <td>0.056991</td>\n",
       "      <td>0.033530</td>\n",
       "      <td>0.024331</td>\n",
       "      <td>0.018640</td>\n",
       "      <td>0.017186</td>\n",
       "      <td>0.020715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6107</th>\n",
       "      <td>0.914033</td>\n",
       "      <td>0.608752</td>\n",
       "      <td>0.300186</td>\n",
       "      <td>0.149531</td>\n",
       "      <td>0.089661</td>\n",
       "      <td>0.026004</td>\n",
       "      <td>0.010477</td>\n",
       "      <td>-0.026670</td>\n",
       "      <td>-0.055958</td>\n",
       "      <td>-0.076653</td>\n",
       "      <td>1.909812</td>\n",
       "      <td>1.394225</td>\n",
       "      <td>0.941408</td>\n",
       "      <td>0.663911</td>\n",
       "      <td>0.512475</td>\n",
       "      <td>0.403015</td>\n",
       "      <td>0.346634</td>\n",
       "      <td>0.284168</td>\n",
       "      <td>0.265617</td>\n",
       "      <td>0.259639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6108</th>\n",
       "      <td>-0.070369</td>\n",
       "      <td>-0.104713</td>\n",
       "      <td>0.071311</td>\n",
       "      <td>0.164608</td>\n",
       "      <td>0.172307</td>\n",
       "      <td>0.137374</td>\n",
       "      <td>0.098209</td>\n",
       "      <td>0.066351</td>\n",
       "      <td>0.042581</td>\n",
       "      <td>0.026450</td>\n",
       "      <td>0.192584</td>\n",
       "      <td>0.116951</td>\n",
       "      <td>0.265056</td>\n",
       "      <td>0.342862</td>\n",
       "      <td>0.323550</td>\n",
       "      <td>0.271288</td>\n",
       "      <td>0.221877</td>\n",
       "      <td>0.182179</td>\n",
       "      <td>0.152361</td>\n",
       "      <td>0.131725</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>509 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             surge1_t0  surge1_t1  surge1_t2  surge1_t3  surge1_t4  surge1_t5  \\\n",
       "id_sequence                                                                     \n",
       "5600         -1.199508  -1.171856  -1.131088  -1.066332  -0.990663  -0.864091   \n",
       "5601         -0.189759  -0.080710   0.080381   0.214584   0.307948   0.374662   \n",
       "5602         -0.236023  -0.227396  -0.211098  -0.195290  -0.173217  -0.148450   \n",
       "5603         -0.019892  -0.034730  -0.047265  -0.054631  -0.026954   0.002579   \n",
       "5604          0.047979   0.138255   0.121141   0.073845   0.050825   0.023558   \n",
       "...                ...        ...        ...        ...        ...        ...   \n",
       "6104         -0.135019  -0.469348  -0.601718  -0.616191  -0.556177  -0.512051   \n",
       "6105          0.683089   0.908907   0.897307   0.772712   0.645870   0.534164   \n",
       "6106         -0.003328  -0.161861  -0.211071  -0.234781  -0.243938  -0.244648   \n",
       "6107          0.914033   0.608752   0.300186   0.149531   0.089661   0.026004   \n",
       "6108         -0.070369  -0.104713   0.071311   0.164608   0.172307   0.137374   \n",
       "\n",
       "             surge1_t6  surge1_t7  surge1_t8  surge1_t9  surge2_t0  surge2_t1  \\\n",
       "id_sequence                                                                     \n",
       "5600         -0.801858  -0.746576  -0.696449  -0.651072  -0.726454  -0.739934   \n",
       "5601          0.426763   0.470630   0.509728   0.546062  -0.023364   0.019109   \n",
       "5602         -0.123184  -0.097737  -0.071892  -0.045371   0.345954   0.105727   \n",
       "5603          0.007071  -0.003261   0.008565   0.019975   0.561087   0.464187   \n",
       "5604          0.002464  -0.011403  -0.019011  -0.021168  -0.131821   0.035536   \n",
       "...                ...        ...        ...        ...        ...        ...   \n",
       "6104         -0.472327  -0.434713  -0.399322  -0.341437   1.137056   0.598249   \n",
       "6105          0.454687   0.378453   0.335563   0.306968   1.416681   1.488015   \n",
       "6106         -0.240823  -0.234090  -0.225040  -0.213867   0.487938   0.265363   \n",
       "6107          0.010477  -0.026670  -0.055958  -0.076653   1.909812   1.394225   \n",
       "6108          0.098209   0.066351   0.042581   0.026450   0.192584   0.116951   \n",
       "\n",
       "             surge2_t2  surge2_t3  surge2_t4  surge2_t5  surge2_t6  surge2_t7  \\\n",
       "id_sequence                                                                     \n",
       "5600         -0.704749  -0.652886  -0.602378  -0.553215  -0.507450  -0.465277   \n",
       "5601          0.151435   0.216138   0.207174   0.181843   0.155302   0.228899   \n",
       "5602          0.012435  -0.008099   0.004825   0.026204   0.049977   0.044051   \n",
       "5603          0.381445   0.333398   0.350693   0.344865   0.275331   0.209318   \n",
       "5604          0.045335   0.005460  -0.030595  -0.056825  -0.075062  -0.067410   \n",
       "...                ...        ...        ...        ...        ...        ...   \n",
       "6104          0.287585   0.139036   0.090300   0.081051   0.086205   0.098205   \n",
       "6105          1.375669   1.207726   1.055663   0.932966   0.837150   0.763569   \n",
       "6106          0.157881   0.095710   0.056991   0.033530   0.024331   0.018640   \n",
       "6107          0.941408   0.663911   0.512475   0.403015   0.346634   0.284168   \n",
       "6108          0.265056   0.342862   0.323550   0.271288   0.221877   0.182179   \n",
       "\n",
       "             surge2_t8  surge2_t9  \n",
       "id_sequence                        \n",
       "5600         -0.426380  -0.390044  \n",
       "5601          0.305990   0.377340  \n",
       "5602          0.072038   0.103419  \n",
       "5603          0.188466   0.206171  \n",
       "5604         -0.075035  -0.065493  \n",
       "...                ...        ...  \n",
       "6104          0.114662   0.097834  \n",
       "6105          0.708463   0.595871  \n",
       "6106          0.017186   0.020715  \n",
       "6107          0.265617   0.259639  \n",
       "6108          0.152361   0.131725  \n",
       "\n",
       "[509 rows x 20 columns]"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark\n",
    "Train using kNN of pressure fields at two instants in time, with 40 neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def surge_prediction_metric(dataframe_y_true, dataframe_y_pred):\n",
    "    weights = np.linspace(1, 0.1, 10)[np.newaxis]\n",
    "    surge1_columns = [\n",
    "        'surge1_t0', 'surge1_t1', 'surge1_t2', 'surge1_t3', 'surge1_t4',\n",
    "        'surge1_t5', 'surge1_t6', 'surge1_t7', 'surge1_t8', 'surge1_t9' ]\n",
    "    surge2_columns = [\n",
    "        'surge2_t0', 'surge2_t1', 'surge2_t2', 'surge2_t3', 'surge2_t4',\n",
    "        'surge2_t5', 'surge2_t6', 'surge2_t7', 'surge2_t8', 'surge2_t9' ]\n",
    "    surge1_score = (weights * (dataframe_y_true[surge1_columns].values - dataframe_y_pred[surge1_columns].values)**2).mean()\n",
    "    surge2_score = (weights * (dataframe_y_true[surge2_columns].values - dataframe_y_pred[surge2_columns].values)**2).mean()\n",
    "\n",
    "    return surge1_score + surge2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfields = 2; time_step_slp = 8\n",
    "slp_train = []\n",
    "slp_all = X_train['slp']\n",
    "for i in range(5559):\n",
    "    slp_train.append(np.ndarray.flatten(slp_all[i,-1]))\n",
    "    for j in range(1,nfields):\n",
    "        slp_train[-1] = np.concatenate( ( slp_train[-1], np.ndarray.flatten(slp_all[i,-1-j*time_step_slp]) ) )\n",
    "slp_train = np.array(slp_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "slp_test = []\n",
    "slp_all_test = X_test['slp']\n",
    "for i in range(509):\n",
    "    slp_test.append(np.ndarray.flatten(slp_all_test[i,-1]))\n",
    "    for j in range(1,nfields):\n",
    "        slp_test[-1] = np.concatenate( ( slp_test[-1], np.ndarray.flatten(slp_all_test[i,-1-j*time_step_slp]) ) )\n",
    "slp_test = np.array(slp_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = BallTree(slp_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "surge_test_benchmark = []; k = 40\n",
    "for i in range(509):\n",
    "    dist, ind = tree.query([slp_test[i]], k=k)\n",
    "    surge_test_benchmark.append(np.mean(surge_train[ind[0]], axis=0))\n",
    "surge_test_benchmark = np.array(surge_test_benchmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_columns = [f'surge1_t{i}' for i in range(10)] + [f'surge2_t{i}' for i in range(10)]\n",
    "y_test_benchmark = pd.DataFrame(data=surge_test_benchmark, columns=y_columns, index=X_test['id_sequence'])\n",
    "y_test_benchmark.to_csv('Y_test_benchmark.csv', index_label='id_sequence', sep=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
